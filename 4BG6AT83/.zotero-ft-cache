Diffusion for Multi-Embodiment Grasping
Roman Freiberg1,∗, Alexander Qualmann1,Ngo Anh Vien1 and Gerhard Neumann2
Abstract— Grasping is a fundamental skill in robotics with diverse applications across medical, industrial, and domestic domains. However, current approaches for predicting valid grasps are often tailored to specific grippers, limiting their applicability when gripper designs change. To address this limitation, we explore the transfer of grasping strategies between various gripper designs, enabling the use of data from diverse sources. In this work, we present an approach based on equivariant diffusion that facilitates gripper-agnostic encoding of scenes containing graspable objects and gripper-aware decoding of grasp poses by integrating gripper geometry into the model. We also develop a dataset generation framework that produces cluttered scenes with variable-sized object heaps, improving the training of grasp synthesis methods. Experimental evaluation on diverse object datasets demonstrates the generalizability of our approach across gripper architectures, ranging from simple parallel-jaw grippers to humanoid hands, outperforming both single-gripper and multi-gripper state-of-the-art methods. Index Terms — Multi-Embodiment, Grasping, Diffusion Models, Transfer Learning
I. INTRODUCTION
Recent advancements in imitation learning and reinforcement learning have significantly enhanced data efficiency and performance in robotic grasping methods. Notably, the integration of diffusion models [1]–[3] has shown promising results in handling multimodal grasping data. However, current grasping methods are often designed for task-specific end-effectors and exhibit limited transferability to alternative gripper architectures. These methods are tailored to predefined hardware configurations, assuming fixed characteristics such as degrees of freedom (DoF), gripper geometry, and the physical contact mechanics of the end-effector. Transferability is crucial for developing generalizable grasp detection methods that can work across various robot systems with different gripper designs. It requires solutions that are resilient to specific hyperparameters and hidden biases, allowing them to incorporate data from diverse sources [4], [5]. Incorporating gripper-agnostic design into architectures could pave the way for future foundation models in robotic grasping [6], [7]. Our contribution is twofold. First, we propose an approach based on an SE(3)-equivariant architecture [3], [8] that demonstrates state-of-the-art performance in both singlegripper settings and multi-gripper benchmarks, highlighting the effectiveness of integrating gripper geometry with feature adaptation across diverse gripper configurations. We directly put the diffusion process always relative to the gripper frame, which simplifies the equivariance conditions and improves
1 Bosch Corporate Research, {roman.freiberg, alexander.qualmann, anhvien.ngo}@bosch.com ∗Corresponding author 2Karlsruhe Institute of Technology gerhard.neumann@kit.edu
Equivariant Scene Encoder
Gripper Point Cloud (open)
Equivariant Gripper Encoder Gripper features
Scene Point Cloud Scene features
Score Head
pose estimate
Gripper Point Cloud (closed) Gripper features
Equivariant Gripper Encoder
time
query features
(a) Encoding (b) Diffusion
Fig. 1: Overview of architecture. (a) Point cloud scans of scene with objects and gripper in open and closed configurations are equivariantly encoded to a point cloud feature space. (b) Diffusion process computes pre-grasp pose for given gripper and scene using feature query for current pose estimate.
performance. Second, we provide experimental validation of our approach on diverse object datasets [9], [10]. Finally, we plan to open-source our multi-gripper grasp scene generation framework, which integrates eight gripper types and over 1,000 objects. Our findings strongly suggest that training on cluttered heaps is beneficial for grasp synthesis methods; as such, we include a pipeline for variable-size object heap generation in both tabletop and bin picking settings.
II. RELATED WORK
a) Grasp Detection Methods and Datasets: Data-driven approaches have significantly advanced single-gripper grasping, covering both open-loop and closed-loop strategies [11][18]. By incorporating visual features and gripper-specific priors, these methods have been tailored to end-effectors like parallel-jaw or vacuum grippers. Such approaches effectively leverage fixed hardware configurations, facilitating learningbased strategies. However, their specificity often limits their applicability to other gripper designs. This limitation has spurred interest in multi-embodiment agents and robotics foundation models [6], [7], [19]. Several datasets have been introduced for robotic grasping, each focusing on single gripper types. The Cornell Grasp Dataset [20] provides annotated RGB-D images for grasp detection with a specific gripper. GraspNet-1Billion [11] and EGAD! [21] offer extensive grasp poses, but primarily for parallel-jaw grippers. Other works, including Mahler et al. [22] and Eppner et al. [18], present large-scale grasp datasets but focus on specific grippers like the Franka Panda. This lack of datasets encompassing multiple gripper types across diverse objects highlights the need for our custom dataset.
arXiv:2410.18835v1 [cs.RO] 24 Oct 2024


b) Generalist Agents: Generalist methods aim to minimize inductive biases by treating grasping as a secondary objective within a range of diverse tasks [6], [23]. While these ”one-size-fits-all” representations are versatile and can handle various tasks, they may not emphasize grasp-specific details such as gripper configurations. In contrast, generalizable grasp representations focus on developing models that can transfer across different grasp distributions. A notable example of this approach is the use of contact maps, which highlight successful grasp regions on objects [4], [5]. Gripper-specific encodings directly predict stable grasps by learning gripper-specific information. However, methods that focus solely on top-grasp selection, like AdaGrasp [15], may limit broader applicability. Our work aligns with this category, as we believe it fosters models that are better suited for gripper generalization.
c) Diffusion for Grasp Synthesis: Diffusion methods have revolutionized distribution modeling [3], [24]–[27], particularly for the multimodal distributions present in grasp data [2], [3], [28]. Advances in manifold modeling, such as on SE(3), have further enabled learning of grasp poses [3], [29]. Additionally, zero-shot domain transfer has become an essential technique for bridging the gap between different data distributions, with notable examples including CycleGAN [30] and the Mirage project [31]. While these methods are effective, we avoid relying on a single physical gripper model as the sole reference to prevent potential biases. d) Equivariant Methods: There has been a growing interest in exploiting data symmetries in grasping tasks. Since the pioneering work of Cohen and Welling [32], which introduced the concept of discrete group convolutions in CNN-based architectures, there has been substantial ongoing research in this field. Subsequent investigations [33][35] have expanded the methodology by utilizing steerable basis functions, enabling the representation of continuous groups such as E(N ) on Riemannian manifolds. Compared to traditional approaches, equivariant designs typically exhibit superior generalization and data efficiency [36]. Equivariance, particularly with respect to rotations and translations, offers significant gains in data efficiency, making it highly beneficial for robotic applications [8], [37], [38]. Additionally, graph-based networks have proven effective in modeling structurally complex problems across varying data scales [39]–[41]. Their adaptability in handling diverse data inputs has made them increasingly popular in robotics, particularly for point cloud and context modeling [42]–[45].
III. PROBLEM FORMULATION AND PRELIMINARIES
We assume a given set of grippers {G1, . . . , Gn}, each associated with internal joint configurations {J1, . . . , Jn} with variable DoF. Prominent examples include the gripper width for parallel-jaw architectures, such as the Franka Gripper, and the 20 DoFs in the finger joints of the Shadow Hand. In this work, we predominantly focus on the subproblem with fixed configurations while discussing possible extensions of our approach to the original problem statement. As such, each gripper is described by a pre-grasp and the
corresponding target grasp configuration. Different gripper configurations are treated as distinct gripper types. This is exemplified in our use of the Shadow Hand in various gripper configurations, including two-finger, three-finger, and fullhand grasps, treating each configuration as a distinct gripper type. We denote oGi as the observation of the gripper in its local coordinate frame, consisting of open and closed point clouds rendered from the open and closed gripper geometries, respectively. The target policy distribution is defined by p(r | oGi , os), where r ∈ SE(3) represents the target pre-grasp pose, and os is the observation of the scene s in the world coordinate frame containing at least one graspable object. Potential obstacles are treated as part of the scene.
A. Diffusion for Generative Modeling
Diffusion models have proven to be a viable method for modeling high-dimensional multimodal distributions. Standard diffusion models on Rd are based on the OrnsteinUhlenbeck process [46], described by
dx = −β(t)x dt + p2β(t) dB, x(0) ∼ p0,
where x(t) is an adaptable process on Rd, and B(t) is the Brownian motion. As time t approaches infinity, this distribution converges to the normal distribution with zero mean and Id covariance, where Id is the d-dimensional identity matrix. Here, x(0) represents the process origin at initial time. The reverse process, given by
dx = −β(t) (x + 2∇ log(pt(x))) dt + p2β(t) dB, (1)
allows the computation of the backward path during inference by approximating the score ∇ log(pt(x)) of the process through optimizing the corresponding MSE loss formulation [24].
B. Diffusion on SE(3)
For compact manifolds such as SO(3), De Bortoli et al. [27] proposed the corresponding process dx = dBSO(3), to fully exploit the data structure, where BSO(3)
t represents Brownian motion on the SO(3) manifold. The SE(3) manifold, which combines SO(3) for rotation and R3 for translation, requires a combined process, p(rt | r0) = BSE(3)
t (r−1
0 rt), where rt, r0 ∈ SE(3). A pose r = (ω, p) can be decomposed into its rotational and translational parts, where ω is the rotation angle in the axis-angle parametrization, p is the translation. Similarly, the Brownian motion on SE(3) can be decomposed
into
BSE(3)
t (ω, p) = N (p; p, tId)IGSO(3)(ω, t).
Here, IGSO(3) is the isotropic Gaussian on SO(3) defined by the relation
IGSO(3)(ω, t) ∝
∞
X
l=0
(2l + 1)e−tl(l+1)/2 sin((l + 1/2)ω)
sin(ω/2) .
This formulation allows for the numerical simulation of the forward path, following the sampling procedure presented in the work of Leach et al. [29] and Ryu et al. [3].


C. Equivariance on SE(3)
A function f : X → Y , between two vector spaces X, Y , is said to be equivariant with respect to the group action r ∈ SE(3) if it satisfies the condition
f (DX (r)x) = DY (r)f (x), (2)
where x ∈ X, y ∈ Y , and DX/Y (r) are transformation matrices parameterized by r representing the group action as a matrix in the respective domain.
IV. MULTI-EMBODIMENT GRASPING
In order to achieve multi-embodiment grasping, we first need to create multi-gripper datasets for training our grasp synthesis models. We use eight gripper types and generate grasps for single objects as well as for cluttered scenes. In terms of grasp synthesis, we propose an architecture based on equivariant diffusion for grasp prediction [3]. Here, we introduce several changes to this architecture to perform efficient multi-embodiment grasp synthesis.
A. Dataset Generation
a) Grasp Synthesis: . For our grasp synthesis procedure, we require a highly parallelizable pipeline at any stage to manage computational complexity. To meet this requirement and avoid intensive manual tuning, we opted for a single antipodal sampling strategy with individually aligned approaches and antipodal directions for each gripper. Although this sampling strategy excludes grasp categories as it does not fully exploit the gripper morphology, our generation method still captures gripper-specific data, as summarized in Figure 3. Thus, a gripper-agnostic transfer of grasps does not provide a sufficient method to solve the original problem statement in our dataset. We used MuJoCo simulations [47] and objects from the YCB [9] and Google Scanned Objects datasets [10]. Due to the requirements of MuJoCo for convex collision models, we preprocessed objects with V-HACD [48] to approximate collision surfaces while preserving simulation performance. Figure 4 shows example renderings of collision models of grasping scenes for several gripper types. After removing multi-part objects, the dataset comprised over 1,000 objects with diverse sizes and categories. In some cases, objects were larger than the grippers, necessitating precise placement for a successful grasp. We used a variety of gripper models sourced from Google DeepMind’s Menagerie [49] and robosuite [50], as depicted in Figure 2. Our gripper set contains five two-finger grippers, one three-finger gripper, one fourfinger gripper, and three Shadow Hand configurations. Grasp generation occurred in a gravity-free environment with shaking motions to determine stability, following the approach of Eppner et. al [18]. In general, we simulated 5,000 successful grasps per object-gripper pair. We allowed for fewer than 5,000 successful grasps per gripper-object pair where the yield was below a threshold, defined by an estimated time of arrival computed from prior statistics, making grasp synthesis computationally costly. All computations were performed
on a high-performance cluster running 1,500 CPU cores in parallel.
b) Clutter Scene Generation: For grasp scene generation, we assembled heaps of objects, consisting of one to twelve items, dropped onto a flat table. Thus, our generation pipeline allows for variable complex heap generation. Additionally, we developed a scene variant simulating a bin picking environment with randomly generated variable sizes and colors for the bin to mitigate sim-to-real complications for our real-world bin picking experiments in Section V. All environments introduced light and ground color randomizations to reduce overfitting on specific attributes, thus making the transfer to real environments more robust. After a scene has settled into a stable configuration, previously generated valid grasps were transformed to the object’s pose. All grasps were reevaluated and marked as stable when a gripper could lift an object while maintaining contact throughout the lift operation. Grasps that resulted in a collision in the initial pre-grasp pose were removed upfront. In total, we synthesized and evaluated over 750 million grasps and distilled 2.5 million valid grasps from these simulations across 450 simulation scenes per gripper variant for training. Each sample includes an associated gripper and scene point cloud, sourced from dense and sparse scans. Dense scans are obtained from ten color images containing depth information, while sparse scans are obtained from a single image. For future extensions, our framework supports the integration of all MuJoCo-compatible grippers [49], diverse object datasets, and various grasp sampling strategies.
B. Architecture
In the original architecture of Diffusion-EDFs [3], the equivariance condition is expressed as
p(∆rr | oGi , ∆ros) = p(r | oGi , os), (3)
where ∆r ∈ SE(3) represents an arbitrary transformation. An equivariant model must preserve this condition throughout the computational pipeline. The authors of DiffusionEDFs proposed a U-Net hierarchical encoding of the scene’s point cloud. At each layer, they downsample the point cloud using the farthest-point sampling (FPS) algorithm to capture higher-level features. The equivariant descriptor field (EDF) at each stage comprises points with associated features composed of irreducible representations up to angular momentum type l = 2 [51], which can be transformed equivariantly under SO(3) actions. In the decoder stage, a shallower network is used to query information at specified positions, dependent on the current diffusion time, in the hierarchical EDF. For the pick task, the authors used predefined static query positions. During the diffusion process, the descriptor features are transformed under the rotation group action using the corresponding Wigner D-matrices for each irreducible representation. Finally, the score head employs the queried features to approximate the score of the reverse diffusion process from Equation 1, including the rotational component. In order to make this architecture usable for multiembodiment grasp synthesis, we had to introduce the fol


(a) (b) (c) (d) (e) (f) (g) (h)
Fig. 2: Overview of used grippers. (a) Robotiq 2F-85, (b) Franka Emika Gripper, (c) Google Bot Gripper, (d) Rethink Gripper, (e) ViperX 300s Gripper, (f) Allegro (g) Shadow DEX-EE Hand (h) Shadow Hand
lowing adaptations to the Diffusion-EDF [3] architecture. a) Encoding the Gripper Geometry: As shown in Figure 1, we exploit the query mechanism to incorporate the open and closed gripper geometries, conditioning the diffusion process on end-effector-specific features. The FPS downsampled gripper point clouds encode gripper-specific features using a shallow U-Net. These positions query features at points transformed by the diffusion process. For each layer, the query and EDF point cloud, along with the time embeddings, are passed to the query mechanism, which computes relative position and angular edge encodings between both point clouds. These encodings, along with all components, are passed to an equiformer layer that outputs layer-wise field values. Summing over all layers produces the query features, which are fed to the score head.
b) Using the Gripper-Frame as Reference-Frame: Originally, the authors specified the grasp equivariance condition expressed as
s(r∆−1
r | ∆roGi , os) = [Ad∆r ]−T s(r | oGi , os),
where s(r | oGi , os) is the score function and Ad∆r is an adjoint representation of the SE(3) transformation ∆r [3]. The adjoint representation is defined as
Adr=(p,ω) = ω [p]∧ω
0 ω,
where [p]∧ is the skew-symmetric matrix of p. This relation emerges naturally from the computation of the right equivariant Lie-derivative of the score function (see Proposition 1. in the work of Ryu et. al [3]). Achieving this condition as well as the equivariance condition from Equation 3 requires the introduction of a reference frame selection mechanism. However, in our case, we do not require this grasp equivariance condition because the diffusion process always takes place in the gripper’s reference frame. Since we are interested in grasp synthesis relative to a fixed gripper frame, enforcing reference frame independence is unnecessary. By not imposing this condition, we simplify the model, which significantly facilitates training and increases performance.
c) Equivariant FiLM Layers for Multi-Resolution Diffusion: The original model was split into two parts: one handling the low-resolution diffusion and the other the
1 2 3 4 5 6 7 8 9 10
0
100
200
300
Gripper able to grasp an object
Number of objects
Count of Grippers Able to Grasp an Object
Fig. 3: Count of Grippers Able to Grasp an Object. Grasp generation uses an antipodal sampling strategy for all gripper types, while preserving some gripper-specific properties.
high-resolution diffusion process. We developed a model variant introducing an equivariant version of the FiLM [52] layer, commonly used in diffusion architectures [53]. Specifically, these layers modulate the encoded equivariant features element-wise for each angular momentum type l at each timestep by applying scalar shifts and biases to each feature channel corresponding to the irreducible representations, ensuring that the modulation respects the SO(3) equivariance. The modulation parameters are generated from the time embeddings through learnable mappings and are applied via equivariant tensor products. This design allows the model to account for distribution shifts during the diffusion process. Implementing this variant enabled us to train a single unified model, reducing the total training time by up to 35 percent. Furthermore, our data provided grasp-object relations; thus, we trained on batches of grasps sampled from diverse objects (biased sampling) to mitigate distribution bias toward objects with more successful grasps.
V. EXPERIMENTAL EVALUATIONS
We evaluate our approach to address several key questions, including whether our method sacrifices single-gripper performance, if it successfully captures the multimodalities present in gripper-specific grasps, and how performance is impacted in zero-shot and few-shot tasks. Additionally, we


(a) Franka Gripper (b) Robotiq 2F-85
(c) Robotiq 3-Finger (d) Shadow Hand
Fig. 4: Collision models for grasp scenes. Rendering of collision models in a grasping scene for various gripper types. Each subfigure illustrates an example of a generated grasp pose specific to the gripper type depicted.
examine whether a model trained on our data is applicable to a real robotic bin picking system.
Method Success Rate (%) Non-equivariant U-Net 52.3 Contact-GraspNet 54.2 Diffusion-EDF 63.3 Ours (only Franka gripper) 94.5
TABLE I: Single-Gripper Benchmark Results. Average grasp success rates for Diffusion-EDF [3], Contact-GraspNet [16] (CGN) with original weights, a non-equivariant model, and our method. Except for CGN, all methods were trained on 2,500 scenes using only the Franka gripper. Results show that our approach does not compromise on single-gripper performance.
a) Evaluation Environments: Our evaluation needs to be parallelizable and capable of multiple grasp evaluations on the same scene. Since all scenes contained objects that were more exposed and thus preferred targets for grasp synthesis methods, we designed an evaluation procedure that simulates object removal in each iteration. Given the number of graspable objects in each scene, we evaluated the number of successful grasps out of 100 generated grasps. Iteratively, we removed a single graspable object and repeated the process until no graspable objects remained. This metric, while biased towards object count, standardized the evaluation and allowed for parallelization and pre-generation of scenes while preserving relevant performance indicators. For all our evaluations, we tested on ten scenes in which each graspable object contained at least 100 valid grasps out of the 5,000 previously generated.
b) Single-Gripper Performance: We first validated the improvements of our method against the original DiffusionEDF [3] algorithm. As shown in Table I, our method shows a notable enhancement in single-gripper grasping performance. Additionally, we compared the Diffusion-EDF architecture
[3] with a comparable non-equivariant U-Net–based variant, where we replaced the equiformer layers with similar graph attention layers [54]. This variant has three times as many parameters and requires ten times the training iterations. The results indicate a performance deficit in the non-equivariant model, demonstrating the effectiveness of equivariant methods for this problem. Furthermore, we compared our approach to the state-of-the-art Contact-GraspNet [16] (CGN). Our method sampled 100 grasps without post-processing, while CGN utilized the 100 most confident grasps from ten different camera angles. In this comparison, we only included CGN with its original model weights. Our dataset does not provide the specific contact information required by CGN, preventing proper training of the method on our benchmark. As such, we only used the inference pipeline of CGN’s method to predict point-wise grasps from our RGBD images captured in simulation. Despite these limitations, CGN’s results remain competitive. It is worth noting that CGN was originally trained on a larger dataset comprising over 10,000 scenes, with scenes being rendered dynamically during training. In contrast, our dataset consists of fixed rendered point clouds and contains four times fewer scenes. This difference in dataset size and rendering approach may impact the comparative performance between our method and CGN.
c) Multi-Gripper Performance: We compared the multigripper performance of our method with AdaGrasp [15]. For this comparison, we created a separate dataset consisting of 450 scenes per gripper, each containing 32 valid and 32 invalid top-grasps within a restricted workspace. Each scene included up to five objects placed flat on a table because AdaGrasp does not account for object heaps. We evaluated the methods by executing one grasp per scene over twenty workspace-restricted scenes. To ensure a fair comparison, we deviated from AdaGrasp’s reinforcementstyle data generation and trained the method using supervised learning. Standard epoch-based learning proved unsuccessful, prompting us to adopt a biased training procedure that prioritized samples with significant discrepancies between predicted scores and ground truth labels. These surprise labels were adjusted through Polyak averaging during training. TSDF volumes were rendered from ten images per scene and gripper, matching AdaGrasp’s resolution. Results for both the retrained and original model weights are presented in Table II, where our method demonstrates a clear advantage across all gripper types. Additionally, we evaluated our model trained on both single-object and cluttered scenes. The results in Table III clearly indicate that a model trained on singleobject scenes does not generalize effectively to cluttered environments, whereas training on cluttered scenes allows for effective generalization to single-object scenarios. These findings underscore the importance of training with complex grasping scenarios to achieve robust performance in diverse environments. Selected scenes with synthesized grasps are shown in Figure 4. In our ablation study, we assessed the impact of key model modifications. The results indicate that both enabling batch learning and incorporating closed


Method Robotiq 2F-85 DEX-EE Hand Shadow Hand (three finger) AdaGrasp (original) 20.0 5.0 5.0 AdaGrasp (retrained) 35.0 15.0 10.0 Without closed gripper 82.8 79.3 58.3 Without batch training 86.1 75.7 76.5 With equivariant FiLM 88.1 78.9 80.9 Full model 91.3 75.8 75.7 No Robotiq 2F-85 (zero-shot) 83.2 75.2 82.4 No three finger Shadow Hand (zero-shot) 87.7 75.0 59.5 No Shadow Hand (zero-shot) 93.0 78.8 37.6 Robotiq 2F-85 Hand few-shot (10 shots) 84.8 - Shadow Hand few-shot (10 shots) - - 45.8
TABLE II: Grasp Success Rates (%) across three gripper variants. Models trained on compatible data consisting of 450 scenes for each gripper variant (4500 scenes total). Results show both original and retrained model variants for AdaGrasp [15]. The full model includes the entire dataset, while zero-shot models exclude certain portions, highlighting zero-shot grasp transfer capabilities. Notably, the Shadow Hand dataset includes four distinct gripper configurations, and the corresponding partial model excludes 1350 scenes.
Method Robotiq 2F-85 DEX-EE Hand Shadow Hand (three finger) Trained single-object; Evaluated single-object 99.1 97.2 91.7 Trained clutter-object; Evaluated single-object 97.2 98.6 93.0 Trained single-object; Evaluated clutter-object 67.6 40.4 27.7 Trained clutter-object; Evaluated clutter-object 88.1 78.9 80.9
TABLE III: Grasp Success Rates (%) Single-object and Object-clutter scenes. Our models were trained and evaluated on both single-object and object-clutter scenes, consisting of 450 scenes (4,500 scenes in total). The model trained on cluttered scenes outperforms the single-object model by a substantial margin on the clutter-object benchmark.
gripper geometry significantly enhance performance on the Shadow Hand and Robotiq 2F-85 grippers. Experiments with the equivariant FiLM [52] model variant also demonstrate promise. In conclusion, we observe strong generalizability across all model variants, with performance increases for each introduced modification, particularly evident for the complex gripper type.
d) Zero- and Few-Shot Performance: Our zero-shot and few-shot performance evaluations are presented in Table II. To simulate a zero-shot setting, we excluded specific gripper types from the training set and trained models on these restricted subsets. All models were trained on an A100 GPU for the same duration; however, due to increased computational demands, the equivariant FiLM model variant was not employed for these tasks. The full model, trained on data from all gripper types, significantly outperformed the baseline model across all tested grippers. Zero-shot models exhibited reduced performance when evaluated on unseen gripper types, underscoring the challenges of zero-shot grasp transfer. In the few-shot scenario, we fine-tuned the zero-shot models using ten scenes of the missing gripper type, each containing only one grasp, and trained for a few epochs. This fine-tuning led to noticeable improvements in performance, suggesting that our approach is beneficial when only a limited number of grasps can be manually gathered for an unknown gripper type.
e) Real-World Laboratory Experiments: Figure 5 illustrates our hardware setup for testing zero-shot and sim-to-real transfer under real-world conditions. We utilized the 7-axis Kassow KR 1205 robot, equipped with a custom Schunk gripper exchanger system for quick gripper replacement. For these experiments, we trained a separate model on bin
Experiment Robotiq 2F-140 Schunk WSG-32 Zero-shot bin picking 82.0 88.0
TABLE IV: Grasp Success Rates (%) in Real-World Bin Picking Experiments. The table summarizes the grasp success rates calculated from 50 executed grasps achieved in zero-shot bin picking tasks using the Robotiq 2F-140, Schunk WSG-32 grippers. All experiments were conducted on objects and grippers not included in the training dataset, demonstrating the model’s generalization capabilities.
picking scenes, filtering out grasps with an angle greater than 25 degrees from the gravity z-direction. Our grasp synthesis inference pipeline takes approximately 35 seconds to generate a batch of 20 grasps on an A2000 GPU. We did not apply any post-processing to the generated grasps and consistently used the first grasp from each batch for evaluation. During execution, we positioned the gripper at the proposed pre-grasp pose with a z-offset, then moved the gripper to the predicted height and closed the gripper. Trajectory collision avoidance was implemented only with respect to the test cell and non-movable objects; objects inside the bin were not considered. During evaluation, we tested the Robotiq 2F-140 and Schunk WSG-32 grippers. A RealSense camera was mounted with a top-down view of the bin. To obtain point clouds of the gripper types, we used synthetic rendering of the CAD models of these grippers. Neither the objects nor the grippers used in the tests were not included in our training dataset. Among the objects in the bin, we always included ten graspable and two obstacle objects and performed a bin clearing procedure. The test object set includes everyday household items, packaging


KASSOW KR1205
BIN
REALSENSE CAMERA
SCHUNK WSG 32
ADAPTER PLATE
DROP ZONE
(a) (b)
Fig. 5: Real-world setup. (a) The Kassow KR 1205 robot with 7 axes, equipped with a Schunk WSG-32 gripper and a Schunk tool changer for gripper exchange. A top-down RealSense camera is used to capture the scene’s point cloud. (b) Visualizations of point clouds including generated grasps for the WSG-32 and Robotiq 2F-85 grippers. Note that the grippers and objects are not part of the training dataset.
boxes, and non-empty bottles, with shapes ranging from boxes and tubes to more complex forms, such as hollow objects like tape rolls. We conducted five series of bin picking tasks for each gripper variant, calculating the average grasp success rate from 50 grasps, as presented in Table IV. All evaluated models were trained solely on simulation data. While the demonstrated performance is noteworthy, we observed a few common failure cases. More than five of the failed grasps were not related to the method and were mainly due to either failed collision avoidance in the trajectory or grasp execution precision. Specifically, smaller objects were sometimes misclassified as being ungraspable. In such cases, when no other graspable object was present, the gripper attempted to grasp the bin.
VI. LIMITATIONS AND FUTURE WORK
Our work aims to develop architectures that incorporate various input features—such as color, texture, physical properties like friction and applied forces, and synthetic features from visual foundation models [55]. While we primarily rely on visual features in this study, our initial encoding preprocesses data into a standard format while preserving spatial information, facilitating future inclusion of additional features. We represent joint configurations in a kinematicagnostic manner by encoding the gripper as a featurized point cloud, excluding internal DoF in this initial step toward a generalist grasp synthesis method. This simplification helps demonstrate our approach but requires further validation and extension, particularly as we aim to incorporate variable internal DoF and kinematic features in future work. Our heuristic gripper encoder uses renderings of the gripper in both open and closed states, simplifying kinematic encoding
and providing information about ambiguous states, especially for high-DoF grippers. While adequate for industrial twoand three-finger grippers like the Schunk variants, this design will need revision as we progress. We encountered disproportionate memory and computational bottlenecks due to unoptimized equivariant libraries. We anticipate resolving these issues with more optimized machine learning frameworks like JAX [56] and ongoing hardware advancements. In this work, we deprioritized computational optimization, deferring performance improvements to future iterations. Our original grasp sampling strategy treats all gripper variants as paralleljaw grippers, enabling large-scale parallelization but causing loss of gripper-specific grasps and lower success rates for non-parallel grippers like the Shadow Hand. Certain gripperobject combinations remain infeasible and are absent from our dataset, limiting the performance of gripper-agnostic algorithms in our benchmarks. Nevertheless, the few-shot cases demonstrate a viable method deployable in common industrial settings—such as bin picking and simple manipulation tasks—with the ability to quickly adapt to novel gripper designs. We believe that further expanding the gripper and object dataset will enhance the zero-shot capabilities of our approach.
VII. CONCLUSION
In this work, we introduced a framework for multiembodiment grasp synthesis using equivariant diffusion models. Our approach achieves state-of-the-art performance in single-gripper scenarios and demonstrates robust generalization across multiple gripper types, significantly surpassing state-of-the-art methods in grasp quality and success rates.
REFERENCES
[1] M. Janner, et al., “Planning with diffusion for flexible behavior synthesis,” in International Conference on Machine Learning, 2022. [2] J. Urain, et al., “Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 5923–5930. [3] H. Ryu, et al., “Diffusion-edfs: Bi-equivariant denoising generative modeling on se (3) for visual robotic manipulation,” arXiv preprint arXiv:2309.02685, 2023.
[4] P. Li, et al., “Gendexgrasp: Generalizable dexterous grasping,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 8068–8074. [5] M. Attarian, et al., “Geometry matching for multi-embodiment grasping,” in Conference on Robot Learning. PMLR, 2023, pp. 1242–1256. [6] K. Bousmalis, et al., “Robocat: A self-improving generalist agent for robotic manipulation,” Transactions on Machine Learning Research, 2024. [Online]. Available: https://openreview.net/forum?id= vsCpILiWHu [7] Q. Vuong, et al., “Open x-embodiment: Robotic learning datasets and RT-x models,” in Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL2023, 2023. [Online]. Available: https://openreview.net/forum?id=zraBtFgxT0 [8] Y.-L. Liao and T. Smidt, “Equiformer: Equivariant graph attention transformer for 3d atomistic graphs,” in International Conference on Learning Representations, 2023. [Online]. Available: https: //openreview.net/forum?id=KwmPfARgOTD [9] B. Calli, et al., “Yale-cmu-berkeley dataset for robotic manipulation research,” The International Journal of Robotics Research, vol. 36, no. 3, pp. 261–268, 2017. [10] L. Downs, et al., “Google scanned objects: A high-quality dataset of 3d scanned household items,” in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 2553–2560.


[11] H.-S. Fang, et al., “Graspnet-1billion: A large-scale benchmark for general object grasping,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 444–11 453. [12] P. Schillinger, et al., “Model-free grasping with multi-suction cup grippers for robotic bin picking,” in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 3107–3113. [13] J. Mahler, et al., “Dex-net 3.0: Computing robust vacuum suction grasp targets in point clouds using a new analytic model and deep learning,” in 2018 IEEE International Conference on robotics and automation (ICRA). IEEE, 2018, pp. 5620–5627. [14] S. Song, et al., “Grasping in the wild: Learning 6dof closed-loop grasping from low-cost demonstrations,” IEEE Robotics and Automation Letters, vol. 5, no. 3, pp. 4978–4985, 2020. [15] Z. Xu, et al., “Adagrasp: Learning an adaptive gripper-aware grasping policy,” in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 4620–4626. [16] M. Sundermeyer, et al., “Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes,” in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 13 438–13 444. [17] H.-S. Fang, et al., “Anygrasp: Robust and efficient grasp perception in spatial and temporal domains,” IEEE Transactions on Robotics (T-RO), 2023. [18] C. Eppner, A. Mousavian, and D. Fox, “ACRONYM: A large-scale grasp dataset based on simulation,” in 2021 IEEE Int. Conf. on Robotics and Automation, ICRA, 2020. [19] S. Reed, et al., “A generalist agent,” arXiv preprint arXiv:2205.06175, 2022. [20] Y. Jiang, S. Moseson, and A. Saxena, “Efficient grasping from rgbd images: Learning using a new rectangle representation,” in 2011 IEEE International Conference on Robotics and Automation, 2011, pp. 3304–3311. [21] D. Morrison, P. Corke, and J. Leitner, “Egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation,” IEEE Robotics and Automation Letters, vol. 5, no. 3, pp. 4368–4375, 2020. [22] J. Mahler, et al., “Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics,” Robotics: Science and Systems (RSS), 2017.
[23] Octo Model Team, et al., “Octo: An open-source generalist robot policy,” in Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. [24] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” Advances in neural information processing systems, vol. 33, pp. 6840–6851, 2020. [25] G. Corso, et al., “Diffdock: Diffusion steps, twists, and turns for molecular docking,” in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https: //openreview.net/forum?id=kKF8 K-mBbS [26] Y. Song, et al., “Score-based generatileach2022denoisingve modeling through stochastic differential equations,” in International Conference on Learning Representations (ICLR), 2021. [Online]. Available: https://openreview.net/forum?id=PxTIG12RRHS [27] V. De Bortoli, et al., “Riemannian score-based generative modelling,” Advances in Neural Information Processing Systems, vol. 35, pp. 2406–2422, 2022. [28] A. Simeonov, et al., “Shelving, stacking, hanging: Relational pose diffusion for multi-modal rearrangement,” arXiv preprint arXiv:2307.04751, 2023.
[29] A. Leach, et al., “Denoising diffusion probabilistic models on so (3) for rotational alignment,” in ICLR 2022 Workshop on Geometrical and Topological Representation Learning, 2022.
[30] J.-Y. Zhu, et al., “Unpaired image-to-image translation using cycleconsistent adversarial networks,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2223–2232.
[31] L. Y. Chen, et al., “Mirage: Cross-embodiment zero-shot policy transfer with cross-painting,” arXiv preprint arXiv:2402.19249, 2024. [32] T. Cohen and M. Welling, “Group equivariant convolutional networks,” in Proceedings of The 33rd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New York, New York, USA: PMLR, 20–22 Jun 2016, pp. 2990–2999. [Online]. Available: https://proceedings.mlr.press/v48/cohenc16.html [33] D. E. Worrall, et al., “Harmonic networks: Deep translation and
rotation equivariance,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5028–5037.
[34] E. J. Bekkers, “B-spline {cnn}s on lie groups,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=H1gBhkBFDH [35] M. Weiler, et al., “Coordinate independent convolutional networksisometry and gauge equivariant convolutions on riemannian manifolds,” arXiv preprint arXiv:2106.06020, 2021.
[36] E. J. Bekkers, et al., “Roto-translation covariant convolutional networks for medical image analysis,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, A. F. Frangi, et al., Eds. Cham: Springer International Publishing, 2018, pp. 440–448. [37] H. Ryu, et al., “Equivariant descriptor fields: Se(3)-equivariant energy-based models for end-to-end visual robotic manipulation learning,” in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/ forum?id=dnjZSPGmY5O [38] J. Yang, et al., “Equibot: Sim(3)-equivariant diffusion policy for generalizable and data efficient learning,” 2024. [39] F. Scarselli, et al., “The graph neural network model,” IEEE transactions on neural networks, vol. 20, no. 1, pp. 61–80, 2008. [40] V. P. Dwivedi and X. Bresson, “A generalization of transformer networks to graphs,” AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.
[41] G. Li, et al., “Deepgcns: Can gcns go as deep as cnns?” in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 9267–9276. [42] X. Lou, Y. Yang, and C. Choi, “Learning object relations with graph neural networks for target-driven grasping in dense clutter,” in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 742–748. [43] I. Huang, et al., “Defgraspnets: Grasp planning on 3d fields with graph neural nets,” in 2023 IEEE International Conference on Robocycle gantics and Automation (ICRA). IEEE, 2023, pp. 5894–5901.
[44] E. Chun, et al., “Local neural descriptor fields: Locally conditioned object representations for manipulation,” in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 1830–1836. [45] A. Simeonov, et al., “Neural descriptor fields: Se (3)-equivariant object representations for manipulation,” in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 6394–6400. [46] P. Baldi and P. Baldi, Stochastic calculus. Springer, 2017. [47] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for model-based control,” in 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 5026–5033.
[48] K. Mamou, E. Lengyel, and A. Peters, “Volumetric hierarchical approximate convex decomposition,” Game engine gems, vol. 3, pp. 141–158, 2016. [49] K. Zakka, Y. Tassa, and MuJoCo Menagerie Contributors, “MuJoCo Menagerie: A collection of high-quality simulation models for MuJoCo,” 2022. [Online]. Available: http://github.com/ google-deepmind/mujoco menagerie [50] Y. Zhu, et al., “robosuite: A modular simulation framework and benchmark for robot learning,” in arXiv preprint arXiv:2009.12293, 2020. [51] M. Geiger and T. Smidt, “e3nn: Euclidean neural networks,” 2022. [Online]. Available: https://arxiv.org/abs/2207.09453 [52] E. Perez, et al., “Film: Visual reasoning with a general conditioning layer,” in AAAI, 2018. [53] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, “3d diffuser actor: Policy diffusion with 3d scene representations,” Arxiv, 2024. [54] S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention networks?” in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/ forum?id=F72ximsx7C1 [55] N. Ravi, et al., “Sam 2: Segment anything in images and videos,” arXiv preprint arXiv:2408.00714, 2024. [Online]. Available: https://arxiv.org/abs/2408.00714 [56] J. Bradbury, et al., “JAX: composable transformations of Python+NumPy programs,” 2018. [Online]. Available: http: //github.com/google/jax