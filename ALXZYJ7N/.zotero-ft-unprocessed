{"indexedPages":14,"totalPages":14,"version":"422","text":"Text2Grasp: Grasp synthesis by text prompts of object grasping parts.\nXiaoyun Chang *1 and Yi Sun †1\n1Dalian University of Technology\nAbstract\nThe hand plays a pivotal role in human ability to grasp and manipulate objects and controllable grasp synthesis is the key for successfully performing downstream tasks. Existing methods that use human intention or task-level language as control signals for grasping inherently face ambiguity. To address this challenge, we propose a grasp synthesis method guided by text prompts of object grasping parts, Text2Grasp, which provides more precise control. Specifically, we present a two-stage method that includes a text-guided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. Furthermore, by leveraging Large Language Model, our method facilitates grasp synthesis guided by task-level and personalized text descriptions without additional manual annotations. Extensive experiments demonstrate that our method achieves not only accurate part-level grasp control but also comparable performance in grasp quality.\n1. Introduction\nModeling hand grasps have recently gained extensive attention due to its wide applications in human-computer interaction [24], virtual reality [11, 39], and imitation learning in robotics [12]. To predict plausible human-like grasp poses when given an object, many hand-object interaction datasets [4, 5, 8, 9] has been built to promote research [19, 20, 32] on learning human experience in recent years. However, these works concentrate on stable grasps that are not suitable for task-oriented grasps. Different tasks necessitate specific types of grasps. For instance, in a cutting task, people typically grasp a knife by its handle rather than the blade. Similarly, when handing over a knife, it is safer for the deliverer to hold the blade, minimizing the risk of injury to the receiver. Consequently, controllable grasp synthesis\n*cgsmalcloud83@mail.dlut.edu.cn †Corresponding author, lslwf@dlut.edu.cn\nInput Output Input Output\nTemplate Text\nPersonalized Text\nTask-level Text\nGrasp the bar of the eyeglasses. Grasp the cap of the headphones.\nI want to drink using mug.\nHold onto the bottle’s cap firmly.\nSeize the pen by its body.\nI want to cut apple using knife.\nInput Output Input Output\nInput Output Input Output\nFigure 1. Given an object, Text2Grasp can generate specific hand grasps by interpreting various text inputs: a) Template text. b) Personalized text. c) Task-level text.\nis of paramount important.\nTo facilitate controllable synthesis, many studies [2, 14, 32, 41] introduce various datasets of grasp containing different numbers of human intentions, such as use, pass, twist and so on. Furthermore, [41] and [14] translate these intentions into one-hot embeddings, combining them with object point cloud feature to achieve intention-guided grasp synthesis. Considering that language is more natural mode of interaction, some studies [23, 31, 34] start to employ tasklevel text descriptions as inputs for predicting 6-Dof pose of parallel jaw gripper. However, utilizing fixed set of intentions or task-level text descriptions for grasping inherently faces ambiguity, primarily in two aspects: 1) Same intention but different grasps. For instance, ”lifting a mug” may involve different grasp types of either grasping the handle or the body. 2) Different intentions but same grasps. For instance, ”lifting” or” twisting” might have same initial grasping pose to hold a bottle’s neck. Such complexities\narXiv:2404.15189v1 [cs.AI] 9 Apr 2024\n\n\n increase the difficulty in annotating datasets and achieving model convergence. To overcome these limitations, we propose a grasp synthesis method abbreviated as Text2Grasp that is guided by text prompts of object grasping parts, rather than intentions or task descriptions that are unable to explicitly indicate which part of the object to grasp. Text2Grasp takes an object and a predefined text template: Grasp the [Object Part] of the [Object Category] as input, and generates a grasp pose targeting the specified part of the object for manipulation. This part-level guidance reduces uncertainty compared to intent-based or task-level guidance, facilitating better convergence of the grasp generation network. Specifically, we present a two-stage method that includes a text-guided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. Unlike all-finger optimization approaches that prioritize maximum object-finger contact [5]—resulting in mainly closed-finger grasps—our optimization emphasizes optimizing contact between the fingers and the object part specified by text description. This strategy ensures physical realism, diversity in grasps, and alignment with text. Furthermore, the template representation of text prompts for the object grasping parts also supports grasp synthesis guided by task-level and personalized text prompts, since LLM [3] has been able to divide the task descriptions into several execution steps, including the parts of the object that should be grasped. Subsequently, our Text2Grasp can generate task-level grasps taking the inference results of LLM as input. Moreover, the utilization of LLM allows for the expansion of our designed text templates, enriching the training dataset for personalized text descriptions. In summary, our contributions are as follows: • We propose Text2Grasp, a grasp synthesis method guided by text prompts of object grasping parts, offering a more natural interaction and precise grasp control. • We introduce a two-stage method that includes a textguided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. • By leveraging LLM, our method facilitates grasp synthesis guided by task-level and personalized text descriptions without additional manual annotations. Extensive experiments on public datasets demonstrate that our method achieves not only accurate part-level grasp control but also comparable performance to state-of-the-art methods in terms of grasp quality.\n2. Related Work\nThere has been a significant amount of research in the field of grasp synthesis. Here, we focus on realistic human grasp synthesis and review the most relevant works. Based\non whether the grasp generation is controllable, we categorize the synthesis algorithms into two types: Uncontrolled Grasp Synthesis and Controllable Grasp Synthesis.\nUncontrolled Grasp Synthesis. Uncontrolled grasp synthesis primarily aims to generate hand pose capable of stably grasp objects without considering subsequent tasks. A trend has emerged to develop deep learning solutions, driven by the introduction of large-scale datasets of handobject interactions [2,5,8,9,14,32,41]. These methods learn the latent distribution of hand-object contact information or hand parameters through generative models, including Generative Adversarial Network (GAN) [7] and Conditional Variational Auto-Encoder (CVAE) [30]. GanHand [5] initially predicts the optimal grasp type from a taxonomy of 33 classes, and then employs a discriminator and an optimization to get a refined grasp. Instead of predicting MANO [29] parameters directly, ContactDB [1] use thermal cameras to capture object contact maps that reflect the contact regions of an object post-grasping and utilizes GAN to learn their distribution, facilitating grasp synthesis.\nComparing with GAN [7], CVAE [30] are more popular in hand grasp synthesis because of its simple structure and one-step sampling procedure. GrabNet [32] utilizes CVAE by conditioning on the Basis Point Set [25] of objects and samples from the low-dimensional space mapped through CVAE to generate hand grasps. Additionally, it incorporates a neural network to refine the coarse pose. This approach is also followed by Oakink [41] and AffordPose [14]. Grasp Field [17] and HALO [16] learn an implicit grasping field using CVAE as the hand representation to produce highfidelity hand surface. GraspTTA [15] exploits the contact map introduced by ContactDB [1] to refine the grasps generated by CVAE during reference. Contact2Grasp [19] learns the distribution of contact map for grasps by CVAE and then maps the contact to grasps. Moreover, ContactGen [20] introduces a three-component model to represent the contact of hand-object: the contact probability, the specific hand part making contact, and the orientation of the touch, and a sequential VAE is proposed to learn these aspects for grasp synthesis. Despite its simplicity and direct sampling process, CVAE often suffer from the posterior collapse [13, 37, 43]. This leads to less diverse outputs, including simplistic samples like a slightly closed hand shape. To mitigate this problem, SceneDiffuser [13], UGG [21] and DexDiffuser [38] employ a diffusion-based denoising process, ensuring diverse sample generation by gradually denoising, thus avoiding direct latent space mapping.\nThese aforementioned methods are capable of generating stable grasps. However, these grasps might not be consistent with human manipulation habits, making them less appropriate for the tasks. Consequently, instead of solely relying on object shape as input, we incorporate the text prompts of object grasping parts into diffusion model for\n\n\n controllable grasp synthesis. Moreover, in contrast to methods that utilize global optimization [5, 32] to refine grasps, our work introduces an optimization based on finger perception and object part perception. This strategy not only ensures grasp stability but also maintains diversity.\nControllable Grasp Synthesis. The capacity for controllable grasping is crucial as it represents the first step for manipulation. To facilitate controllable grasp synthesis, datasets [2, 14, 32, 41] encompass a range of human intentions for dexterous hand grasping. ContactPose [2] identifies two basic intentions: use and pass. Expanding on this, GrabNet [32] introduces lifting and off-hand passing. OakInk [41] goes further by incorporating intentions such as holding and receiving. AffordPose [14] elaborates on the use intention, creating hand-centric categories like twisting, pulling, handle grasping, among eight total intentions. Additionally, to generate intent-driven grasps, OakInk [41] and AffordPose [14] translate these intentions into word embeddings, combining them with object point cloud features as the condition of CVAE to produce matching grasp pose. Considering that language is one of the most natural forms of human interaction, some studies employ task-level text descriptions as inputs for predicting grasps with parallel jaw gripper. These methods initially construct extensive datasets of grasps that include task-level text descriptions. Based on these datasets, [31] and [33] adopt a generatethen-select methodology. It involves initially generating a number of poses for parallel jaw gripper, followed by a selection process guided by task-level text descriptions. In contrast, [34] and [23] directly predict the position of gripper on the input RGB image or object point cloud based on task-level text description guidance. Comparing to the simple closing of a gripper, the human hand, with its higher degree of freedom, must not only ensure stable grasping but also maintain the rationality of itself and interaction, making grasp synthesis for it more challenging. These methods, utilizing fixed set of intentions or tasklevel text descriptions for grasping, inherently face ambiguity, especially when defining intentions or tasks for identical parts of an object, such as a mug’s handle and body. To address this, we develop a grasp synthesis method guided by text prompts of object grasping parts. Compared to the ambiguity of intentions or task-level guidance, partlevel guidance offers lower uncertainty, which facilitates the convergence of grasp synthesis networks. Furthermore, our method contrasts with those requiring manual labeling [23,31,33,34], by leveraging Large Language Model [3] to facilitate grasp synthesis guided by task-level and personalized text descriptions without additional manual labels.\n3. Methods\nOur aim is to achieve controllable grasp synthesis when given an object’s point cloud and a text prompt of object\ngrasping part, ensuring the generated hand grasps stably hold the object while aligning with the input text. To this end, we introduce a two-stage method that includes a textguided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. The overview of our method is illustrated in Fig. 2. In this section, we first present our semi-automated text generation method in 3.1. We then detail the text-guided conditional diffusion model-TextGraspDiff in 3.2, and the hand-object contact optimization in Section 3.3.\n3.1. Semi-automatic Text Generation for Grasp\nThe key idea behind Text2Grasp is to leverage text prompts of object grasping parts to control grasp synthesis. Instead of relying on extensive manual annotations, which are extremely labor-intensive and time-consuming, we design a semi-automatic approach to generate text prompts for existing hand grasp dataset, as illustrated in Fig. 2. First, we predefined the text template, i.e., Grasp the [Object Part] of the [Object Category]. The object category can be directly provided by existing datasets, while the object part corresponding to each grasp can be determined through computation. Specifically, given the point cloud of an object and the hand mesh grasping it, we first calculate the contact between the object and the hand, assigning a contact label to each point on the object. And the “Object Part” label for each grasp is determined by the object part with the most contact points. Finally, we can generate a text template for each grasp in the datasets. Furthermore, we leverage Large Language Model [3] with strong text comprehension capabilities to expand the template text, thereby generating more personalized text descriptions. For example, given the prompt “Please write [N] sentences with the same meanings as [template].”, where N is the number of generated text descriptions, LLM can then infer a variety of plausible text descriptions to form our candidate text list L. During training, we randomly select one description from L as a training label for each grasp. This semi-automatic text generation approach facilitates personalized text inputs, thereby enhancing the flexibility of grasp synthesis control. In addition, the representation of the text prompts for the object grasping parts gives our method the ability to achieve task-level grasp synthesis because Large Language Model [3] can provide a description of the grasping action from a task description, such as grasping the mug’s handle for drinking task. Thus we can accomplish task-level grasp synthesis without extra training.\n3.2. Text to Grasp via Diffusion Model\nIn this section, we introduce TextGraspDiff, a conditional diffusion model for grasp synthesis that is guided by\n\n\n Text\n“Grasp the handle of the mug.”\nClip\nPointNet++\nDiffuse Process\ngt\nTime Embedding\n...\nResBlock Attention Conv\ng0\nt\nMulti-Modal Attention\ng0 gt−1\nQ KV\n...\nDenoising Network G\nObject Part Perception\nFinger Perception\nText\nTextSegNet\nTextGraspDiff Optimization\n11100\nFinger Vector\no\nPose Shape HO_distance Finger Vector\nGrasp Vector\nsteps\nT\nl\nSemi-automatic Text Generation\nContact Points “Grasp the\nPart of the\nObject.”\nPart Category\nObject Category\nTemplate Text\nLLM\nPrompt\n“More descriptions.”\nPersonalized Text\n\"1. Take hold of the mug's handle.\" \"2. Hold on the mug's handle firmly.\" \"3. Grip the handle of the mug.\" “4. ......”\nL\nFigure 2. The Overview of Text2Grasp. We present a semi-automatic approach to generate both the template text and the personalized text prompts for each grasp in the datasets, which are used to train TexGraspDiff. And given the point cloud of object and text description of object grasping parts, we introduce a two-stage method that includes a text-guided diffusion model TextGraspDiff to first generate a coarse grasp pose, then apply a hand-object contact optimization process to ensure both plausibility and diversity. The final hand mesh can be obtained by MANO model [29].\ntext prompts of object grasping part. The overview of our method is illustrated in Fig. 2. Taking the object point cloud o ∈ RN×3 and the part-level text prompts l, TextGraspDiff outputs a hand grasp vector g ∈ R66. This grasp vector encompasses MANO [29] model pose gθ ∈ R48, shape gβ ∈ R10, the distance gdis ∈ R3 between object and hand centroids, and the finger vector gf ∈ R5, which indicates which fingers are being used for grasping. Adhering to the diffusion model outlined in [10], our method is comprised of both a forward process and a reverse process. Forward process. Given a grasp vector g0, sampling from the ground-truth data distribution, we add the infinitesimal Gaussian noise εt ∼ N (0, βtI) into g0 and get a sequence of noised data {gi}tT=1 after T step. βt adheres to a linear variance schedule.\nq(gt | g0) = N (gt; √α ̄tg0, (1 − α ̄t)I) (1)\nwhere αt = 1 − βt, αt = Qt\ns=1 αs. After T steps, if the amount of noise added is sufficiently large, then gT approximately converges to a standard Gaussian distribution. Reverse process. The process reverses noise sampled from a Gaussian distribution back into a sample from the data distribution for a fixed timestep. In our work, with\nthe grasp vector g0 as the target for denoising process, and object point cloud o and its text prompt l of object grasping part as conditions, the conditional diffusion model leads to p(gt−1|gt, o, l). Following [27, 35], we predict the grasp vector g0 using a neural network Gθ. This process can be formalized as:\np gt−1| gt, o, l = N gt−1; μθ(gt, o, l, t), βetI (2)\nμ ̃θ(gt, o, l) =\n√α ̄t−1βt 1 − αt\nGθ(gt, o, l, t) +\n√αt(1 − αt−1)\n1 − αt\ngt\n(3) The detailed structure of the denoising network Gθ is shown in Fig. 2, we employ a Transformer [36] as the denoising network’s backbone, which has demonstrated promising results in human motion synthesis [27, 35] and robotic hand grasp synthesis [13, 21]. For multi-condition inputs, including point clouds and text, we initially employ the PointNet++ [26] and the pretrained CLIP [28] model as respective encoders to extract the point cloud feature and text feature. Instead of simply adding these multi-modal features, we design a Multi-Modal Attention Module based on Transformer [36] for effective fusion, leveraging point cloud feature fp as the query and text feature fl as the\n\n\n key and the value. This fusion mechanism enables us to achieve more precise control over grasp locations. Following [13], we incorporate a timestep-residual block and cross-attention for input noise embedding feature-condition fusion to ensure the network is effectively guided by step t and the condition c. Finally, the grasp vector g0 can be predicted by final output layer. The loss function of the network Gθ is:\nL = Eg0∼q(g0|o,l),t∼[1,T ][∥g0 − Gθ(gt, t, o, l)∥2\n2] (4)\nAfter completing the training of the denoising network Gθ , when given a new object’s point cloud and text description as conditions, we first sample random noise from a Gaussian distribution, then apply the denoising network Gθ and Eq. (2) and Eq. (3) over T steps, and finally we obtain the grasp vector matching the object’s part-level text description. The grasp hand mesh is then generated by applying the final grasp vector to the MANO model [29].\n3.3. Text-guided Contact Optimization\nTo produce physically more plausible grasps, many works [5, 9, 15, 19] introduce a refinement stage to enhance contact and minimize penetration. Their main focus is on stable grasping by aligning hand with the closest object surface points, but these points may not match the text-described object parts, potentially leading to inaccurate grasp locations. Therefore, we propose a text-guided contact optimization method based on finger perception and text-guided object part perception. It guides specific fingers toward the object part described by text description, further enhancing grasp stability, diversity, and grasp part accuracy. Hand finger perception. Rather than minimizing the distance between all prior hand contact vertices often utilized for grasping and object points, we specifically optimize the distance between the object and the particular fingers used for grasping. We utilize a five-dimensional finger vector to define which fingers are used in grasping the object. For instance, as shown in Fig. 3, if the grasp involves using the thumb, index, and middle finger, then the finger vector gf is [1,1,1,0,0]. Following human habits, this vector is generated alongside the grasp. During optimization, we minimize the distance only between the object and those fingers indicated by a 1 in the finger vector, avoiding the issue of all fingers contacting the object. The loss for the finger perception optimization is formulated as:\nHc =\n5\n[\ni=1\n{Ci | gi\nf = 1} (5)\nL\nhc (Hc, O) = 1\n| Hc |\nX\nh∈Hc\nmkin ∥h, Ok∥ (6)\nOptimization w/ finger perception\nOptimization w/ object part perception\nFigure 3. The Contact optimization. The contact optimization consists of finger perception and object part perception. The finger perception optimization directs the particular fingers used for grasping towards object and the object part optimization guided fingers toward the object part specified by text.\nwhere Ci represents the set of hand vertices which belongs to the ith fingertips, statistics by [9]. And Hc denotes the set of points for all fingers making contact.\nText-guided object part perception. As shown in Fig. 3 we minimize the distance between hand contact points and object part specified by the text input to guide hand fingers to grasp the correct object part. Specifically, using a pretrained text-guided segmentation network TextSegNet, we first segment the input object point cloud into targeted Oc and non-targeted parts Onc based on the input text prompts of object grasping part. During the optimization, we assign higher weights to the targeted part, directing the hand contact points toward it to enhance the accuracy of the grasp part. The hand-object contact loss is formulated as follows:\nLc(Hc, O) = λ1Lnc(Hc, Oc) + λ2Lnc(Hc, Onc) (7)\nwhere λ1 and λ2 are hyperparameters. Oc = {pi ∈ O| Fseg(pi, l) = 1} and Onc = {pi ∈ O| Fgcd (pi, l) = 0} respectively represent the targeted and non-targeted parts. Fseg is the pretrained text-guided segmentation network TextSegNet. We also use PointNet++ [26] and CLIP [28] for point cloud and text encoding, followed by a multi-layer fully connected network to output segmentation labels. The training loss utilizes negative log-likelihood loss. Others. Following [5, 9], we minimize the object points that are inside the hand distance to their closest hand surface points to penalize hand and object interpenetration by Lptr. Furthermore, following [44], we incorporate a joint angle limitation loss Langle and a self-collision loss Lself for the hand to ensure the plausibility of the grasping hand pose. Ultimately, our overall optimization objective is formulated as follows:\nmin\ngθ ,gβ ,gdis\nλcLc+λptrLptr +λangleLangle+λself Lself (8)\n\n\n where λc , λptr , λangle and λself is a hyper-parameter. We utilize this objective function to optimize the networkpredicted MANO model pose gθ , shape gβ, the distance gdis between object and hand centroids, aiming to further enhance the quality of generated grasps and the part accuracy of grasp .\n4. Experiments\nIn this section, we demonstrate the performance of our proposed PLAN-Grasp. We first introduce our implementation details in Sec. 4.1, followed by the used datasets and evaluation metrics in Sec. 4.2 and Sec. 4.3, respectively. In Sec. 4.4, we compare our method with the state-of-the-art methods and various applications that we can support. Finally, in Sec. 4.5, we conduct ablation studies to verify the effectiveness of components we design.\n4.1. Implementation Details\nWe conduct all the experiments using a single NVIDIA GeForce RTX4090 GPU with 24G memory. We sample N=2048 points sampling from the object surface as the input object points. During the training, we use the Adam optimizer [18] with the learning rate of 1e-4 to train the denoising network LAN-GraspDiff for 1000 epochs. The training batch size is 64. Following Scenediffuser [13], we set the diffusion step T to 100 , which is enough for a single 3D hand pose. During the refinement stage, we utilize Adamax [18] to optimize the grasp vector, applying different learning rates for its components: 1e-2 for hand pose, 1e-5 for hand shape, and 1e-4 for the distance between hand and object centroids, across a total of 200 epochs.\n4.2. Dataset\nOakInk. The OakInk [41] is a large-scale dataset that captures hand-object interactions oriented around 5 intents: use, hold, lift-up, hand-out, and receive. It provides 1800 object models of 32 categories with their part labels and interacting hand poses. We use the shape-based subset OakShape to conduct experiments, 1308 objects for training and 183 objects for evaluation. AffordPose. The AffordPose [14] is a large dataset of hand-object interactions with 8 affordance-driven labels such as twist, lift, and press. It comprises 641 objects from 13 categories in PartNet [22] and PartNet-Mobility [40]. To evaluate the generalization ability of our method, we select 6 object categories identical to OakInk [41]: bottle, disperser, earphone, knife, mug and scissors, and randomly chose 30 instances from each category for testing.\n4.3. Metrics\nA superior text-guided grasp should not only securely hold the object but also grasp the correct object part spec\nified by text prompts. In this work, we adopt 4 metrics in total cover both grasp quality and grasp part accuracy. Penetration. Following [19, 41, 42], we compute the Penetration Depth and the Solid Intersection Volume between hand and object to measure the hand-object penetration. The PD is the maximum distance of all the penetrated hand vertices to their closet object surface, and the SIV is calculated by summing the volume of object voxels that are inside the hand surface.\nSimulation Displacement. Following [9, 20, 41], we place the object and the predicted hand into simulator [6], and calculate the displacement of the object center over a period of time by applying gravity to the object. Diversity. Following [16, 20], we measure the diversity by clustering generated grasps into 20 clusters using K-means, and calculate the entropy of the cluster assignments and the average cluster size. Grasp Part Accuracy. Employing the approach introduced in Sec. 3.2, we assign text template to each generated grasp and determine their accuracy by comparing with the input text descriptions. Grasp Part Accuracy is defined as the ratio of correctly identified grasps to the overall number of the generated grasps.\n4.4. Comparison with the State-Of-The-Arts\nTo evaluate the controllability of grasp synthesis in our method, we utilize two class-level public datasets, OakInk [41] and AffordPose [14], comprising multiple categories and instances within each category. Instance-level datasets like Grab [32] and HO3D [8] are unsuitable for evaluating our method because they contain only one instance per category, and only achieve controllability on a single instance cannot verify our generalization ability. For a fair comparison, we compare the state-of-the-art method trained on OakInk [41]: GrabNet [32], which is used for grasp generation in newest work [14, 41]. We train it on the OakInk dataset using its officially released code and test it and our method on 183 unseen objects from the OakInk dataset and 180 out-of-domain objects from the AffordPose dataset. Following [16,20], we generate 20 hand grasps for each test object. Specifically for our methods, we randomly create 20 text prompts based on the parts of each test object, using these prompts and the objects as inputs to produce grasps. We first present the quantitative comparison results on the in-domain OakInk [41] dataset and the out-of-domain AffordPose [14] dataset as shown in Tab. 1. It can be seen that our method achieves the lower penetration and simulation displacement on the OakInk dataset indicating the higher grasp quality than GrabNet [32]. Besides, our results are close to and even outperform the ground truth in diversity that demonstrate our method achieves more diverse and natural grasps. Experimental results on AffordPose [14] Dataset demonstrate that our method achieves the\n\n\n Dataset Methods Penetration Simulation\nDisplacement Mean ± Var↓\nDiversity Part Accuracy↑ Depth↓ Volume↓ Entropy↑ Cluster Size↑\nOakInk\nTestGT 0.11 0.65 1.80 ± 2.04 2.91 4.11 100.00 GrabNet [32] 0.48 2.97 2.84 ± 2.81 2.95 2.57 Ourstemplate 0.40 1.89 2.49 ± 2.51 2.92 4.70 87.76 Ourspersonalized 0.41 1.73 2.49 ± 2.57 2.92 4.74 82.32\nAffordPose\nGrabNet [32] 0.54 3.77 3.09 ± 2.74 2.94 2.52 Ourstemplate 0.66 5.05 2.93 ± 2.67 2.90 4.88 78.53 Ourspersonalized 0.59 3.84 3.00 ± 2.86 2.87 4.79 73.83\nTable 1. The quantitative results on the OakInk [41] dataset and the AffordPose [14] dataset.TestGT means the grouth-truth grasps on the OakInk Test datasets. Ourstemplate and Ourspersonalized refer to the grasps generated when using template and personalized text description inputs, respectively. ↑ denotes higher values are better, ↓ denotes lower values are better.\nOurs\nGrabNet\nGrabNet\nOurs\nFigure 4. The qualitative results on the OakInk [41] dataset and the AffordPose [14] dataset. The results demonstrated above the dotted line are from OakInk [41] dataset, while below are from AffordPose [14] dataset.\ncomparable generalization ability, with the lower simulation displacement, higher diversity, and comparable penetration. More importantly, we achieve the grasp controllability by text prompts of object grasping parts, a capability not present in GrabNet [32] and get grasp part accuracy of 87.76% with template text and 82.32% with personalized text on OakInk dataset [41] as shown in Tab. 1. Furthermore, to evaluate the performance qualitatively, we visualize the generated hand grasps for both in-domain and out-of-domain objects by GrabNet [32] and our method. As shown in Fig. 4, it can be seen that both methods can generate plausible hand grasps for object part with sample shapes such as the body of mug and the cap of earphones.\nBut for specific-part grasp such as the cap of bottle, we can observe clearly from the red boxes that the grasps our method generated has the smaller penetration and more natural contact. These results demonstrate our method’s capability to generate physically plausible and stable grasp. Moreover, we visualize the multi-grasps for each object in Fig. 5. In contrast to the predominantly closed hand poses with five fingers generated by GrabNet [32], ours is better suitable for the specific shape of the object, as demonstrated Fig. 5 with the eyeglasses and knife. To show our grasp controllability, we visualize the results of grasp synthesis guided by text prompts of object grasping parts. which consist of template and personalized\n\n\n Ours\nGrabNet\nInput\nbottle eyeglasses headphones knife scissors\nFigure 5. The qualitative results of the diverse grasps on the objects. For each object, we visualize five grasps and the red shape represents abnormal grasps.\nGrasp the cap of the cylinder bottle.\nGrasp the bridge of the eyeglasses.\nGrasp the trigger of the trigger sprayer.\nGrasp the head of the pen.\nGrasp the handle of the mug.\nGrasp the handle of the screwdriver.\nGrasp the handle of the fryingpan.\nGrasp the headband of the headphones.\nGrasp the lotion pump around its head.\nClasp the bottle's cylindrical body firmly.\nHold the knife‘s cutting edge.\nPinch the cap of the bottle.\nHold onto the eyeglasses' frame.\nSeize the panel of the game controller.\nTake hold of the wineglass's stem.\nWrap your fingers around the knife's handle.\nFigure 6. Visualization of the grasps generated when using different types of text inputs. The top row displays grasps produced from template text inputs, while the bottom row exhibits those generated from personalized text inputs.\ntext descriptions. As shown in Fig. 6, our method not only generates hand poses that grasp the objects of different categories in a manner consistent with human habits but also directly produces grasps in a text-controlled manner. This level of controllability enables precise object part grasping for subsequent tasks.\n4.5. Ablation Study\nVAE vs Diffusion. To fairly evaluate the effectiveness of the diffusion model we employed, we construct a variant of our method by replace the diffusion model with VAE for part-level grasp sythesis. Both this model and ours remove the optimization process. The results on the OakInk [41] dataset are shown in the first two rows of Tab. 2 and Fig. 7. From the experimental results we can see diffusion model\nachieves the higher grasp quality with lower penetration and higher grasp part accuracy. More importantly, it can be seen obviously from Fig. 7 that our diffusion model can generate more diverse hand pose for mug and bottle than VAE.\nMulti-Modal Attention. We evaluate the effectiveness of the Multi-Modal Attention which is designed to fuse the text feature and the object point feature. Specifically, we compare this module with feature addition. As shown in the 3rd and the 4th of Tab. 2, the multi-modal attention outperforms feature addition across all metrics, especially in the grasp part accuracy.\nOptimiziation. We evaluate the effectiveness of optimization based on finger perception and text-guided object part perception, and the results are shown in Tab. 2 and Fig. 8. Note that here we use Baseline (Base.) to represent\n\n\n Methods Penetration Simulation\nDisplacement Mean ± Var↓\nDiversity Part Accuracy↑ Depth↓ Volume↓ Entropy↑ Cluster Size↑\nBase.(VAE) 0.55 8.44 2.48±2.56 2.90 3.15 77.38 Base. (Diffusion) 0.38 2.82 3.00±3.04 2.93 3.46 85.25\nBase. w/ Add fusion 0.38 2.89 3.09±2.92 2.90 3.45 83.44 Base. w/ Attention fusion 0.38 2.82 3.00±3.04 2.93 3.46 85.25\nBase. (Diffusion) 0.38 2.82 3.00±3.04 2.93 3.46 85.25 Base. + opt. w/ global 0.39 1.82 2.39±2.34 2.95 4.18 83.69 Base. + opt. w/ finger perception 0.38 1.79 2.50±2.56 2.95 4.41 83.74 Base. + opt. w/ finger perception\nand object part perception 0.40 1.89 2.49±2.51 2.92 4.70 87.76\nBase. + opt. w/ refinenet [32] 0.30 1.38 3.11±2.81 2.87 2.86 83.55\nTable 2. The quantitative results of ablation study on the OakInk [41] dataset. ↑ denotes higher values are better, ↓ denotes lower values are better.\nBase. (VAE) Base. (Diffusion)\nFigure 7. Comparisons of ours based on VAE(Base. (VAE)) and diffusion model (Base. (Diffusion)).\nBaseline Opt. w/o finger perception Opt. w/ finger perception\nBaseline Opt. w/o part perception\nOpt. w/ Refinenet[]\nOpt. w/ part perception Opt. w/ Refinenet[]\nFigure 8. Comparisons of different optimization strategies.\nFigure 9. Visualization of the grasps generated from the text input ’grasp the handle of the faucet’ given an unseen category faucet.\nour method without any optimization. It can be seen the grasp quality achieves a significant improvement by adding the global optimization which leads all-fingers toward object for Baseline. Specifically, the penetration volume and the simulation displacement have decreased by 35.46%, and 20.33% respectively, while diversity has improving by 20.81%. However, it directs all fingers towards the object’s nearest point, limiting the diversity and leading to inaccuracies in the contact part, as shown in Fig. 8. In contrast, our optimization, grounded in finger perception, fine-tunes only the fingers involved in grasping, while the rest maintain a natural state. This approach enables us to maintain grasp quality while achieving greater diversity. Furthermore, as shown in Fig. 8, the optimization based on text-guided object part perception focuses on directly the hand towards the\npart described by text description, enabling us to achieve a higher grasp part accuracy. In addition, we compare our optimization with the RefineNet used by GrabNet [32]. It is trained to denoise on the dataset built by adding random noise into ground-truth hand-object interaction. As the Fig. 8 illustrated, grasps optimized by RefineNet still do not fully contact the blade. In contrast, our method, optimized for specific situations, performs better in detail. And the quantitative results in Tab. 2 demonstrate our methods achieve better balance between penetration and simulation displacement.\n5. Discussion\nThe part control ability of our method can be easily transferred among the objects in the seen categories. However, due to the limited categories in the training dataset, when faced with the objects of new categories that have never been seen in the training dataset, although reasonable grasp can be generated, the contact parts can not be identified because of the lack of understanding of the new categories. As shown in Fig. 9, our method can produce a reasonable grasp\n\n\n for the faucet but cannot accurately grasp the faucet’s handle. Therefore, it is necessary to train on a grasp dataset with more categories, but such a dataset is currently not available. In addition, with the help of the Large Language Models [3], we can achieve task-level static grasp synthesis, such as grasp the handle of the knife rather than the blade when cutting fruit. However, correctly grasping the part of an object is the first step to complete the task and the ability to dynamically manipulate objects is also the key for task. We will do more exploration in the future.\n6. Conclusion\nIn this work, we introduce Text2Grasp, a grasp synthesis method guided by text prompts of object grasping parts. It begins with a text-guided diffusion model, termed TextGraspDiff, which is responsible for generating an initial, coarse grasp pose. This is subsequently refined through a hand-object contact optimization process. This method ensures that the generated grasps are not only physically plausible and diverse but also precisely aimed at specific object parts described by text prompts. Furthermore, our method also supports grasp synthesis guided by personalized text and task-level text descriptions by LLM without extra manual annotations. Extensive experiments conducted on two public datasets demonstrate our methods achieves not only the comparable performance in grasp quality but also precise part-level grasp control.\nReferences\n[1] Samarth Brahmbhatt, Cusuh Ham, Charles C Kemp, and James Hays. Contactdb: Analyzing and predicting grasp contact via thermal imaging. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8709–8719, 2019. 2 [2] Samarth Brahmbhatt, Chengcheng Tang, Christopher D Twigg, Charles C Kemp, and James Hays. Contactpose: A dataset of grasps with object contact and hand pose. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIII 16, pages 361–378. Springer, 2020. 1, 2, 3 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 2, 3, 10 [4] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: A benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9044–9053, 2021. 1\n[5] Enric Corona, Albert Pumarola, Guillem Alenya, Francesc Moreno-Noguer, and Gr ́egory Rogez. Ganhand: Predicting human grasp affordances in multi-object scenes. In Proceed\nings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5031–5041, 2020. 1, 2, 3, 5 [6] Erwin Coumans. Bullet physics simulation. In ACM SIGGRAPH 2015 Courses, page 1. Association for Computing Machinery, 2015. 6 [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020. 2 [8] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3d annotation of hand and object poses. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3196–3206, 2020. 1, 2, 6 [9] Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11807–11816, 2019. 1, 2, 5, 6 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. 4 [11] Markus Ho ̈ll, Markus Oberweger, Clemens Arth, and Vincent Lepetit. Efficient physics-based implementation for realistic hand-object interaction in virtual reality. In 2018 IEEE conference on virtual reality and 3D user interfaces (VR), pages 175–182. IEEE, 2018. 1 [12] Kaijen Hsiao and Tomas Lozano-Perez. Imitation learning of whole-body grasps. In 2006 IEEE/RSJ international conference on intelligent robots and systems, pages 5657–5662. IEEE, 2006. 1 [13] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusionbased generation, optimization, and planning in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16750–16761, 2023. 2, 4, 5, 6 [14] Juntao Jian, Xiuping Liu, Manyi Li, Ruizhen Hu, and Jian Liu. Affordpose: A large-scale dataset of hand-object interactions with affordance-driven hand pose. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14713–14724, 2023. 1, 2, 3, 6, 7 [15] Hanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong Wang. Hand-object contact consistency reasoning for human grasps generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 11107–11116, 2021. 2, 5 [16] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar Hilliges, and Siyu Tang. A skeleton-driven neural occupancy representation for articulated hands. In 2021 International Conference on 3D Vision (3DV), pages 11–21. IEEE, 2021. 2, 6 [17] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael J Black, Krikamol Muandet, and Siyu Tang. Grasping field: Learning implicit representations for human grasps. In 2020 International Conference on 3D Vision (3DV), pages 333–344. IEEE, 2020. 2\n\n\n [18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [19] Haoming Li, Xinzhuo Lin, Yang Zhou, Xiang Li, Yuchi Huo, Jiming Chen, and Qi Ye. Contact2grasp: 3d grasp synthesis via hand-object contact constraint. arXiv preprint arXiv:2210.09245, 2022. 1, 2, 5, 6\n[20] Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, and Shenlong Wang. Contactgen: Generative contact modeling for grasp generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2060920620, 2023. 1, 2, 6 [21] Jiaxin Lu, Hao Kang, Haoxiang Li, Bo Liu, Yiding Yang, Qixing Huang, and Gang Hua. Ugg: Unified generative grasping. arXiv preprint arXiv:2311.16917, 2023. 2, 4\n[22] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A largescale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 909–918, 2019. 6 [23] Toan Nguyen, Minh Nhat Vu, Baoru Huang, Tuan Van Vo, Vy Truong, Ngan Le, Thieu Vo, Bac Le, and Anh Nguyen. Language-conditioned affordance-pose detection in 3d point clouds. arXiv preprint arXiv:2309.10911, 2023. 1, 3\n[24] Nancy S Pollard and Victor Brian Zordan. Physically based grasping control from example. In Proceedings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer animation, pages 311–318, 2005. 1 [25] Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning on point clouds with basis point sets. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4332–4341, 2019. 2 [26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. 4, 5\n[27] Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, and Shuicheng Yan. Diffdance: Cascaded human motion diffusion model for dance generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 1374–1382, 2023. 4 [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 4, 5 [29] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands. ACM Transactions on Graphics, 36(6):1–17, 2017. 2, 4, 5 [30] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015. 2 [31] Yaoxian Song, Penglei Sun, Yi Ren, Yu Zheng, and Yue Zhang. Learning 6-dof fine-grained grasp detec\ntion based on part affordance grounding. arXiv preprint arXiv:2301.11564, 2023. 1, 3\n[32] Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas. Grab: A dataset of whole-body human grasping of objects. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16, pages 581–600. Springer, 2020. 1, 2, 3, 6, 7, 9 [33] Chao Tang, Dehao Huang, Wenqi Ge, Weiyu Liu, and Hong Zhang. Graspgpt: Leveraging semantic knowledge from a large language model for task-oriented grasping. IEEE Robotics and Automation Letters, 2023. 3\n[34] Chao Tang, Dehao Huang, Lingxiao Meng, Weiyu Liu, and Hong Zhang. Task-oriented grasp prediction with visuallanguage inputs. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 48814888. IEEE, 2023. 1, 3 [35] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. 4\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4\n[37] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9401–9411, 2021. 2 [38] Zehang Weng, Haofei Lu, Danica Kragic, and Jens Lundell. Dexdiffuser: Generating dexterous grasps with diffusion models. arXiv preprint arXiv:2402.02989, 2024. 2\n[39] Min-Yu Wu, Pai-Wen Ting, Ya-Hui Tang, En-Te Chou, and Li-Chen Fu. Hand pose estimation in object-interaction based on deep learning for virtual reality applications. Journal of Visual Communication and Image Representation, 70:102802, 2020. 1 [40] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1109711107, 2020. 6 [41] Lixin Yang, Kailin Li, Xinyu Zhan, Fei Wu, Anran Xu, Liu Liu, and Cewu Lu. Oakink: A large-scale knowledge repository for understanding hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20953–20962, 2022. 1, 2, 3, 6, 7, 8, 9 [42] Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, and Cewu Lu. Cpf: Learning a contact potential field to model the hand-object interaction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11097–11106, 2021. 6 [43] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, Au\n\n\n gust 23–28, 2020, Proceedings, Part IX 16, pages 346–364. Springer, 2020. 2 [44] Tianqiang Zhu, Rina Wu, Jinglue Hang, Xiangbo Lin, and Yi Sun. Toward human-like grasp: Functional grasp by dexterous robotic hand via object-hand semantic representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 5\n\n\n Appendix\nGrasp the head of the lotion pump\nGrasp the body of the lotion pump\nGrasp the body of\nthe lotion pump Grasp the body of\nthe cylinder bottle\nGrasp the cap of the cylinder bottle\nGrasp the body of the cylinder bottle\nGrasp the handle of the knife\nGrasp the blade of the knife\nGrasp the blade of the knife\nGrasp the cap of the pen\nGrasp the body of the pen\nGrasp the body of the pen\nGrasp the cap of the pen\nGrasp the body of the bowl\nGrasp the body of the cylinder bottle\nGrasp the body of the cylinder bottle\nGrasp the cap of the cylinder bottle\nGrasp the cap of the cylinder bottle\nGrasp the bar of the eyeglasses\nGrasp the bridge of the eyeglasses\nGrasp the bridge of the eyeglasses\nGrasp the eyepiece of the binoculars\nGrasp the lens of the binoculars\nGrasp the handles of the gamecontroller\nGrasp the pannel of the gamecontroller\nGrasp the body of the phone\nGrasp the body of the phone\nGrasp the body of the phone\nGrasp the body of the apple\nGrasp the body of the apple\nGrasp the body of the banana\nGrasp the body of the banana\nGrasp the tip of the screwdriver\nGrasp the handle of the screwdriver\nGrasp the grip of the hammer\nFigure A1. Visualization of the grasps generated when using template text inputs.\n\n\n Get a firm hold on the trigger sprayer's body\nSecurely clutch the trigger of the trigger sprayer\nSecurely clutch the lotion pump's top portion\nSecurely clutch the lotion pump's main portion\nGrasp the lotion pump around its head\nGet a firm hold on the cylinder bottle's body\nClench the bottle's cap firmly\nSeize the knife's blade securely\nEmbrace the knife's handle\nHold the cap of the pen tightly\nBe certain to grip the pen's main portion\nTake hold of the pen's cap\nGet a firm hold on the pen's body\nSecurely clutch the pen's body\nEnsure you have a solid grasp on the scissors' handle\nGet a firm hold on the scissors' blades\nWrap your fingers around the scissors' handle\nHold onto the scissors' handle\nGrip the cap of the headphones\nClench the headphones' cap securely\nGrip the headband of the headphones\nSqueeze the cap of the cylinder bottle\nSecurely clutch the main portionof the cylinder bottle\nTake a firm grip on the bottle's cap\nGet a firm hold on the cylinder bottle's body\nGrasp the body of the bottle in your hand\nWrap your fingers around the body of the mug firmly\nHold the mug by its handle\nGrasp the bowl\naround its body Grasping the bowl's body,\nlift it carefully\nSeize the camera by its panel\nHold the camera securely around its body\nGrasp the body of the banana\nGrasp the body of the banana\nClasp the wineglass's bowl firmly\nSeize the wineglass by its stem\nSecurely grip the frying pan's handle\nClasp the tip of the screwdriver firmly\nClench the hammer's head tightly\nClench the eyeglasses' bar securely\nClench the eyeglasses' bridge confidently\nWrap your fingers around the eyeglasses' bridge\nFigure A2. Visualization of the grasps generated when using personalized text inputs."}