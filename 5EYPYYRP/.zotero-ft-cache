GAPartManip: A Large-scale Part-centric Dataset for
Material-Agnostic Articulated Object Manipulation
Wenbo Cui* 1,2, Chengyang Zhao* 3,4, Songlin Wei* 3,6, Jiazhao Zhang3,6, Haoran Geng3,5, Yaran Chen1, He Wang† 2,3,6
Abstract— Effectively manipulating articulated objects in household scenarios is a crucial step toward achieving general embodied artificial intelligence. Mainstream research in 3D vision has primarily focused on manipulation through depth perception and pose detection. However, in real-world environments, these methods often face challenges due to imperfect depth perception, such as with transparent lids and reflective handles. Moreover, they generally lack the diversity in partbased interactions required for flexible and adaptable manipulation. To address these challenges, we introduced a largescale part-centric dataset for articulated object manipulation that features both photo-realistic material randomizations and detailed annotations of part-oriented, scene-level actionable interaction poses. We evaluated the effectiveness of our dataset by integrating it with several state-of-the-art methods for depth estimation and interaction pose prediction. Additionally, we proposed a novel modular framework that delivers superior and robust performance for generalizable articulated object manipulation. Our extensive experiments demonstrate that our dataset significantly improves the performance of depth perception and actionable interaction pose prediction in both simulation and real-world scenarios.
I. INTRODUCTION
Articulated objects are ubiquitous in people’s daily lives, ranging from tabletop items like microwaves and kitchen pots to larger items like cabinets and washing machines. Unlike simple, single-function rigid objects, articulated objects consist of multiple parts with different functions, featuring varied geometric shapes and kinematic structures, making generalizable perception and manipulation towards them highly non-trivial [1]. Some existing works tried to simplify this problem by developing intermediate representations to encode the similarities across different objects implicitly, such as affordance [2]–[5] and motion flow [6]–[8], thereby achieving generalization across objects. Another series of work [9]–[11] tried to tackle the articulated object perception and manipulation based on a more explicit and fundamental concept called Generalizable and Actionable Part (GAPart), demonstrating more manipulation capabilities attributed to its 7-DoF pose representation compared to value map representation of visual affordance. However, we observe that two critical limitations impede their real-world performance.
*Equal Contribution. 1Institute of Automation, Chinese Academy of Sciences, 2Beijing Academy of Artificial Intelligence, 3CFCS, School of Computer Science, Peking University, 4Carnegie Mellon University, 5University of California, Berkeley, 6Galbot, †Corresponding to hewang@pku.edu.cn.
Table
Part-level Grasping Poses (8B actionable poses)
Box
Door
Microwave
Cabinet
Refrigerator
WashingMachine
Toilet Washing Machine
Left/Right IR RGB Depth Mask RGB-D Active Stereo Camera (240K images)
Grasping Poses
Observations
Sim-to-Real Transfer
Laptop
SuitCase
...
Fig. 1. GAPartManip. We introduce a large-scale part-centric dataset for material-agnostic articulated object manipulation. It encompasses 19 common household articulated categories, totaling 918 object instances, 240k photo-realistic rendering images, and 8 billion scene-level actionable interaction poses. GAPartManip enables robust zero-shot sim-to-real transfer for accomplishing articulated object manipulation tasks.
Firstly, the material of articulated objects significantly impacts the quality of point cloud data. Most existing work relies on point clouds, and these methods struggle due to the sim-to-real gap of depth estimation [9], [10], [12], [13]. Some neural-based stereo-matching depth reconstruction methods are proposed and show some success on rigid objects [14], [15]. These methods use neural networks to encode the disparity in stereo infrared (IR) patterns projected by structured light cameras. However, due to the limited diversity in the stereo IR dataset, these methods are constrained to small rigid objects and perform poorly on large articulated objects.
Secondly, there is no method that can predict stable and actionable interactive poses across categories for articulated objects. Some work employs heuristic-based methods [9] to interact with articulated objects, but it is limited in diversity and fails to account for the geometric details necessary for robust interactions in real-world settings [3]. Some methods for rigid objects grasping pose prediction can generate stable
arXiv:2411.18276v1 [cs.RO] 27 Nov 2024


poses. However, due to the lack of data on articulated objects, it is challenging to discern whether each link can interact independently, resulting in poses that are mostly nonactionable [16]. Affordance-based methods [2], [13], [17] receive widespread attention for interacting with articulated objects by generating heatmaps. However, these heatmaps are ambiguous, hard to annotate and struggle to produce stable grasping interactive poses [12]. In this paper, we address these limitations from a datacentric perspective. We introduce GAPartManip, a novel large-scale synthetic dataset that features two important aspects: (1) realistic, physics-based IR image rendering of various parts in diverse scenes, and (2) part-oriented actionable interaction pose annotations for a wide range of articulated objects. Our GAPartManip inherits 918 object instances across 19 categories from the previous GAPartNet dataset [9]. By leveraging these assets, we developed a data generation pipeline for part manipulation, producing the synthetic data needed to address the previously mentioned limitations. To improve generalizability and mitigate the simto-real gap, we incorporate domain randomization techniques [15] during data generation, ensuring a diverse range of outputs. In total, our dataset contains approximately 14000 scene-level samples with 8 billion part-oriented actionable pose annotations, encompassing a wide array of physical materials, object states, and camera perspectives. Trained on the proposed dataset, we can obtain a depth reconstruction network and an actionable pose prediction network separately to address the two limitations mentioned earlier. Moreover, we compose these two neural networks modular to a novel articulated object manipulation framework. Through extensive experiments on both synthetic and real worlds, our method achieves state-of-the-art (SOTA) performance in both individual module experiments and part manipulation experiments. To summarize, our main contributions are as follows: • We introduce GAPartManip, a novel large-scale dataset of various articulated objects featuring realistic, physicsbased rendering and diverse scene-level, part-oriented actionable interaction pose annotations. • We propose a novel framework for articulated object manipulation and evaluate each module separately, demonstrating superior effectiveness and robustness compared to baseline methods. • We conduct comprehensive experiments in the real world and achieve SOTA performance on articulated object manipulation tasks.
II. RELATED WORK
A. Articulated object dataset
Articulated object dataset and modeling is a crucial and longstanding research field in 3D vision and robotics, encompassing a wide range of work in perception [9], [18]–[23], generation [24]–[28], and manipulation [9]–[11], [29]–[33]. As to manipulation dataset, GAPartNet [9] annotates 6-DoF part pose to manipulate parts. Graspnet [34] and Contactgrasp [35] build several datasets, but these datasets all focus
on rigid objects, neglecting the kinematic semantics specific to articulated objects. Where2act [2] first introduces a data generation pipeline for articulated objects, and they generate data by sampling successful poses in the simulator. AOGrasp [16] leverages a curvature-based sampling method to accelerate data collection efficiency and proposes an 87k dataset of actionable poses. RPMart [12] manually annotated affordance maps for articulated objects and provided rendered data in SAPIEN [1]. None of the current datasets provide sufficient photo-realistic rendering data to improve algorithms’ perception of articulated objects during sim2real, limiting real-world performance, especially with imperfect point clouds [10], [12]. Additionally, the data collection processes are inefficient and result in small datasets, hindering algorithm generalization to unknown objects. This work aims to create a large-scale dataset with diverse photo-realistic and actionable pose data covering all types of GAParts.
B. Articulated object manipulation
Due to unique kinematic structures and geometric shapes, articulated objects present significant challenges in manipulation. Current methods can be broadly categorized into: learning-based methods and prediction-planning methods. Learning-based methods, such as reinforcement learning [10], [31] and imitation learning [32], [36], require a large amount of high-quality robot demonstration. However, collecting such data is both impractical and time-consuming, and their sim-to-real performance heavily relies on simulator. Current prediction-planning methods [2], [9], [11], [37]–[39] focus on visual affordance but offer ambiguous interactive poses and struggle to generalize due to limited data. They rely on 3D point clouds, ignoring the impact of object materials. In the real world, depth cameras often miss critical points like handles and lids, reducing sim-to-real performance.
III. GAPARTMANIP DATASET
A. Overview
We construct a large-scale dataset, GAPartManip, to address both depth estimation and actionable interaction pose prediction challenges in articulated object manipulation in real-world scenarios from a data-centric perspective. It contains 19 common household articulated categories from GAPartNet, including Box, Bucket, CoffeeMachine, Dishwasher, Door, KitchenPot, Laptop, Microwave, Oven, Printer, Refrigerator, Safe, StorageFurniture, Suitcase, Table, Toaster, Toilet, TrashCan, and WashingMachine, comprising a total of 918 object instances after removing problematic assets. We build a photo-realistic rendering pipeline for each asset in indoor scenes. We render RGB images, IR images, depth maps, and part-level segmentations. Additionally, we create high-quality and physics-plausible interaction pose annotations for each part of the articulated object. Then leverage our GPU-accelerated scene-level pose annotation pipeline to generate dense, part-oriented actionable interaction pose annotations for each data sample. Our dataset contains over 8 billion actionable poses across 241680 data samples. Fig. 2


RGB Image
IR Rendering Interaction Poses
Fig. 2. Data Examples in GAPartManip. GAPartManip is a novel large-scale synthetic dataset for articulated objects, featuring two important aspects: (1) realistic, physics-based IR rendering for various object materials in diverse scenes, and (2) part-oriented actionable interaction pose annotations for a wide range of articulated objects. Each column shows a data sample. From top to bottom, each column displays the RGB image, the IR image (only the left IR image is shown here), and the scene-level actionable interaction pose annotations.
shows examples of data samples from our dataset. Our whole data generation pipeline is illustrated in Fig. 3.
Mesh Fusion FPS
Part-level Stable Pose Annotation
Photo-realistic Scenelevel Rendering
Scene-level Actionable Pose Annotation
Fig. 3. Dataset Generation Pipeline. For scene-level data sample rendering, we input the object asset into our photo-realistic rendering pipeline, generating one RGB image and two IR images (left and right) for each camera perspective. For pose annotation, we begin by performing mesh fusion on each GAPart of the object to establish a one-to-one correspondence between GAParts and meshes. Then, we use FPS to obtain the point cloud for each GAPart, enabling part-level stable interaction pose annotation. These poses are further utilized for scene-level actionable interaction pose annotation for each rendered data sample.
B. Photo-realistic Scene-level Rendering
Our photo-realistic rendering pipeline is built upon NVIDIA Isaac Sim [40]. Specifically, we simulate the RGB and IR imaging process of Intel RealSense D415, a widelyused structured light camera for real-world depth estimation in previous research works. We replicate the layout of the D415 imaging system consisting of four hardware modules, i.e., an IR projector, an RGB camera, and two infrared (IR) cameras. We also project a similar shadow pattern onto the scenes with D415. Inspired by previous works [14], [41], we incorporate domain randomization techniques into our rendering pipeline to mimic the IR rendering under various lighting conditions and material properties in the real world. We render each object in 20 different scenes with various domain randomization settings. Concretely, we randomly vary ambient lighting, background, and object material properties in the scene, generating more diverse data that covers a wider range of real-world imaging conditions. we further randomize the ambient light positions and intensities within each scene.
More importantly, we randomize the parameters of all diffuse, transparent, specular, and metal materials of each part corresponding to their semantics. Finally, we uniformly randomize the joint poses of the object within its joint limits in each scene during the rendering process. We render the objects and parts from different distances. We render each scene with 5 object-centric camera perspectives for the whole object and 5 part-centric camera perspectives for each part. To place the object within the camera view, i.e., the object-centric perspective, the camera is positioned at a latitude of ranged in [10°,60°] and a longitude ranged in [-60°,60°] in the target object. To capture the more fine-grained parts, i.e., the part-centric perspective, we leverage part pose annotations from GAPartNet and the current joint poses to determine the position and orientation of each part in the scene. The camera is then randomly positioned around each part, aiming directly toward the part center. As a result, the target part occupies the primary area of the image. During this process, camera viewpoints are randomly sampled within a latitude range of [0°,60°] and a longitude range of [-75°,75°].
C. GPU-accelerated Scene-level Pose Annotation
a) Part-level Stable Pose Annotation: We employ a pose sampling strategy similar to GraspNet [34] to annotate dense and diverse stable interaction poses for each GAPart, based on the original semantic annotations in GAPartNet [9]. First, we perform mesh fusion for each part, merging the meshes corresponding to the same part to establish a one-toone correspondence between parts and meshes. Then, we apply Farthest Point Sampling (FPS) to downsample the mesh for each part, resulting in N candidate points for pose sampling. For each candidate point, we uniformly generate V × A × D candidate poses, where V is the number of gripper views distributed uniformly over a spherical surface, A represents the number of in-plane gripper rotations, and D refers to the number of gripper depths. In our case, N = 512, V = 64, A = 12, and D = 4. We follow GraspNet to calculate the pose score based on antipodal analysis.
b) Scene-level Actionable Pose Annotation: To obtain part-centric grasping poses, We first project the part-level grasping poses into the scene using the part pose annotations,


and then filter out unreasonable and unreachable poses. More concretely, We classify grasping poses that do not align with single-view partial point clouds as unreasonable. Meanwhile, we consider poses that cause collisions between the gripper and other objects as unreachable. However, such a filtering process is computationally demanding due to the large amounts of points in the scene. To accelerate the pose annotation process, we implemented a CUDA-based optimization for the filtering process. Our optimization significantly reduces the processing time from 5 minutes to less than 2 seconds for each part which is nearly a 150 times speed-up. As a result, the originally year-long pose reduction process can now be completed within 3 days.
IV. FRAMEWORK
We propose a novel framework to address cross-category articulated object manipulation in real-world settings. As illustrated in Fig. 4, the framework primarily consists of three modules: a depth reconstruction module, a pose prediction module, and a local planner module.
A. Depth reconstruction module
The input to our system is a single view RGB-D observation including a raw depth Id, left IR image Il
ir, right IR
image Ir
ir, and an RGB image Ic. However, the raw sensor depth are often imcomplete and even incorrect because transparent and reflective surfaces are inherently ambiguous for structured light and Time-of-Flight depth cameras. Therefore, we leverage diffusion model-based approaches to estimate and restore the incomplete depths of raw sensor outputs. We use D3RoMa [14] as our depth predictor and fine-tune it on our dataset.
B. Pose prediction module
Different from 6 DoF grapsing pose prediction for rigid object manipulation, we need to predict both the 6-DoF part grasping pose and the 2-DoF movement direction after grasping. We adapt the SOTA method Economicgrasp [42] as our actionable pose estimator dubbed Part-aware EcoGrasp and use pretrained GAPartNet [9] to predict the part movement direction. To precisely annotate the part-centric interaction pose, we propose actionness instead of graspness in contrast to Economicgrasp. To annotate actionness, we first denote the scene as a point cloud P = {pi|1, ..., N } with N points. Then for each point pi, we uniformly discretize its sphere space into V = {vj|j = 1, ..., V } approaching directions. For each view vj of point pi, we generate L actionable
pose candidates Ai,j
k ∈ SE(3) indexed by k ∈ [1, L] by grid sampling along gripper depths and in-plane rotation angels respectively. We employ antipodal analysis [34] to calculate the grasping quliaty score qi,j
k ∈ [0, 1.2]. Next, We define an
actionable label cia ∈ {0, 1} for each point indicating whether this point is on a interacble part. We also define a scenelevel collision label ci,j
k ∈ {0, 1} for each pose indicating whether this pose will cause collision. Finally, the point-wise
actionness score sP
i and view-wise actionness score sV
i are defined as:
sP
i= 1
X
j,k
Ai,j
k
ci
a
X
j,k
1 qi,j
k > T ci,j
k , (1)
sV
i= 1
X
k
Ai,j
k
ci
a
X
k
1 qi,j
k > T ci,j
k , (2)
where T is a predefined threshold to filter out inferior quality poses. We then train Part-aware EcoGrasp [42] following [42]. Additionally, we utilize the pre-trained GAPartNet [9] to predict the motion direction which speficies the part movement direction after grasping the actionable part.
C. Local planner module
We use CuRobo [43] as our motion planner. It optimizes motion trajectories based on actionable poses given by the pose prediction module, computes joint angles through inverse kinematics, and drives the robot to execute trajectory actions through joint control modes. Subsequently, the robot executes actions based on the motion directionr ⃗ p .
V. EXPERIMENTS
We conduct experiments for each module. The depth estimation and actionable pose prediction experiments are conducted to illustrate the significance of our dataset in articulated object manipulation tasks. Meanwhile, real-world experiments are carried out to compare the performance of our framework with existing methods. We also performed ablation studies for each module.
A. Depth Estimation Experiments
In this section, we evaluate different depth estimation methods with our GAPartManip to demonstrate the effectiveness of our dataset for improving articulated object depth estimation in both simulation and the real world. Data Preparation. We split the dataset into training and test sets using an approximate 8:2 ratio. To maintain comprehensive coverage, each object category is split carefully, ensuring that both the training and test sets include samples from all categories. Additionally, we make sure that samples rendered from the same object category are assigned exclusively to either the training or test set. We compare our method with following baselines: leftmargin=10pt
• SGM [44] is one of the most widely-used traditional algorithm for dense binocular stereo matching. • RAFT-Stereo (RS) [45] is a learning-based binocular stereo matching architecture built upon the dense optical flow estimation framework RAFT [46], using an iterative update strategy to recursively refine the disparity map. • D3RoMa (DR) [14] is a SOTA, learning-based stereo depth estimation framework based on the diffusion model. It excels at restoring noisy depth maps, especially for transparent and specular surfaces.
Evaluation Metrics. We evaluate the estimated disparity and depth using the following metrics:


IR Images
Actionable Pose Estimator
Motion Direction Estimator
7-DoF Actionable Poses
TODO
6-DoF-based Motion
Reconsturted Depth
Raw Depth
Depth Reconstruction Module Pose Prediction Module Local Planner Module
Depth
Estimator cuRobo
Fig. 4. Framework overview. Given IR images and raw depth, the depth reconstruction module first performs depth recovery. Subsequently, the pose prediction module generates a 7-DOF actionable pose and a 3-DOF motion directive based on the reconstructed depth. Finally, the local planner module carries out the action execution.
leftmargin=10pt
• EPE: Mean absolute difference between the ground truth and the estimated disparity map across all pixels. • RMSE: Root mean square of depth errors across all pixels. • MAE: Mean absolute depth error across all pixels. • REL: Mean relative depth error across all pixels. • δi: Percentage of pixels satisfying max d
dˆ, dˆ
d < δi. d
denotes the estimated depth. dˆ denotes the ground truth.
TABLE I
QUANTITATIVE RESULTS FOR DEPTH ESTIMATION IN SIMULATION
Methods EPE ↓ RMSE ↓ REL ↓ MAE ↓ δ1.05 ↑ δ1.10 ↑ δ1.25 ↑
SGM [44] 6.82 1.623 0.561 0.794 34.71 38.94 46.27
RS [45] 5.28 1.497 0.506 0.618 36.82 41.05 49.92
DR [14] 2.82 0.732 0.268 0.317 46.22 67.62 83.09
RS* [45] 2.79 0.798 0.247 0.309 52.83 68.30 80.15
Ours* 0.69 0.225 0.041 0.050 86.22 93.45 97.41
* indicates that the method is trained on the GAPartManip dataset.
Fig. 5. Qualitative Results for Depth Estimation in the Real World. Our refined depth is more robust for transparent and translucent lids and small handles compared to RAFT-Stereo. Zoom in to better observe small parts like handles and knobs. Results and Analysis. The quantitative results in simulation are presented in Tab. I. The results indicate that the traditional stereo matching algorithm, SGM, struggles in scenes with articulated objects with challenging material characteristics. The same observation applies to the pre-trained RAFTStereo. Meanwhile, the pre-trained D3RoMa models demon
strate reasonably good stereo depth estimation capabilities in the experiments. However, both RAFT-Stereo and D3RoMa are significantly enhanced when fine-tuned on GAPartManip. Specifically, RAFT-Stereo achieves a 150% improvement in MAE compared to its pre-trained version, while our model exhibits a 600% improvement, achieving the best performance in the simulation. As illustrated in Fig. 5, the fine-tuned models also demonstrate strong depth estimation performance in real-world scenarios. In particular, in realworld environments with challenging materials, as shown in the first three rows of the figure, our model significantly outperforms the fine-tuned RAFT-Stereo and the raw depth, exhibiting noticeably better robustness. Both simulation and real-world experiments demonstrate the effectiveness of our proposed GAPartManip in substantially improving depth estimation for articulated objects with challenging materials.
B. Actionable Pose prediction Experiments
In this section, we evaluate the impact of our dataset on improving the method for articulated object actionable pose estimation. Data Preparation. We split the dataset into training and testing sets using an approximate 7:3 ratio. Specifically, we further divide the test sets into 3 categories: seen instances, unseen but similar instances, and novel instances. We compare our methods with the following baselines: leftmargin=10pt
• GSNet (GS) [47] is a grasping pose prediction model trained on the GraspNet-1 billion [34] dataset for rigid object. We evaluate both the pre-trained model and the fine-tuned model separately. • Where2Act (WA) [2] is an affordance-based method for interacting with articulated objects. Unlike the original approach, we do not train a separate network for each task. As Where2act cannot generate stable grasping poses, we integrated GSNet, as referenced in [12], to enhance where2act’s capabilities to align with experimental setting. • EconomicGrasp (EG) [42] is also a pose prediction method for rigid objects, which includes an interactive grasp head and composite score estimation to enhance the precision of specific grasps.


Pre-trained
EconomicGrasp
Ours
Fig. 6. Qualitative comparison of actionable pose prediction on synthetic data.
Evaluation Metrics. Following [34], we utilize precision to evaluate the performance of actionable pose estimation.
P recisionμ = nsucμ /ngrasp (3)
P recisionμ represents the success rate (SR) of predicted interaction poses at friction coefficient μ, where ngrasp denotes the number of predicted poses, and nsucμ denotes the number of successful grasps predicted under μ. Results and Analysis. Our quantitative results in simulation are presented in Tab. II. Even though both GSNet and our Part-aware EcoGrasp are trained on our data, they outperform Where2Act, possibly because Where2Act struggles with cross-category and cross-action reasoning. Partaware EcoGrasp and fine-tuned GSNet show a substantial improvement in precision compared to pretrained models. It is evident that our data significantly enhances the capability of existing methods in actionable pose estimation for articulated objects. Specifically, our dataset offers strong geometric priors for parts, enabling networks to focus more on actionable parts rather than non-actionable links. For instance, although the pre-trained EconomicGrasp in Fig. 6 generates a set of stable grasping poses, it cannot differentiate whether these poses act on actionable parts, meaning they may fail in interacting with articulated objects.
TABLE II
QUANTITATIVE RESULTS FOR ACTIONABLE POSE PREDICTION IN
SIMULATION
Method Seen Unseen Novel
P P0.8 P0.4 P P0.8 P0.4 P P0.4 P0.8 GS [47] 13.28 11.55 6.70 17.36 15.57 9.19 9.76 8.43 5.25 EG [42] 24.72 19.65 9.97 23.91 20.29 9.90 14.56 12.02 9.23 GS* [47] 25.70 20.26 9.00 25.45 20.28 9.67 23.99 20.55 11.20 WA* [2] 14.43 12.44 6.53 11.04 7.41 2.52 4.17 1.85 0.47 Ours* 55.33 51.19 30.25 56.26 53.02 32.91 41.65 39.06 23.25
* indicates that the method is trained on the GAPartManip dataset.
C. Real-World experiment
To validate the sim-to-real generalizability of GAPartManip, we conducte real-world experiments. We use a Franka robot arm with an Intel RealSense camera to capture depth and IR images. We compare our method with three baselines: Where2act, AO-Grasp, GSNet, and, like in V-B, We modified the Where2act interaction pipeline to finish our tasks. The
RGB
Raw Depth
Ours
EconomicGrasp
Ours
Fig. 7. Qualitative Results For Real-world Manipulation. The top-15 scored actionable poses are displayed, with the red gripper representing the top-1 pose.
experiment consists of 7 distinct instances, including StorageFurniture, Box, and Microwave, evaluating the success rate of the top-1 interactive pose for each method across open (n=14) and close (n=17) tasks. As shown in Tab. V-C, the overall success rate of GAPartManip is 61.29%, showcasing not only a successful transfer to the real world but also a significant performance boost compared to other methods. Additionally, we perform ablation studies to assess how different modules affect the overall pipeline performance. As shown in Fig. 7, depth cameras yield poor depth data when faced with certain materials, significantly impacting subsequent manipulations. Our depth reconstruction module effectively addresses this issue by repairing 2D depth map, thereby enhancing the performance of subsequent modules. Similarly, as shown in Fig 7, GAPartManip tends to prioritize interactable GAParts. This part-aware capability could possibly explain why our method leads to such significant performance disparities as seen in Tab. V-C.
TABLE III
REAL-WORLD ARTICULATED OBJECTS MANIPULATION RESULTS
Method Success Rate (%) ↑
Open Close Overall AO-Grasp 28.57 29.41 29.03 Where2act 21.42 17.64 19.35 GSNet 42.85 23.53 32.25 Ours w/o Part-aware EcoGrasp 64.28 41.17 51.61 Ours w/o Depth Reconstruction 50.00 29.41 38.70 Ours 64.28 58.82 61.29
VI. CONCLUSIONS
In this paper, we build a large-scale synthetic dataset for generalizable and actionable part manipulation with materialagnostic articulated objects. Our dataset is the first largescale, diverse in instances, categories, scenes, and materials articulated object dataset. Meanwhile, we propose an articulated object manipulation framework capable of zeroshot transfer to the real world. We conduct experiments on


individual modules and real-world overall experiments, with results indicating the competitiveness of our approach. Our dataset will be released.
REFERENCES
[1] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, “SAPIEN: A simulated part-based interactive environment,” in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 1, 2 [2] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani, “Where2act: From pixels to actions for articulated 3d objects,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 6813–6823. 1, 2, 5, 6 [3] R. Wu, Y. Zhao, K. Mo, Z. Guo, Y. Wang, T. Wu, Q. Fan, X. Chen, L. Guibas, and H. Dong, “Vat-mart: Learning visual action trajectory proposals for manipulating 3d articulated objects,” arXiv preprint arXiv:2106.14440, 2021. 1
[4] Y. Wang, R. Wu, K. Mo, J. Ke, Q. Fan, L. J. Guibas, and H. Dong, “Adaafford: Learning to adapt manipulation affordance for 3d articulated objects via few-shot interactions,” in European conference on computer vision. Springer, 2022, pp. 90–107. 1 [5] Y. Zhao, R. Wu, Z. Chen, Y. Zhang, Q. Fan, K. Mo, and H. Dong, “Dualafford: Learning collaborative visual affordance for dual-gripper manipulation,” arXiv preprint arXiv:2207.01971, 2022. 1
[6] B. Eisner, H. Zhang, and D. Held, “Flowbot3d: Learning 3d articulation flow to manipulate articulated objects,” arXiv preprint arXiv:2205.04382, 2022. 1
[7] H. Zhang, B. Eisner, and D. Held, “Flowbot++: Learning generalized articulated objects manipulation via articulation projection,” arXiv preprint arXiv:2306.12893, 2023. 1
[8] C. Zhong, Y. Zheng, Y. Zheng, H. Zhao, L. Yi, X. Mu, L. Wang, P. Li, G. Zhou, C. Yang, et al., “3d implicit transporter for temporally consistent keypoint discovery,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 3869–3880. 1
[9] H. Geng, H. Xu, C. Zhao, C. Xu, L. Yi, S. Huang, and H. Wang, “Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7081–7091. 1, 2, 3, 4 [10] H. Geng, Z. Li, Y. Geng, J. Chen, H. Dong, and H. Wang, “Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 2978–2988. 1, 2 [11] H. Geng, S. Wei, C. Deng, B. Shen, H. Wang, and L. Guibas, “Sage: Bridging semantic and actionable parts for generalizable manipulation of articulated objects,” 2024. 1, 2 [12] J. Wang, W. Liu, Q. Yu, Y. You, L. Liu, W. Wang, and C. Lu, “Rpmart: Towards robust perception and manipulation for articulated objects,” arXiv preprint arXiv:2403.16023, 2024. 1, 2, 5
[13] Y. Geng, B. An, H. Geng, Y. Chen, Y. Yang, and H. Dong, “Rlafford: End-to-end affordance learning for robotic manipulation,” in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 5880–5886. 1, 2 [14] S. Wei, H. Geng, J. Chen, C. Deng, W. Cui, C. Zhao, X. Fang, L. Guibas, and H. Wang, “D3roma: Disparity diffusion-based depth sensing for material-agnostic robotic manipulation,” in 8th Annual Conference on Robot Learning (CoRL), 2024. 1, 3, 4, 5
[15] J. Shi, A. Yong, Y. Jin, D. Li, H. Niu, Z. Jin, and H. Wang, “Asgrasp: Generalizable transparent object reconstruction and 6-dof grasp detection from rgb-d active stereo camera,” in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 5441–5447. 1, 2 [16] C. P. Morlans, C. Chen, Y. Weng, M. Yi, Y. Huang, N. Heppert, L. Zhou, L. Guibas, and J. Bohg, “Ao-grasp: Articulated object grasp generation,” arXiv preprint arXiv:2310.15928, 2023. 2
[17] B. An, Y. Geng, K. Chen, X. Li, Q. Dou, and H. Dong, “Rgbmanip: Monocular image-based robotic manipulation through active object pose estimation,” in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 7748–7755. 2
[18] L. Yi, H. Huang, D. Liu, E. Kalogerakis, H. Su, and L. Guibas, “Deep part induction from articulated object pairs,” arXiv preprint arXiv:1809.07417, 2018. 2
[19] C. Deng, J. Lei, W. B. Shen, K. Daniilidis, and L. J. Guibas, “Banana: Banach fixed-point network for pointcloud segmentation with interpart equivariance,” in NeurIPS, 2024. 2 [20] X. Li, H. Wang, L. Yi, L. J. Guibas, A. L. Abbott, and S. Song, “Category-level articulated object pose estimation,” in CVPR, 2020. 2 [21] G. Liu, Q. Sun, H. Huang, C. Ma, Y. Guo, L. Yi, H. Huang, and R. Hu, “Semi-weakly supervised object kinematic motion prediction,” in CVPR, 2023. 2 [22] J. Lyu, Y. Chen, T. Du, F. Zhu, H. Liu, Y. Wang, and H. Wang, “Scissorbot: Learning generalizable scissor skill for paper cutting via simulation, imitation, and sim2real,” in 8th Annual Conference on Robot Learning, 2024. [Online]. Available: https://openreview.net/forum?id=PAtsxVz0ND 2 [23] J. Zhang, N. Gireesh, J. Wang, X. Fang, C. Xu, W. Chen, L. Dai, and H. Wang, “Gamma: Graspability-aware mobile manipulation policy learning based on online grasping pose fusion,” in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 1399–1405. 2 [24] Q. Chen, M. Memmel, A. Fang, A. Walsman, D. Fox, and A. Gupta, “Urdformer: Constructing interactive realistic scenes from real images via simulation and generative modeling,” in Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL 2023, 2023. 2 [25] J. Mu, W. Qiu, A. Kortylewski, A. Yuille, N. Vasconcelos, and X. Wang, “A-sdf: Learning disentangled signed distance functions for articulated shape representation,” in ICCV, 2021. 2 [26] Z. Jiang, C.-C. Hsu, and Y. Zhu, “Ditto: Building digital twins of articulated objects from interaction,” in CVPR, 2022. 2 [27] W.-C. Tseng, H.-J. Liao, L. Yen-Chen, and M. Sun, “Cla-nerf: Category-level articulated neural radiance field,” in ICRA, 2022. 2 [28] R. Luo, H. Geng, C. Deng, P. Li, Z. Wang, B. Jia, L. Guibas, and S. Huang, “Physpart: Physically plausible part completion for interactable objects,” 2024. [Online]. Available: https://arxiv.org/abs/ 2408.13724 2 [29] J. Lei, C. Deng, B. Shen, L. Guibas, and K. Daniilidis, “Nap: Neural 3d articulation prior,” arXiv preprint arXiv:2305.16315, 2023. 2
[30] J. Liu, H. I. I. Tam, A. Mahdavi-Amiri, and M. Savva, “Cage: Controllable articulation generation,” in CVPR, 2024. 2 [31] Y. Geng, B. An, H. Geng, Y. Chen, Y. Yang, and H. Dong, “Endto-end affordance learning for robotic manipulation,” in ICRA, 2023. 2
[32] R. Gong, J. Huang, Y. Zhao, H. Geng, X. Gao, Q. Wu, W. Ai, Z. Zhou, D. Terzopoulos, S.-C. Zhu, et al., “Arnold: A benchmark for languagegrounded task learning with continuous states in realistic 3d scenes,” in ICCV, 2023. 2 [33] Y. Kuang, J. Ye, H. Geng, J. Mao, C. Deng, L. Guibas, H. Wang, and Y. Wang, “Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation,” 2024. [Online]. Available: https://arxiv.org/abs/2407.04689 2 [34] H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-1billion: A largescale benchmark for general object grasping,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11 444–11 453. 2, 3, 4, 5, 6 [35] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, “Contactgraspnet: Efficient 6-dof grasp generation in cluttered scenes,” 2021. [Online]. Available: https://arxiv.org/abs/2103.14127 2 [36] P.-L. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, and C. Schmid, “Instruction-driven history-aware policies for robotic manipulations,” in Conference on Robot Learning. PMLR, 2023, pp. 175–187. 2 [37] W. Liu, J. Mao, J. Hsu, T. Hermans, A. Garg, and J. Wu, “Composable part-based manipulation,” 2024. [Online]. Available: https://arxiv.org/abs/2405.05876 2 [38] S. Ling, Y. Wang, S. Wu, Y. Zhuang, T. Xu, Y. Li, C. Liu, and H. Dong, “Articulated object manipulation with coarse-to-fine affordance for mitigating the effect of point cloud noise,” 2024. [Online]. Available: https://arxiv.org/abs/2402.18699 2 [39] Y. Kuang, J. Ye, H. Geng, J. Mao, C. Deng, L. Guibas, H. Wang, and Y. Wang, “Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation,” in 8th Annual Conference on Robot Learning. 2


[40] J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox, “Gpu-accelerated robotic simulation for distributed reinforcement learning,” 2018. 3 [41] Q. Dai, J. Zhang, Q. Li, T. Wu, H. Dong, Z. Liu, P. Tan, and H. Wang, “Domain randomization-enhanced depth simulation and restoration for perceiving and grasping specular and transparent objects,” in European Conference on Computer Vision (ECCV), 2022. 3
[42] X.-M. Wu, J.-F. Cai, J.-J. Jiang, D. Zheng, Y.-L. Wei, and W.-S. Zheng, “An economic framework for 6-dof grasp detection,” 2024. [Online]. Available: https://arxiv.org/abs/2407.08366 4, 5, 6 [43] B. Sundaralingam, S. K. S. Hari, A. Fishman, C. Garrett, K. Van Wyk, V. Blukis, A. Millane, H. Oleynikova, A. Handa, F. Ramos, N. Ratliff, and D. Fox, “Curobo: Parallelized collision-free robot motion generation,” in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 8112–8119. 4 [44] H. Hirschmuller, “Stereo processing by semiglobal matching and mutual information,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 2, pp. 328–341, 2008. 4, 5 [45] L. Lipson, Z. Teed, and J. Deng, “Raft-stereo: Multilevel recurrent field transforms for stereo matching,” in 2021 International Conference on 3D Vision (3DV). IEEE, 2021, pp. 218–227. 4, 5 [46] Z. Teed and J. Deng, “Raft: Recurrent all-pairs field transforms for optical flow,” in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16. Springer, 2020, pp. 402–419. 4 [47] C. Wang, H.-S. Fang, M. Gou, H. Fang, J. Gao, and C. Lu, “Graspness discovery in clutters for fast and accurate grasp detection,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15 964–15 973. 5, 6