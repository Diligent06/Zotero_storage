Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens
Shuqi Lu1∗ Haowei Lin1,5∗ Lin Yao1∗ Zhifeng Gao1,2 Xiaohong Ji1 Weinan E2,3,4† Linfeng Zhang1,2† Guolin Ke1,2†
1DP Technology, Beijing, 100080, China. 2AI for Science Institute, Beijing 100080, China. 3School of Mathematical Sciences, Peking University, Beijing, 100871, China. 4Center for Machine Learning Research, Peking University, Beijing 100084, China. 5Institute for Artificial Intelligence, Peking University, Beijing 100871, China.
Abstract
Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding (3D GU) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates 3D GU tasks via autoregressive prediction. At its core, Uni3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse 3D GU tasks within a single autoregressive framework. Extensive experiments across multiple microscopic 3D GU tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.
1 Introduction
Recent successes in large language models (LLMs) demonstrate that unifying generation and understanding is essential for generalist models [1, 2]. In deep learning, generation refers to synthesizing new data, while understanding is reflected in tasks like classification and regression. LLMs achieve this unification via autoregressive next-token prediction. Moreover, this paradigm has been extended to multi-modal data (e.g., speech, images, and video) by tokenizing them within a unified next-token prediction framework, ultimately leading to a unified multi-modal large language model [3, 4, 5].
∗Equal contribution. For a full description of each author’s contribution, see Author Contributions. †Corresponding authors.
Preprint. Under review.
arXiv:2503.16278v2 [cs.LG] 21 Mar 2025


Model
Tokenizer
3D Structures from Micro to Macro
Compressed Spatial Tokens
Single-Frame Generation
Multi-Frame Generation
Token-Level Understanding
Structure-Level Understanding
Unified Framework for Various Tasks
Octree + 2-level subtree compression + Fine-grained leaf tokens Compress Full Voxel Grids into Hundreds of Tokens
Autoregressive Transformer + Masked Next-Token Prediction
1
1 00 1
0001 1000
[m] [m]
...
Cross-Modal Inputs
Coarse-to-fine subdivision
Corresponding octree
Optional
Natural language
Biology sequences Spectral signal
Figure 1: Overview of the Uni-3DAR framework. It supports diverse 3D structures from microscopic to macroscopic scales via our proposed tokenizer. Its autoregressive modeling enables both 3D structure generation and understanding within a single model.
In AI for science, accurately modeling three-dimensional (3D) structures is critical, as even minor spatial differences can profoundly impact reactivity and functionality. Tasks such as protein folding [6], molecular docking [7], and property prediction [8, 9, 10] heavily rely on capturing these spatial interactions. However, existing approaches for 3D generation and understanding have typically been developed independently. Generation predominantly employs diffusion models to synthesize structures from noise [11, 12], while understanding usually relies on unsupervised, BERT-style pretraining [13, 8]. Furthermore, previous models are often tailored for specific types of 3D structures. For example, models designed for crystal structures cannot be directly applied to proteins. Such separate developments limit overall effectiveness. Moreover, although autoregressive models have demonstrated great success in natural language and image domains, their application to 3D structural data remains largely unexplored. This paper aims to bridge this gap by proposing an autoregressive prediction framework that unifies 3D structural generation and understanding across diverse 3D structural data—including molecules, proteins, polymers, and crystals 3.
One critical challenge in achieving this unified framework is designing an effective tokenization strategy that converts a 3D structure into a 1D sequence of tokens. Existing approaches broadly categorized into two types, each with significant limitations. Point-based (or atom-based in microscopic structures) tokenization leverages the sparsity of 3D structures by sequentially predicting points’ coordinates and properties (e.g., atom types) [14, 15]. However, it struggles to simultaneously generate 3D coordinates, properties, and the corresponding prediction order. Grid-based tokenization discretizes 3D space into voxel grids [16, 17, 18], enabling the application of image-based token prediction techniques. However, this approach suffers from severe computational inefficiency due to the cubic growth in token count. For instance, even small molecules may require tens of thousands of tokens [18], which makes such methods impractical for real-world applications. In summary, pointbased methods exploit the inherent sparsity of 3D structures but fail to capture full spatial context, while grid-based methods retain complete spatial information at the cost of efficiency. This trade-off underscores the need for a more balanced and scalable approach to 3D structural tokenization.
To address these limitations, we propose a hierarchical tokenization method that losslessly and efficiently compresses the full-size 3D grid. As illustrated in Fig. 2 (a) and (b), we first construct an octree hierarchically based on data sparsity and a predefined maximum depth. We then introduce a fine-grained structural tokenization that encodes essential details—such as atomic types and precise coordinates for microscopic 3D structures. Concatenating these tokens level by level produces a hierarchical, coarse-to-fine 1D token sequence that effectively represents the 3D structure (Fig. 2 (c)). Compared to point-based methods, our hierarchical tokenization preserves approximate 3D positional context from preceding levels, thereby improving prediction accuracy. In contrast to full-size grid-based approaches, it efficiently compresses 3D space and avoids cubic token growth, making it significantly more scalable for real-world applications.
3Although we primarily use microscopic 3D data in our experiments to illustrate and validate our method, the proposed approach can be seamlessly extended to various types of sparse 3D data.
2


(4, 4)
(2, 2) (6, 2) (2, 6) (6, 6)
(1, 1) (3, 1) (1, 3) (3, 3) (5, 5) (7, 5) (5, 7) (7, 7)
Pos: (3, 3), Atom: H In-cell X: 12 In-cell Y: 25
Pos: (5, 5), Atom: H In-cell X: 36 In-cell Y: 32
HH
level: 0
level: 1
level: 2
level: 3 (Fine-grained tokens)
Pos (4, 4) (2, 2) (6, 2) (2, 6) (6, 6) (1, 1) (3, 1) (1, 3) (3, 3) (5, 5) (7, 5) (5, 7) (7, 7) (3, 3) (5, 5)
Level 0 1 1 1 1 2 2 2 2 2 2 2 2 3 3
Type 1 1 0 0 1 0 0 0 1 1 0 0 0 H H
In-cell X 25 25 25 25 25 25 25 25 25 25 25 25 25 12 36
In-cell Y 25 25 25 25 25 25 25 25 25 25 25 25 25 25 32
Octree tokens Fine-grained tokens
Sequence Order (Level-Wise)
In this example, in-cell positions are discretized into $[0,50)$, with a default value of 25.
H
H
level: 0, cell length: 8
HH
H
(2, 2)
(6, 6)
(4, 4) (3, 3)
(5, 5)
(1, 1)
(7, 7)
(2, 6)
(6, 2)
(1, 3)
(3, 1)
(5, 7)
(7, 5)
H
x
y
04
4
x
y
0 2 6 1357
2
7
6
1
3
5
x
y
0
level: 1, cell length: 4 level: 2, cell length: 2
partition (4,4) partition (2,2) & (6,6)
1
10 0 1
0001 1 0 0
0
1001
0001 1000
Pos (4, 4) (2, 2) (6, 6) (3, 3) (5, 5)
Level 0 1 1 3 3
Type 9 8 1 H H
In-cell X 25 25 25 12 36
In-cell Y 25 25 25 25 32
Compressed Tokens:
Pos: (4, 4)
Pos: (6, 6)
Pos: (2, 2) compress
(a) Coarse-to-fine Subdivision
(b) Octree Compression
(c) Octree Spatial Tokens
(d) 2-Level Subtree Compression
Figure 2: Overview of the proposed tokenization (illustrated in 2D using a quadtree for clarity). (a) Adaptive coarse-to-fine subdivision of grid cells, where darker nodes indicate non-empty cells that can be further partitioned. (b) This partitioning process constructs an octree, providing a lossless compression of the full-size 3D grid. (c) Uni-3DAR’s tokenization consists of two components: hierarchical spatial compression via an octree and fine-grained structural tokenization. Each node’s position is determined by its tree level and cell center. (d) The proposed 2-level subtree compression further reduces the octree tokens by up to 8x (4x in the illustrated quadtree).
Building on this hierarchical tokenization, we employ autoregressive transformer models [19] to unify 3D structural generation and understanding. We call this model Uni-3DAR, with two further optimizations:
1. 2-Level Subtree Compression: Rather than assigning each tree node an individual token, we compress a 2-level subtree (8 subcells) into a single token (Fig. 2 (d)). As each subcell is classified binary (empty or not), grouping 8 subcells results in 28 = 256 distinct states, reducing the sequence length approximately 8x and converting 8 binary classifications into one 256-class task.
2. Masked Next-Token Prediction: Unlike text or image autoregressive models, where the next token’s position is fixed, our hierarchical tokenization produces dynamic token positions due to inherent sparsity. Without an explicitly provided next position, the model cannot reliably predict the next token. Simply appending the next position to the current token did not yield satisfactory results. To address this, we propose a masked next-token prediction strategy that combines masked prediction with next-token prediction. Specifically, as illustrated in Fig. 3 (a), we duplicate each token so that it appears twice with the same token position, and then replace the first copy with a “[MASK]” token. Based on this duplicated sequence, we still use next-token prediction, but exclusively on the masked token. Notably, this setup ensures that prediction is performed using the correct positional information
3


for the intended next token, effectively addressing the challenge of dynamic token positions. Although this approach doubles the sequence length, it leads to significant performance gains.
By employing duplicated masked tokens, Uni-3DAR seamlessly unifies 3D structural generation and understanding tasks within a single framework (Fig. 3 (b)). In this setup, generation leverages masked tokens, token-level (atom-level) understanding operates on fine-grained tokens, and structurelevel tasks focus on the “[EoS]” (End of Structure) tokens, ensuring distinct roles without mutual interference. Additionally, the autoregressive architecture inherently supports conditional and multiframe structural generation.
Extensive experiments across diverse tasks—including molecular and crystal generation, protein pocket prediction, molecular docking, and molecular pretraining—demonstrate Uni-3DAR’s superior or competitive performance compared to existing methods. Notably, Uni-3DAR consistently outperforms diffusion-based generation models, underscoring its strong generative capabilities. Further evaluations confirm the benefits of unifying generation and understanding, highlight the contributions of each component via ablation studies, and reveal significant efficiency advantages in inference speed. Overall, these results validate the effectiveness of the proposed Uni-3DAR framework.
2 Method
2.1 Dynamic Coarse-to-Fine Tokenization for 3D Structures
3D structures inherently exhibit sparsity. At the microscopic scale, most space is empty except for a few scattered atoms; similarly, at the macroscopic level, the vast majority of volume is empty, with detailed representations only needed at object surfaces. Hence, using a full-size voxel grid to represent these structures is highly inefficient. To address this, we propose a hierarchical, coarse-tofine approach for tokenizing 3D structures by leveraging their inherent sparsity. As shown in Fig. 1, our approach consists of two parts: (1) a hierarchical compression of 3D space using an octree, and (2) a fine-grained structural tokenization.
The first component is the octree, an efficient data structure for losslessly compressing 3D grids. We begin with a single grid cell encompassing the entire 3D structure and recursively subdivide it based on a simple rule: if a cell is not empty (i.e., contains atoms), it is further partitioned. At each subdivision, each dimension is halved, yielding 23 = 8 equally sized subcells (hence the name "octree"). This subdivision continues for a predefined number of levels, denoted by L. Let c0 be the
cell length at root level; then the cell length at the final level, L − 1, is given by cL−1 = c0/2L−1.
The second component is fine-grained tokenization of structural details. While the octree provides a coarse representation of non-empty regions, it lacks finer details within these regions, such as atom types and precise 3D coordinates in microscopic structures or surface details in macroscopic structures. While deeper octrees, as used in previous work [20], can capture finer details, they are computationally inefficient due to increased octree tokens. Instead, we adopt a more efficient approach inspired by the concept of 2D image patches in recent image transformer models [21]. In our method, we extend this idea to 3D structures by tokenizing the fine-grained structural details within the final-level non-empty regions, which we refer to as “3D patches”. A well-established technique in the image domain is vector quantized tokenization, as employed in VQ-VAE [22]. Similarly, we can quantize fine-grained 3D patches into discrete variables and perform autoregressive prediction on them. Alternatively, we can leverage recent patch-level diffusion loss strategies from MAR [23] to directly model continuous vector representations of 3D patches.
Finally, we concatenate these tokens level by level. Beyond token content, we represent each token’s positional information using its tree level and the spatial coordinates of its cell center. For instance, the root cell is at level 0 with a center at (c0/2, c0/2, c0/2). During autoregressive prediction, since octree tokens are dynamically unfolded level by level, the positions of all tokens at the current level are known based on the predictions from previous level. This explicit knowledge of the token position is crucial, as autoregressive models predict only token content. In contrast, previous point-based tokenization lacks inherent positional information. As a result, previous models must infer both precise positions and contents simultaneously, making the task significantly more challenging.
2-Level Subtree Compression Although our octree tokenization avoids the cubic growth of cells, it remains inefficient for large 3D structures. At each level, there are at most 8N tokens (where N is the number of non-empty cells at the final level), leading to a total of up to 8N L tokens over L levels—approximately two orders of magnitude larger than N . To further reduce the token count,
4


(6,6)
Cur Pos: (3,3) (5,5) Next Pos: (3,3) (5,5)
Cur Pos: (3,3) (3,3) (5,5) (5,5)
[m] [m]
......
Next-Token Prediction
(3,3)
Cur Pos: (5,5)
...
Masked Prediction
[m] [m]
Masked Next-Token Prediction (Our)
Requires additional next pos
Cur pos only, parallel independent sampling
Cur pos only, causal sampling
(6,6)
[m] [m]
[m] [m] ... [m] [m] ...
condition structure 1’s octree tokens structure 1’s fine-grained tokens structure 2’s tokens
...
[eos] [eos]
token-level properties Structure-level properties
Challenge: dynamic token positions across samples
: Generation (Next-Token Prediction) : Token-Level Understanding : Structure-Level Understanding
(a) Masked Next-Token Prediction
(b) Unified Framework for 3D GU
Figure 3: (a) The proposed masked next-token prediction mechanism effectively addresses the challenge of dynamically varying token positions. (b) Based on the duplicated masked token, Uni3DAR unifies multi-frame 3D structural generation with token-level and structure-level understanding tasks in a single model, assigning each token a distinct role.
we introduce 2-level subtree compression, which merges a 2-level subtree into a single token. Each 2-level subtree consists of one parent node and 8 child nodes. Since the parent’s type is always 1, the entire subtree can be fully represented by the types of its 8 children, resulting in 28 = 256 possible states. This compression reduces the total token count by approximately a factor of 8, yielding at most N (L − 1) tokens. For positional information (i.e., cell center and level) of the compressed nodes, we retain the parent’s position.
Fine-Grained Atom Tokenization for Microscopic Structures For the microscopic 3D structures focused on in this study, we adopt a simple fine-grained tokenization approach instead of the universal solution for various 3D structures. Specifically, by setting the final-level cell size cL−1 sufficiently small (e.g., 0.24Å in our experiments), we ensure that each cell contains at most one atom. Consequently, each fine-grained token corresponds to a single atom, represented by four variables: the atom type and its 3D coordinates within the cell. To further discretize in-cell positions, we use a predefined resolution cr (e.g., 0.01Å), converting continuous coordinates into integers ranging from 0 to Np = cL−1/cr. Formally, we denote the content of the i-th token as (ti, ei), where ti is the token type and ei = (e0
i , e1
i , e2
i ) ∈ {0, 1, . . . , Np − 1}3 represents its discretized in-cell 3D coordinates. Since octree tokens do not have in-cell positions, ei is set to the default value Np/2.
2.2 Masked Next-Token Prediction for Dynamic Token Positions
In traditional autoregressive models, token positions follow a fixed pattern. For instance, in text models, the token at the i-th position is always followed by the token at position i + 1. Due to this static structure, the next token’s position can be directly inferred from the current token’s position, and explicit modeling of the next position is unnecessary.
In our proposed coarse-to-fine tokenization method for 3D structures, token positions are inherently dynamic and vary across samples. This variability makes it much more challenging to infer the next token’s position, so it is preferable to explicitly provide the next position to the model.
One straightforward approach, as suggested by previous work [20], is to encode both the current and the next token positions. However, we found that this method did not yield satisfactory results. In particular, the dynamic nature of the next token’s position introduces randomness that acts like noise, adversely affecting the current token’s representation. Moreover, the model must simultaneously learn to encode a token’s content based on its current position and predict the next token’s content based on a variable next position. This dual learning burden poses significant challenges and may lead to degraded performance.
Recently, many generative approaches based on masked prediction have emerged [24, 23]. At masked positions, these methods replace the token with a “[MASK]” symbol and predict its content based on the token’s current positional information. This approach sidesteps issues related to dynamic next positions and has proven effective in generation scenarios where the token order is randomized [23].
5


However, masked prediction typically relies on bi-directional attention, whereas our tokenization unfolds level-by-level in a uni-directional manner, making the direct application of bi-directional masked prediction challenging. Moreover, masked generation depends on parallel, independent sampling, which differs fundamentally from the causal sampling used in next-token prediction. Consequently, additional rule-based sampling strategies are often required during inference to balance performance and efficiency [23].
To address these challenges, we propose a simple solution that integrates masked prediction into the next-token prediction framework. Specifically, for each token, we duplicate it so that both copies share the same current position, and then replace the content of the first copy with a “[MASK]” token. Next-token prediction is then applied exclusively on the masked token. Notably, this process effectively performs masked prediction: the model receives a “[MASK]” input at that position and is tasked with predicting the token content. Compared with standard next-token prediction, our approach better handles the challenge of dynamic next positions; and compared with conventional masked prediction, it avoids the drawbacks of parallel independent sampling while retaining the benefits of causal sampling. Although this method doubles the sequence length, the performance gains justify the trade-off (see Sec. 3.8). Furthermore, as shown in Sec. 3.9, our efficiency optimizations enable masked next-token prediction with an inference speed reduction of only 15% to 30% compared to standard next-token prediction.
2.3 Unified 3D Generation and Understanding Framework
With the two optimizations described above, Uni-3DAR reduces the token count by approximately 8x while maintaining strong performance. Additionally, by leveraging duplicated masked tokens, Uni-3DAR can unify a broad range of existing 3D structural tasks within a single model.
Specifically, 3D structural generation and understanding tasks can be categorized into four types, which may be applied individually or in combination:
1. Single-Frame Generation: This task processes a single 3D structure. Generation may be unconditional or conditioned on external modalities such as properties or textual descriptions. In Uni-3DAR, generation is performed on masked tokens, making this the most straightforward application.
2. Multi-Frame Generation: This task involves generating a sequence of 3D structures, typically conditioned on an initial structure. Common applications include molecular dynamics, where an initial conformation conditions the generation of subsequent states, and pocket-based generation, where a 3D protein pocket guides the synthesis of a binding molecule. Leveraging Uni-3DAR’s autoregressive generation, structures are produced frame by frame, with each frame distinguished by an additional input embedding for the frame index. Notably, even if only a single frame is generated, the task is still classified as multi-frame generation.
3. Token-Level Understanding: This task focuses on predicting properties of individual tokens. For example, in microscopic structures, tokens (atoms) possess atom-level properties such as partial charges, energies, and forces. To support token-level understanding, Uni-3DAR incorporates an additional prediction head on fine-grained structural tokens.
4. Structure-Level Understanding: This task captures global properties of 3D structures. In small molecules, for example, key properties include solubility, toxicity, and quantum mechanical attributes. To enable structure-level understanding, Uni-3DAR attaches a dedicated prediction head to the “[EoS]” (End of Structure) token. Given the scarcity of labeled data, prior works frequently employ unsupervised pretraining to enhance performance. Similarly, we can pretrain Uni-3DAR on large-scale unlabeled 3D structural data using the masked next-token prediction approach, enabling efficient fine-tuning for property prediction in alignment with existing models.
In Uni-3DAR, each token is assigned a distinct role, enabling the model to address four tasks within a unified framework. Specifically, generation (both single- and multi-frame) is performed on masked tokens, token-level understanding on fine-grained structural tokens, and structure-level understanding via the “[EoS]” token. Because these tasks operate independently on different tokens, they do not interfere with one another. As a result, Uni-3DAR unifies 3D structural generation and understanding in a single model and supports joint training across tasks.
Furthermore, the autoregressive nature of Uni-3DAR allows seamless integration of data from other modalities. This capability is particularly valuable because numerous 3D structural tasks inherently involve multi-modal inputs. For instance, protein folding commonly relies on protein sequences
6


as conditioning inputs to generate corresponding 3D structures. Similarly, tasks such as predicting microscopic 3D structures from powder X-ray diffraction (PXRD) or nuclear magnetic resonance spectroscopy (NMR) signals benefit from a multi-modal, autoregressive approach. Sec. 3.2 further illustrates this approach through the example of PXRD-guided crystal structure prediction.
2.4 Implementation Details
We detail the implementation of microscopic 3D structure modeling, which is the primary focus of this study. Notably, apart from the fine-grained tokenization tailored for atoms, most other components can be seamlessly adapted to other 3D structures.
Tokenization Details We always apply a random rotation before tokenization for data augmentation. For all experiments, we set the final tree level’s cell edge length to cL−1 = 0.24Å and use a resolution of cr = 0.01Å. The number of tree levels L is chosen based on the system size. With these settings, the token count is approximately 10 times the number of atoms after applying both 2-level subtree compression and duplicated masked tokens. For example, on the QM9 dataset [25], we use L = 6, resulting in roughly 160 tokens per structure (with an average of about 18 atoms), whereas using a full-size grid would yield (26)3 = 262, 144 tokens. During training, we found that randomly dropping a small fraction of tokens from the final octree layer (using a ratio sampled from [0, γ], where γ is a hyperparameter) improves overall performance. We hypothesize that this noise injection enhances robustness in generating fine-grained tokens, thereby boosting final performance.
Model Architecture We adopt a modern transformer decoder-only architecture [19]. Each layer consists of a uni-directional self-attention module and a SwiGLU [26] feed-forward network. We use a pre-norm design [27] with RMSNorm [28] for normalization. Unless otherwise specified, our default configuration is based on the GPT-2 model size, with 12 layers, an embedding dimension of 768, and a head dimension of 64, resulting in a model with approximately 90M parameters.
Input Embedding and Positional Encoding For the i-th token, its content consists of a token type and in-cell positions, denoted as (ti, ei) ∈ N4. We further define its tree level as li ∈ N, its
frame index as fi ∈ N (if it is a multi-frame generation), and its cell position as ci ∈ R3. For octree and masked tokens, ci is set to the center of the corresponding cell; for atom tokens, where each cell contains one atom, we use the atom’s precise coordinate for ci to better represent its position. Then, (ti, ei, li, fi) are converted into numerical features by summing their corresponding embedding layers, which then serve as the model’s input. Notably, we do not include any features from 2D graphs, such as bond information, ensuring that the proposed method can be applied to a wide range of 3D data. For pair-wise positional information, we simply apply RoPE-3D [29] to ci.
Generation Heads The generation tasks are performed exclusively on the masked token. For octree tokens, since ei is fixed to Np/2, only ti needs to be predicted, which can be accomplished with a simple classification head. In contrast, for atom tokens, both ti and ei must be predicted. We implement two solutions: autoregressive prediction and diffusion prediction. In both approaches, we first predict ti. Then, for autoregressive prediction, we sequentially predict (e0
i , e1
i , e2
i ), using Alg. 1. For diffusion prediction, we adopt the token-level diffusion module from MAR [23] to predict ei within each token. Since ei originates from a continuous 3D space, employing diffusion to generate it is a reasonable choice. In our empirical experiments, both methods achieve similar performance (See Sec.3.8); therefore, by default, we use the more efficient autoregressive prediction. During inference, to balance quality and diversity, we first sample with a slightly higher temperature to enhance diversity, then select the top-r samples ranked by cumulative autoregressive probabilities. Empirically, this approach outperforms low-temperature sampling in generating diverse yet high-quality samples.
Efficiency Optimizations During training, we leverage FlashAttention [30] with bfloat16 to accelerate computation and reduce peak memory usage. Additionally, we employ sequence packing by concatenating tokens from multiple samples into a single large sequence, thereby eliminating the extra cost associated with traditional padding solutions. With these optimizations, Uni-3DAR can efficiently train on large systems with many atoms, such as proteins. During inference, we utilize a KV-cache to accelerate prediction. Furthermore, to accelerate masked next-token prediction, we generate tokens in pairs rather than one-by-one as in standard next-token prediction. Since the inputs for masked tokens are predetermined and do not require model prediction, we can pack two adjacent tokens together to increase GPU utilization.
7


Algorithm 1 A Simple Autoregressive Head for Sequential Target Prediction
Require: Input tensor x, number of targets n, prediction heads for each target {pred_heads}, embedding layers for each prediction {emb_layers} 1: y ← x 2: Initialize preds ← {} 3: for i ← 1 to n do
4: p ← pred_heads[i](y) 5: Append p to preds 6: y ← y + emb_layers[i](p) {Teacher-forcing during training} 7: end for 8: return preds
Table 1: Our experiments cover a broad spectrum of real-world tasks, each of which can be seamlessly adapted by the unified framework of Uni-3DAR.
Section Data Type Single-Frame Gen. Multi-Frame Gen. Token Und. Structure Und.
Sec. 3.1 Molecule ✓
Sec. 3.2 Crystal + PXRD ✓
Sec. 3.3 Protein ✓ ✓
Sec. 3.4 Protein + Molecule ✓
Sec. 3.5 Molecule ✓ ✓
Sec. 3.6 Polymer ✓ ✓
3 Experiments
In this section, we perform real-world experiments to empirically validate the effectiveness of our proposed framework. Specifically, Table 1 summarizes key information across these tasks, illustrating that Uni-3DAR can address a diverse range of tasks with various data types within a unified framework.
3.1 3D Small Molecule Generation
Generating small organic molecules with accurate 3D conformations is a classical, benchmarkrich task in molecular modeling, yet the inherent flexibility due to rotatable bonds and diverse conformations poses significant challenges. Evaluating Uni-3DAR on this task directly tests its capability to generate realistic 3D molecular structures through a straightforward application of its single-frame generation methodology.
Dataset and Metric Consistent with previous studies [11], we use the QM9 [31] and GEOM-DRUG [32] datasets for unconditional 3D molecular generation. QM9, a widely-used molecular machine learning benchmark, contains 130K small molecules with high-quality 3D conformations (up to 9 heavy atoms and 29 total atoms including hydrogens), split into training (100K), validation (18K), and test sets (13K). GEOM-DRUG, in contrast, features larger organic compounds containing up to 181 atoms (averaging 44.2 atoms across 5 types), covering approximately 37 million conformations for around 450K unique molecules. Following established protocols [11], we select the 30 lowest-energy conformations per molecule for training.
Model performance is evaluated based on chemical feasibility. Bond types (single, double, triple, or none) are inferred from molecular geometries using pairwise atomic distances and atom types. Metrics include Atom Stability (the fraction of atoms exhibiting correct valency), Molecule Stability (the percentage of molecules where all atoms are stable), validity (percentage of chemically valid molecules verified by RDKit), and uniqueness (percentage of unique compounds among generated molecules). Metrics are computed consistently using the evaluation code from previous studies [11].
8


Table 2: Performance comparison on unconditional 3D molecular generation. Results for UniGEM are marked with an asterisk (*) to indicate the use of additional molecular property information during training to enhance generation performance.
QM9 DRUG
Atom Sta(%)↑ Mol Sta(%)↑ Valid(%)↑ V × U(%)↑ Atom Sta(%)↑ Valid(%)↑
Data 99.0 95.2 97.7 97.7 86.5 99.9
ENF [34] 85.0 4.9 40.2 39.4 - G-Schnet [33] 95.7 68.1 85.5 80.3 - GDM [11] 97.0 63.2 - - 75.0 90.8 GDM-AUG [11] 97.6 71.6 90.4 89.5 77.7 91.8 EDM [11] 98.7 82.0 91.9 90.7 81.3 92.6 EDM-Bridge [35] 98.8 84.6 92.0 90.7 82.4 92.8 GeoLDM [12] 98.9 89.4 93.8 92.7 84.4 99.3 UniGEM* [36] 99.0 89.8 95.0 93.2 85.1 98.4
Uni-3DAR 99.4 93.7 98.0 94.0 85.5 99.4
Baselines and Implementation We benchmark Uni-3DAR against established models, including G-SchNet [33], ENF [34], EDM [11] and its variants GDM [11], EDM-Bridge [35], GeoLDM [12], and UniGEM [36], which uses additional molecular properties to enhance generation performance.
Uni-3DAR employs a single-frame generation approach with a batch size of 64 for QM9 and 128 for GEOM-DRUG. The model is trained for 500K steps (approximately 320 epochs for QM9 and 12 epochs for GEOM-DRUG). We apply a peak learning rate of 3e-4, incorporating a 6% linear warmup phase followed by cosine decay. Training duration is approximately 6.9 hours on 4 NVIDIA 4090 GPUs for QM9 and around 11.7 hours on 8 NVIDIA 4090 GPUs for GEOM-DRUG.
Results Table 2 clearly demonstrates Uni-3DAR’s superior performance, significantly surpassing all baseline models. On the QM9 dataset, Uni-3DAR achieves notable improvements in Molecule Stability and Validity, substantially outperforming the second-best method. These results highlight Uni-3DAR’s robust capability to generate high-quality molecules. Furthermore, even compared to UniGEM, which benefits from additional molecular property information, Uni-3DAR achieves superior performance using only 3D molecular data. This further underscores the efficacy and robustness of the proposed Uni-3DAR model.
3.2 Crystal Generation
Tasks Unlike organic molecules, crystal structures are typically rigid with stable conformations. However, crystals introduce unique challenges due to their inherent symmetry and periodic arrangement in 3D space. A crystal is conventionally represented by its lattice (a parallelepiped unit cell) along with atomic details, including atom types and their coordinates within the lattice.
In Uni-3DAR, crystal structure generation is approached as a two-frame generative process: first generating the eight vertices defining the lattice, followed by generating the atomic configurations inside the generated lattice. Notably, unlike previous methods employing fractional coordinates, we consistently use physical coordinates to maintain uniformity across various molecular data types.
Based on this generation approach, we define and address three distinct tasks:
1. De Novo Crystal Generation: Learning the distribution of crystal structures from data to generate novel samples unconditionally.
2. Crystal Structure Prediction (CSP): Predicting crystal structures from given chemical compositions (atom types and counts). During inference, the chemical composition is provided as condition, enabling the model to generate the corresponding crystal structure.
3. PXRD-guided Crystal Structure Prediction: Establishing a cross-modal mapping from powder X-ray diffraction (PXRD) signals and chemical compositions to reconstruct crystal structures that accurately match observed PXRD patterns. This task has significant practical implications, as PXRD analysis is widely used in crystal structure determination and validation of novel materials in real-world scenarios.
9


Table 3: Results on de novo crystal generation. Baseline results are taken from Xie et al. [37]. For FlowMM, we report its result with 1000 integration steps.
Data Method Validity (%) ↑ Coverage (%) ↑ Property ↓
Struc. Comp. COV-R COV-P dp dE delem
Carbon-24 FTCP [43] 0.08 – 0.00 0.00 5.206 19.05 G-SchNet [44] 99.94 – 0.00 0.00 0.9427 1.320 P-G-SchNet [44] 48.39 – 0.00 0.00 1.533 134.7 CDVAE [37] 100.0 – 99.80 83.08 0.1407 0.2850 DiffCSP [38] 100.0 – 99.90 97.27 0.0805 0.0820 Uni-3DAR 99.99 – 100.0 98.16 0.0660 0.0289 
MP-20 FTCP [43] 1.55 48.37 4.72 0.09 23.71 160.9 0.7363 G-SchNet [44] 99.65 75.96 38.33 99.57 3.034 42.09 0.6411 P-G-SchNet [44] 77.51 76.40 41.93 99.74 4.04 2.448 0.6234 CDVAE [37] 100.0 86.70 99.15 99.49 0.6875 0.2778 1.432 DiffCSP [38] 100.0 83.25 99.71 99.76 0.3502 0.1247 0.3398 FlowMM [39] 96.85 83.19 99.49 99.58 0.239 – 0.083 Uni-3DAR 99.89 90.31 99.62 99.83 0.4768 0.1237 0.0694
Table 4: Results on crystal structure prediction (CSP) and PXRD-guided CSP. For a fair comparison, we report UniGenX results obtained from the model trained from scratch, rather than using its default configuration that leverages large-scale datasets for additional pretraining and fine-tuning.
Method Carbon-24 MPTS-52 MP-20 MP-20 (PXRD-Guided)
Match Rate (%) ↑
RMSE ↓
Match Rate (%) ↑
RMSE ↓
Match Rate (%) ↑
RMSE ↓
Match Rate (%) ↑
RMSE ↓
CDVAE [37] 17.09 0.2969 5.34 0.2106 33.90 0.1045 – DiffCSP [38] 17.54 0.2759 12.19 0.1786 51.49 0.0631 – FlowMM [39] 23.47 0.4122 17.54 0.1726 61.39 0.0566 – UniGenX [45] 27.09 0.2264 29.09 0.1256 63.88 0.0598 – PXRDGEN [46] – – – – – – 68.68 0.0707
Uni-3DAR 31.23 0.2194 32.44 0.0684 65.48 0.0317 75.08 0.0276
Dataset and Metric We employ established datasets consistent with prior studies [37, 38, 39] for both training and evaluation purposes. Specifically, we employ the Carbon-24 dataset [40], containing 10,153 carbon-based structures with cells composed of 6 to 24 atoms. The MP-20 dataset [41], derived from the Materials Project [41], includes 45,231 stable inorganic materials representing a wide range of experimentally validated compounds, each containing up to 20 atoms per cell. Additionally, we use the more challenging MPTS-52 dataset, an extended version of MP-20, comprising 40,476 structures with up to 52 atoms per cell, organized by the earliest publication year. We follow the same data split strategy as outlined in previous work [38].
To evaluate de novo crystal generation performance, we adopt the standard evaluation framework proposed by Xie et al. [37], which includes three key metrics: validity, coverage, and property statistics. Validity quantifies the proportion of generated structures that satisfy established physical plausibility criteria. Coverage measures the ability of generated structures to capture the diversity present in the test set. Property statistics compare essential attributes such as density, formation energy, and elemental composition between generated and ground-truth distributions.
For assessing performance in CSP and PXRD-guided CSP tasks, we align our evaluation methodology with prior research [39]. We compute the top-1 match rate alongside the corresponding average root-mean-square error (RMSE) for matched structures. We employ StructureMatcher[42], using thresholds set to stol=0.5, angle_tol=10, and ltol=0.3, consistent with the methodology of previous studies [39].
Baseline Models and Implementation We benchmark Uni-3DAR against established methods, including FTCP [43], G-SchNet [44], P-G-SchNet [44], CDVAE [37], DiffCSP [38], and FlowMM [39].
10


Additionally, we evaluate Uni-3DAR against the recent UniGenX [45] for the CSP task. For PXRDguided CSP, we compare Uni-3DAR with PXRDGEN [46], a model tailored for this task.
In Uni-3DAR, we use a 12-layer model with a 768-dimensional embedding for de novo crystal generation, while a larger 24-layer model with a 1024-dimensional embedding is employed for CSP and PXRD-guided CSP tasks. All models are trained for 400k steps with a batch size of 64 and a peak learning rate of 3e-4. For chemical composition conditioning, we prepend a token derived from a multi-hot atom-type vector. PXRD data, spanning angles from 0° to 120°, is converted into a 1200-dimensional vector with a 0.1° resolution, evenly divided into four segments, each represented by a conditional token. As a result, PXRD-guided CSP utilizes a total of five conditional tokens (one for composition and four for PXRD signals). The autoregressive nature of Uni-3DAR enables seamless integration of these conditional tokens, eliminating the need for additional encoders required by previous methods [46, 47].
Results of De Novo Crystal Generation The performance of Uni-3DAR on the Carbon-24 and MP-20 datasets is presented in Table 3. On Carbon-24, Uni-3DAR outperforms existing models, particularly excelling in coverage, demonstrating its ability to generate diverse and realistic structures. On MP-20, Uni-3DAR significantly enhances component validity compared to previous approaches while maintaining competitive performance on other metrics. These results underscore Uni-3DAR’s strength in producing chemically valid crystal structures that closely align with key physical and chemical properties.
Results of Crystal Structure Prediction (CSP) We evaluate Uni-3DAR’s performance on CSP across all datasets, as summarized in Table 4. Uni-3DAR consistently outperforms baseline methods by significant margins. Specifically, on Carbon-24, it improves the match rate by 4.14% over the previous best method, demonstrating superior accuracy in reconstructing crystal structures. On MP-20, Uni-3DAR achieves a substantial improvement in RMSE, reducing it from 0.0566 to 0.0317, a relative improvement of 178% over the second-best model. Furthermore, on MPTS-52, Uni-3DAR achieves an impressively low RMSE of 0.0684, representing a 184% relative improvement, despite the increased structural complexity. This result highlights its exceptional precision in atomic placement. Overall, these findings demonstrate Uni-3DAR’s strong generalization capability across datasets of varying difficulty levels.
Results of PXRD-Guided CSP Table 4 demonstrates Uni-3DAR’s performance in PXRD-guided CSP on the MP-20 dataset, benchmarked against PXRDGEN [46]. Uni-3DAR substantially outperforms PXRDGEN, elevating the match rate from 68.68% to 75.08% while drastically reducing the RMSE from 0.0707 to 0.0276—a 256% relative improvement. This significant RMSE reduction underscores Uni-3DAR’s exceptional ability to generate crystal structures that precisely correspond to experimental PXRD patterns. Collectively, these results underscore the superior capability of Uni-3DAR in harnessing diffraction constraints to reliably predict crystal structures.
3.3 Protein Pocket Prediction
Proteins are a crucial class of biological structures, and accurate prediction of binding pockets is essential for de novo drug design and applications such as molecular docking. Traditionally, pocket prediction is formulated as an atom-level or residue-level classification task. Each atom or residue is assigned a binary label indicating whether it belongs to a binding pocket. We adopt this classical formulation to evaluate Uni-3DAR’s token-level understanding capabilities.
Dataset and Metric We follow previous studies [48] and employ a binding site dataset constructed from the CASF-2016 core set [49], PDBBind v2020 refined set [50], and MOAD [51]. The dataset consists of 23k training samples, 5k validation samples, and five test sets of roughly 1k samples each. Model performance is assessed using the Intersection-over-Union (IoU) metric, consistent with previous evaluations [48].
Baselines and Implementation We benchmark Uni-3DAR against established methods. Our comparisons include non-pretrained approaches (e.g., FPocket [52], SiteHound [53], etc.) and pretrained models (e.g., ESM2_150M [54], GearNet [55], Siamdiff [56], and Vabs-Net [48]). In line with prior works [48], we pretrain Uni-3DAR on approximately 1.3 million protein structures before
11


Table 5: Results for atom-level binding site prediction measured by IoU (%). Baseline results are taken from Zhao et al. [48]. For a fair comparison with other methods, we report Vabs-Net’s result using only α-carbon atoms.
Method pretrained B277↑ DT198↑ ASTEX85↑ CHEN251↑ COACH420↑
FPocket [52] × 31.5 23.2 34.1 25.4 30.0 SiteHound [53] × 36.4 23.1 38.9 29.4 34.9 MetaPocket2 [57] × 37.3 25.8 37.5 32.8 37.7 DeepSite [58] × 34.0 29.1 37.4 27.4 33.9 P2Rank [59] × 49.8 38.6 47.4 56.5 45.3
ESM2_150M [54] √ 19.6 16.6 20.5 18.9 22.0
GearNet [55] √ 39.9 35.8 41.0 36.4 41.3
Siamdiff [56] √ 37.7 31.0 40.7 35.3 40.3
Vabs-Net [48] √ - - - - 56.3
Uni-3DAR √ 53.4 46.7 51.4 47.9 56.2
Table 6: Comparison of docking performance on the Top1- and Top5-RMSD metrics. The first group of five baselines comprises classical docking software, while the second group of eight baselines consists of deep learning–based methods. The results are reproduced directly from Cao et al. [60]. The best outcomes are shown in bold, and the second-best are underlined.
Top1-RMSD Top5-RMSD
%<1Å ↑ %<2Å ↑ Med(Å) ↓ %<1Å ↑ %<2Å ↑ Med(Å) ↓
Uni-Dock [61] 32.51±0.39 50.69±0.59 1.89±0.04 47.11±0.22 67.03±0.94 1.10±0.02 Glide SP [62] 17.36±0.00 44.63±0.00 2.27±0.00 31.13±0.00 60.06±0.00 1.54±0.00 GNINA [63] 21.12±0.26 43.62±1.06 2.45±0.07 28.47±0.57 58.13±0.81 1.65±0.02 SMINA [64] 18.73±0.00 31.68±0.00 3.99±0.00 28.47±0.56 48.48±0.00 2.07±0.00 Vina [65] 18.32±0.02 36.64±0.05 3.42±0.01 24.79±0.00 50.96±0.00 1.87±0.01
EquiBind [66] / 5.5±1.2 6.2±0.3 / / / TANKBind [67] 2.66±0.26 18.18±0.60 4.2±0.05 4.13±0.0 20.39±0.45 3.5±0.04 E3Bind [68] / 25.6 7.2 / / / KarmaDock [69] / 56.2 / / / / DiffDock(Pocket) [70] / 51.8 2.0 / 60.7 1.9 DiffDock [70] 15.15 36.09 3.35 21.76 43.52 2.46 DiffDock-L [71] 19.07±0.57 40.74±1.25 2.88±0.18 21.95±0.39 48.15±0.91 2.05±0.04 SurfDock [60] 40.96±0.34 68.41±0.26 1.18±0.00 54.18±0.13 75.11±0.13 0.94±0.00
Uni-3DAR 44.75±2.63 69.06±0.75 1.08±0.04 56.35±1.99 72.38±0.73 0.76±0.02
fine-tuning it on the binding site dataset. Unlike Vabs-Net, which employs full-atom representations, our experiments are restricted to α-carbon atoms to facilitate direct comparisons.
Pretraining is conducted using a single-frame generation approach for 300k steps with a batch size of 64. We use a peak learning rate of 3e-4 with a 10% linear warmup followed by cosine decay, which requires approximately 19 hours on 16 NVIDIA A100 GPUs. Fine-tuning adopts an atom-level classification strategy, conducted for 100 epochs with a batch size of 32, a peak learning rate of 1e-4, requiring roughly 7 hours on 8 NVIDIA A100 GPUs.
Results As summarized in Table 5, Uni-3DAR achieves state-of-the-art performance compared to other baselines. Importantly, Uni-3DAR reaches comparable or superior results despite using only 3D protein structures, whereas methods like Vabs-Net additionally leverage ESM embeddings and Solvent Accessible Surface Area. These findings underscore Uni-3DAR’s strong capacity for accurately understanding protein 3D structures and effectively performing atom-level tasks.
12


3.4 Molecular Docking
Molecular docking predicts how a ligand binds to a target protein, playing a crucial role in drug discovery. In Uni-3DAR, this process is structured as a three-frame generation task. The first two frames represent the protein and the initial ligand, both provided as inputs during inference, while the third frame corresponds to the predicted docked conformation of the ligand.
Dataset and Metric Following Cao et al. [60], we train and evaluate docking methods on the PDBbind2020 dataset. The training and validation set consists of 17,000 complexes from 2018 or earlier, while the test set includes 363 structures from 2019, ensuring no ligand overlap with the training data. Given a protein-binding pocket and a randomly generated ligand conformation from RDKit, the goal is to generate a user-specified number of poses (set to 40, as in Cao et al. [60]). Docking methods typically incorporate a confidence scoring mechanism to rank these poses. Performance is assessed using the percentage of predictions with RMSD < 1Å and RMSD < 2Å, as well as the median RMSD for the top-ranked pose and the best pose among the top five ranked poses.
Baselines and Implementation We evaluate Uni-3DAR against 13 baselines, including five classical docking software tools and eight deep learning–based methods. Most existing deep learning approaches rely on complex featurizations, such as using protein language model embeddings (e.g., from ESM2 [72]). To simplify and unify the molecular tasks, we omit these complicated features in Uni-3DAR and instead use only atom types and coordinates. We also adopt a full-atom representation of the protein pocket to enhance expressive power. We frame docking as an autoregressive generation task by embedding both the pocket and the RDKit conformation as two frames, concatenating them into a single input sequence, and training the model to generate the docked molecule conformation as a new frame sequence. For further simplicity, we do not impose constraints such as matching the number and types of atoms in the output frame to those of the input molecule. Also, we do not train a separate scoring model for pose ranking. Instead, we use the cumulative probability derived from autoregressive generation to score each generated pose. We train Uni-3DAR for 300k steps (approximately 300 epochs) with a batch size of 16. The learning rate schedule follows the same configuration as the experiments detailed in section 3.1. The training is completed in approximately one day on 4 NVIDIA A100 GPUs.
Results Experimental results are summarized in Table 6. Uni-3DAR outperforms to the state-ofthe-art method, SurfDock, demonstrating similar percentages of poses with RMSD below 1Å and 2Å. Notably, Uni-3DAR excels in generating higher-quality poses, reflected by its lower median RMSD values. However, Uni-3DAR exhibits slightly inferior performance in selecting Top-5 poses for challenging cases, as evidenced by a lower percentage of poses with RMSD below 2Å in the Top5RMSD evaluation (72.38% vs. 75.11% for SurfDock). This gap may arise because the scoring module in Uni-3DAR has not been explicitly trained and is only exposed to ground-truth conformations during the training phase. Addressing this limitation by training a dedicated scoring module could potentially enhance its selection performance. Moreover, since Uni-3DAR avoids complex feature engineering, its docking accuracy might further benefit from multitask learning strategies, emphasizing the promise of a unified foundational model for molecular applications.
3.5 Molecular Property Prediction via Pretraining
Molecular property prediction through pretraining has emerged as an effective strategy to address data scarcity challenges in areas like drug discovery and material design. As a classical task with established benchmarks, molecular property prediction directly assesses a model’s capacity to comprehend 3D molecular structures. Applying Uni-3DAR’s structure-level understanding framework is thus straightforward.
Dataset and Metric We utilize the same pretraining dataset as employed by Uni-Mol [8] and SpaceFormer [73], comprising approximately 19 million molecules. For downstream evaluations, we follow the datasets and evaluation settings used by the state-of-the-art SpaceFormer [73]. These include a 20K dataset predicting electronic properties (HOMO, LUMO, GAP), a 21K dataset targeting energy properties (E1-CC2, E2-CC2, f1-CC2, f2-CC2), and an 8K dataset predicting mechanical and electronic properties (Dipmom, aIP, and D3 Dispersion Corrections). Data splits align exactly with
13


Table 7: Results on molecular property prediction performance. The best results are highlighted in bold, and the second-best results are underlined. Baseline results are taken from Lu et al. [73].
Model HOMO↓
(Hartree)
LUMO ↓
(Hartree)
GAP ↓
(Hartree)
E1-CC2 ↓
(eV)
E2-CC2 ↓
(eV)
f1-CC2 ↓ f2-CC2 ↓ Dipmom ↓ (Debye)
aIP↓
(eV)
D3_disp↓ _corr (eV)
GROVER 0.0075
± 2.0e-4
0.0086
± 8.0e-4
0.0109
± 1.4e-3
0.0101
± 9.7e-4
0.0129
± 4.6e-4
0.0219
± 3.5e-4
0.0401
± 1.2e-3
0.0752
± 1.1e-3
0.1467
± 1.5e-2
0.2516
± 5.3e-2
GEM 0.0068
± 7.0e-5
0.0080
± 2.0e-5
0.0107
± 1.9e-4
0.0090
± 1.3e-4
0.0102
± 2.3e-4
0.0170
± 4.3e-4
0.0352
± 5.4e-4
0.0289
± 1.2e-3
0.0207
± 2.6e-4
0.0077
± 6.6e-4
3D Infomax 0.0065
± 1.0e-5
0.0070
± 1.0e-4
0.0095
± 1.0e-4
0.0089
± 2.0e-4
0.0091
± 3.0e-4
0.0172
± 4.0e-4
0.0364
± 9.0e-4
0.0291
± 1.7e-3
0.0526
± 1.4e-4
0.2285
± 7.5e-3
Uni-Mol 0.0052
± 2.0e-5
0.0060
± 6.0e-5
0.0081
± 4.0e-5
0.0067
± 4.0e-5
0.0080
± 4.0e-5
0.0143
± 2.0e-4
0.0309
± 9.4e-4
0.0106
± 3.1e-4
0.0095
± 6.4e-4
0.0047
± 5.6e-4
Mol-AE 0.0050
± 8.0e-5
0.0057
± 4.7e-4
0.0080
± 8.0e-5
0.0070
± 6.0e-5
0.0080
± 4.0e-5
0.0140
± 4.0e-5
0.0307
± 1.3e-3
0.0113
± 4.7e-4
0.0103
± 1.3e-4
0.0077
± 1.3e-3
SpaceFormer 0.0042
± 1.0e-5
0.0040
± 2.0e-5
0.0064
± 1.2e-4
0.0058
± 8.0e-5
0.0074
± 8.4e-5
0.0142
± 3.7e-4
0.0294
± 7.1e-4
0.0083
± 5.0e-4
0.0090
± 5.9e-4
0.0053
± 1.2e-3
Uni-3DAR 0.0048
± 2.1e-5
0.0044
± 3.2e-5
0.0065
± 8.8e-5
0.0056
± 2.2e-5
0.0067
± 2.0e-5
0.0134
± 7.0e-5
0.0286
± 1.6e-4
0.0114
± 6.9e-4
0.0127
± 1.1e-4
0.0052
± 3.2e-4
SpaceFormer’s methodology [73]. Performance across all tasks is measured using the Mean Absolute Error (MAE) metric.
Baselines and Implementation Our baselines encompass several prominent models, including Uni-Mol [8], Mol-AE [74], 3D Infomax [75], GROVER [76], GEM [77], and the most recent state-of-the-art method, SpaceFormer [73]. For pretraining, we use the proposed masked next-token prediction as pretraining task, training the model for 500k steps with a batch size of 128. The peak learning rate is set to 3e-4, incorporating a 10% linear warmup followed by cosine decay, requiring approximately 11.5 hours on 8 NVIDIA 4090 GPUs.
During fine-tuning, we adopt a structure-level understanding strategy, supplemented by a masked next-token prediction auxiliary generative loss. Training is conducted over a maximum of 200 epochs. We systematically explore hyperparameter combinations, considering two batch sizes (32, 64) and two learning rates (5e-4, 1e-4), resulting in four distinct setups. For each hyperparameter configuration, models are trained three times using different random seeds, and we report the mean performance along with standard deviation. The best-performing model based on validation loss is selected for evaluation.
Results The experimental results summarized in Table 7 clearly illustrate the strong performance of Uni-3DAR. Uni-3DAR achieves top performance, ranking first in 4 out of the 10 tasks evaluated and placing within the top two positions in 8 out of 10 tasks. Notably, Uni-3DAR demonstrates comparable performance to the specialized SpaceFormer model, indicating that Uni-3DAR possesses robust representational capabilities competitive with state-of-the-art specialized representation models.
3.6 Polymer Property Prediction via Pretraining
Polymers, synthesized through various polymerization methods such as addition, ring-opening, and condensation, consist of repeating monomer units. These materials play essential roles across multiple fields, including materials science, drug design, and bioinformatics, necessitating accurate property prediction methods. Here, we demonstrate Uni-3DAR’s structure-level understanding capability by focusing on homopolymer property prediction.
Dataset and Metric Following prior research [78, 79], we use eight publicly available polymer property datasets (Egc, Egb, Eea, Ei, Xc, EPS, Nc, and Eat), obtained via density functional theory (DFT) calculations. Given that all tasks involve structure-level regression, we employ a robust evaluation strategy using 5-fold cross-validation with random splits, consistent with previous work [79]. Results are reported as the root mean squared error (RMSE), averaged across three different random seeds.
14


Table 8: Polymer properties prediction performance. The best results are highlighted in bold, and the second-best results are underlined.
Model Egc ↓ Egb ↓ Eea ↓ Ei ↓ Xc ↓ Eps ↓ Nc ↓ Eat ↓ (eV) (eV) (eV) (eV) % 1 1 eV/atom
ChemBERTa 0.539
± 0.049
0.664
± 0.079
0.350
± 0.036
0.485
± 0.086
18.711
± 1.396
0.603
± 0.083
0.140
± 0.010
0.219
± 0.056
Uni-Mol 0.489
± 0.028
0.531
± 0.055
0.332
± 0.027
0.407
± 0.080
17.414
± 1.581
0.536
± 0.053
0.095
± 0.013
0.084
± 0.034
SML 0.489
± 0.056
0.547
± 0.110
0.313
± 0.016
0.432
± 0.060
18.981
± 1.258
0.576
± 0.020
0.102
± 0.010
0.062
± 0.014
PLM 0.459
± 0.036
0.528
± 0.081
0.322
± 0.037
0.444
± 0.062
19.181
± 1.308
0.576
± 0.060
0.100
± 0.010
0.050
± 0.010
polyBERT 0.553
± 0.011
0.759
± 0.042
0.363
± 0.037
0.526
± 0.068
18.437
± 0.560
0.618
± 0.049
0.113
± 0.003
0.172
± 0.016
Transpolymer 0.453
± 0.007
0.576
± 0.021
0.326
± 0.040
0.397
± 0.061
17.740
± 0.732
0.547
± 0.051
0.096
± 0.016
0.147
± 0.093
MMPolymer 0.431
± 0.017
0.503
± 0.038
0.286
± 0.029
0.390
± 0.057
16.814
± 0.867
0.511
± 0.035
0.087
± 0.010
0.061
± 0.016
Uni-3DAR 0.426
± 0.022
0.498
± 0.048
0.291
± 0.022
0.396
± 0.072
17.16
± 1.498
0.487
± 0.034
0.087
± 0.011
0.066
± 0.031
Baselines and Implementation Baseline methods include ChemBERTa [80], Uni-Mol [8], SML [78], PML [78], polyBERT [81], Transpolymer [82], and MMPolymer [79]. For pretraining, we represent homopolymers as specialized molecular structures using the star substitution strategy proposed in [79]. The model is pretrained using our masked next-token prediction strategy for 1 million steps with a batch size of 128. All other experimental details follow the settings previously described in the molecular property prediction experiments.
During fine-tuning, we adopt structure-level understanding strategy with masked next-token prediction auxiliary generative loss. Training is capped at 200 epochs. We thoroughly investigate various hyperparameter combinations by using three different batch sizes (32, 64, 128) and three learning rates (5e-4, 1e-4, 3e-4), creating nine unique configurations. Each configuration is tested by training models three times with different random seeds. We follow the same 5-fold split index align with [79], by averaging the best validation metrics in each fold. Subsequently, we present the mean performance along with the standard deviation across three seeds.
Results The experimental results, summarized in Table 8, highlight Uni-3DAR’s competitive performance. Uni-3DAR ranks first in 4 out of 8 tasks and is within the top two in 7 out of 8 tasks. These findings underscore Uni-3DAR’s robust representational capabilities across molecular and homopolymer property prediction tasks.
3.7 Mutual Benefits of Generation and Understanding Tasks
In the previous experiments, we applied Uni-3DAR independently to each task to ensure fair comparisons with established approaches, rather than employing joint training across multiple tasks and diverse data sources. Although earlier results already demonstrate Uni-3DAR’s effectiveness, the advantages of joint training, particularly combining generation and understanding tasks, remain less explored. Due to resource limitations, comprehensive large-scale joint training was not feasible in this paper. Nonetheless, this subsection presents two additional experiments that clearly illustrate how generation and understanding tasks can mutually reinforce each other, highlighting the potential for enhanced performance through joint training in Uni-3DAR.
The first experiment leverages the pretrained molecular representation described in Sec. 3.5. Typically, during downstream fine-tuning, we include an auxiliary generative loss by predicting ground-truth atom types and positions with the proposed masked next-token prediction. To investigate the contribution of this auxiliary generation task, we performed an ablation experiment by removing the generation loss during fine-tuning (results shown in Table 9). The results indicate a notable perfor
15


Table 9: In the molecular pretrained representation task, incorporating a generation loss during downstream fine-tuning improves performance.
HOMO ↓ LUMO ↓ E1-CC2 ↓ E2-CC2 ↓ (Hartree) (Hartree) (eV) (eV)
Uni-3DAR w/o Gen. loss 0.0052 0.0049 0.0063 0.0077 Uni-3DAR 0.0048 0.0044 0.0056 0.0067
Table 10: In the QM9 unconditional generation task, incorporating a structure-level understanding task further enhances the quality of the generated samples.
QM9
Atom Sta(%)↑ Mol Sta(%)↑ Valid(%)↑ V × U(%)↑
Uni-3DAR 99.4 93.7 98.0 94.0 Uni-3DAR w/ Structure Und. loss 99.6 95.8 98.5 93.1
mance drop without the generation loss, clearly demonstrating that generative training significantly strengthens structure-level understanding.
The second experiment builds upon the unconditional 3D molecule generation task using the QM9 dataset described in Sec. 3.1. Previously, to align with prior studies, we used only 3D molecular structure data. Here, we additionally incorporate a structure-level understanding task by predicting the molecular property U (internal energy at 298.15 K), with results shown in Table 10. Models trained with this auxiliary structure-level understanding task consistently outperform those without, especially in metrics such as molecular stability and validity. This demonstrates that structure-level understanding significantly enhances generative performance.
In summary, these experiments robustly illustrate that generation and understanding tasks positively reinforce one another. The findings underscore that integrating diverse datasets and joint task training can establish a more powerful and effective foundation model for 3D structural modeling.
3.8 Ablation Study
We conducted comprehensive ablation experiments on QM9 generation task (Sec. 3.1) to evaluate the contributions of key components in Uni-3DAR. The experimental results, summarized in Table 11, lead to the following insights:
1. Masked Next-Token Prediction significantly enhances generation performance. In experiment No.2, we followed previous work [20] that merely appends the position of the next token to the current token, without using our proposed masked next-token prediction. Comparing experiments No.1 and No.2 clearly demonstrates that our proposed masked next-token prediction substantially outperforms this baseline approach.
2. 2-Level Subtree Compression boosts efficiency without compromising performance. Experiment No.3 evaluates performance without 2-level subtree compression. Comparing No.1 (with compression) and No.3 (without compression), we observe that using subtree compression reduces token count by approximately 6x, leading to significantly faster training with comparable results. Interestingly, experiment No.4 (No.2 without subtree compression) outperforms No.2. This indicates that while subtree compression alone may slightly impact performance negatively (No.2 vs. No.4), when combined with masked next-token prediction (No.1 vs. No.3), it achieves comparable performance efficiently.
3. Coarse-to-fine octree tokens provide essential spatial information. In experiment No.5, we removed octree tokens, significantly degrading model performance. Without coarse-to-fine tokenization, the model degrades to atom-based autoregressive prediction of both atom types and positions, a much more challenging task. Our coarse-to-fine octree tokenization method effectively provides positional priors from preceding levels, substantially enhancing performance. This clearly validates the importance of the coarse-to-fine tokenization strategy for 3D structural generation.
16


Table 11: Ablation Studies for Uni-3DAR. MNTP (Masked Next-Token Prediction) boosts performance, while 2LSC (2-Level Subtree Compression) enhances efficiency. Uni-3DAR integrates both techniques to balance effectiveness and efficiency. Token-level diffusion loss (diff. loss) performs comparably to our proposed simple autoregressive head. Training cost is measured using 4 NVIDIA 4090 GPUs.
No. Settings QM9 # AVG. Training Atom Sta(%)↑ Mol Sta(%)↑ Valid(%)↑ V × U(%)↑ Tokens Cost ↓
1 Uni-3DAR 99.4 93.7 98.0 94.0 160 6.9h 2 1 w/o MNTP 98.7 88.2 97.0 91.5 80 6h 3 1 w/o 2LSC 99.4 94.4 98.2 92.1 1060 20h 4 2 w/o 2LSC 99.3 94.2 97.7 92.7 530 11h 5 2 w/o octree 87.7 25.3 72.1 65.7 18 3h 6 1 w/ diff. loss 99.4 93.6 98.2 94.0 160 7.8h 7 5 w/ diff. loss 88.3 35.4 67.3 46.5 18 5.6h
4. Token-level diffusion loss yields comparable performance to the autoregressive head but with lower efficiency. Our default generation head uses a simple autoregressive head (refer to Alg. 1) to sequentially predict atom types and in-cell positions. We examined whether employing a more powerful head, such as the token-level diffusion loss from MAR [23], could further enhance performance. Experiment No.6, utilizing the diffusion head, achieved similar results but required more computational time. Therefore, we opt for the simpler, more efficient autoregressive head by default.
5. Combining atom-based autoregressive and diffusion losses without spatial tokenization is insufficient. Recent works have explored improving atom-based autoregressive generation through token-level diffusion losses [45]. We tested this approach by adding a token-level diffusion loss to experiment No.5, resulting in experiment No.7. Although No.7 performed slightly better than No.5, it remained significantly inferior to the proposed Uni-3DAR. This underscores that comprehensive spatial information, as provided by our tokenization strategy, is crucial, mere integration of diffusion-based methods into atom-based autoregressive model, without spatial tokenization, cannot achieve substantial performance improvements.
3.9 Inference Speed
We benchmarked Uni-3DAR against the diffusion-based generative model GeoLDM [12] on QM9 generation task (Sec.3.1) by evaluating the throughput (i.e., the number of molecules generated per second). Model throughput was evaluated across a range of batch sizes, with all experiments conducted on a single Nvidia 4090 GPU. As shown in Fig.4, Uni-3DAR consistently outperforms the diffusion-based approach in sampling efficiency, achieving significantly reduced generation times across all tested settings. In particular, at larger batch sizes, Uni-3DAR is approximately 21.8x faster than GeoLDM, and even at a small batch size of 64, it remains about 7.5x faster. Additionally, we assessed the inference overhead introduced by masked next-token prediction. Thanks to our optimizations (Sec. 2.4), we find that masked next-token prediction incurs only a 15% to 30% slowdown. Given its substantial performance gains, this additional cost is well justified.
4 Related Work
Octree and Hierarchical Autoregressive Models The coarse-to-fine hierarchical structure is widely used in 3D vision [83, 84, 85, 86, 87, 20, 88, 89]. Among these works, [20] is most similar to our Uni-3DAR, as it also employs autoregressive generation using an octree. However, our method differs in three key aspects: (1) instead of relying on deep tree-level generation for fine details, we add an extra layer of fine-grained tokens to avoid excessively deep trees; (2) rather than compressing nodes via convolutional layers, we represent a compressed subtree with a single token; and (3) to handle dynamic token positions, while [20] appends the next position to the current token, we adopt a masked next-token prediction strategy. These innovations make Uni-3DAR more efficient and effective than [20]. Recently, some image generative models have adopted a coarse-to-fine, level-by-level generation approach, such as VAR [90]. Although the high-level idea appears similar,
17


64 128 256 512 1024
Batch Size
0
20
40
60
80
Number of Molecule Per Second
7.5×8.5×
12.1×
14.3×
16.7×
20.4× 19.1×
26.2×
21.8×
28.5×
Generation Speed Compared With Diffusion-based Method
GeoLDM Uni-3DAR Uni-3DAR w/o MNTP
64 128 256 512 1024
Batch Size
0
10
20
30
40
50
60
70
Number of Molecule Per Second
Generation Speed Compared On different Rank Ratio
GeoLDM Uni-3DAR r=0.2 Uni-3DAR r=0.4 Uni-3DAR r=0.8 Uni-3DAR r=1.0
Figure 4: Left: Uni-3DAR generation speed on different batch sizes compared with the diffusionbased method; Right: Uni-3DAR generation speed on different rank ratios r compared with the diffusion-based method (higher is better).
our motivation is distinct: Uni-3DAR is designed to avoid the inefficiencies of a full-size cubic grid, whereas VAR uses more tokens to boost performance. Moreover, Uni-3DAR remains within the next-token prediction framework, while VAR employs next-scale prediction to parallelize token generation at each level.
Microscopic 3D Structure Modeling Most previous generative models for microscopic 3D structures employ diffusion-based approaches [35, 91, 11, 12, 38] to generate atomic positions from noise. However, diffusion models have two major limitations. First, they require the number of atoms to be predetermined. Second, atom types are sampled from a categorical distribution, for which a proper score function is not well defined. Some studies have explored grid-based generation [18] to address these issues, but using a full-size 3D grid is computationally prohibitive. Other works have investigated autoregressive models for 3D molecules [14, 15, 45], but these models generate molecules atom by atom, requiring a predefined sequence order. Furthermore, during generation, the position of the next atom is unknown and must be predicted along with its type, making the process significantly more challenging and affecting overall performance.
For microscopic 3D structure understanding, prior studies primarily leverage SE(3)-invariant or equivariant models [92, 93]. Additionally, unsupervised pretraining is widely used to mitigate the scarcity of labeled data [75, 94, 74, 13, 8]. These models typically follow a BERT-style pretraining framework [95], where some atoms are masked, their 3D positions are perturbed, and the model is trained to recover the ground truth. While highly effective for understanding tasks, most of these models cannot be directly applied to generation.
Some recent efforts have attempted to unify generation and understanding for microscopic data. However, most focus solely on sequence data (e.g., 1D SMILES, nucleotide sequences, or textual descriptions) and directly apply autoregressive language models [96, 97, 98, 99]. While these models are straightforward, they lack essential 3D structural information, limiting their performance and applicability. Recent studies have also explored diffusion-based approaches. For example, UniGEM [36] demonstrated that a two-phase, multi-task training strategy can improve performance for both tasks. This approach combines diffusion loss with a prediction task applied during later diffusion steps. In summary, while previous work has made progress in bridging generation and understanding, Uni-3DAR is the first autoregressive framework to unify both tasks for 3D microscopic structure modeling.
Macroscopic 3D Structure Modeling In machine learning, macroscopic 3D structure modeling encompasses the understanding and generation of everyday objects [100, 101], scenes [102], CAD models [103, 104, 105], avatars [106], and more. Similar to microscopic 3D structures, macroscopic 3D structures lack a unified representation format. Commonly used 3D representations include voxels [107], point clouds [108], polygon meshes [109], implicit functions [85], and 3D Gaussian Splatting [110]. Recent methods, such as 3DShape2VecSet [111], Michelangelo [112], CLAY [88], and Dora [113], encode 3D shapes into compressed, compact latent codes, substantially improving
18


representation efficiency. Specifically, 3DShape2VecSet introduces a transformer-based 3D VAE that encodes uniformly sampled surface points into a compact latent code, upon which CLAY further builds using the latent Diffusion Transformer (DiT) [114]. Dora advances this line of work by improving both the surface sampling strategy and the 3D VAE architecture. Previous literature also explored autoregressive modeling for macroscopic 3D structures. For example, Polygen [115] and MeshGPT [116] generate mesh faces sequentially from lowest to highest on the vertical axis, corresponding to the point-based tokenization strategy as discussed in Sec. 1, suffering from the same challenges in dynamic token positions. All of these approaches, however, differ fundamentally from the proposed Uni-3DAR. Our unified framework can be applied directly to macroscopic datasets or combined with state-of-the-art representations for potentially even stronger performance.
Another category of 3D structure generation methods, known as optimization-based approaches [117, 118, 119, 120], leverages text-to-image generative models and refines 3D representations by distilling information from 2D images [120]. Unlike true 3D generation, these methods primarily perform 3D reconstruction, making them fundamentally distinct from the previously mentioned 3D generation techniques and Uni-3DAR.
5 Conclusion
In this paper, we introduced Uni-3DAR, a unified autoregressive framework for 3D structural generation and understanding. By leveraging a novel coarse-to-fine tokenization method based on an octree, along with 2-Level Subtree Compression and Masked Next-Token Prediction, Uni-3DAR efficiently captures the full spatial context while avoiding cubic token growth. Extensive experiments across a range of tasks on microscopic structures demonstrate that our approach consistently outperforms or matches existing methods while being significantly more efficient.
For future work, we aim to extend Uni-3DAR to a broader range of 3D structures, particularly macroscopic ones. Additionally, jointly training a large-scale Uni-3DAR foundation model on diverse data sources and tasks, as well as integrating it with other modalities, presents an exciting research direction.
Author Contributions
S. LU initially explored autoregressive modeling on 3D structure data, and conducted experiments for protein pocket prediction and small molecule pretraining. H. LIN implemented the token-level diffusion loss as well as the experiments for molecular docking, and revised the manuscript. L. YAO and G. KE conducted experiments for small molecule and crystal generation. Z. GAO conducted experiments for polymers. X. JI accelerated the inference pipeline and performed a comparative study of inference performance. L. Zhang and W. E contributed technical advice and revised the manuscript. G. KE led the project, proposed and implemented the ideas of two-level subtree compression and masked next-token prediction. He also conducted the ablation study and wrote the manuscript.
Acknowledgments
We thank Siyuan Liu, Zhen Wang, Shuwen Yang, and Xi Fang for their contributions during the early stages of this project.
References
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020.
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
19


[3] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.
[4] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024.
[5] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024.
[6] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583–589, 2021.
[7] Eric Alcaide, Zhifeng Gao, Guolin Ke, Yaqi Li, Linfeng Zhang, Hang Zheng, and Gengmo Zhou. Uni-mol docking v2: Towards realistic and accurate binding pose prediction. arXiv preprint arXiv:2405.11769, 2024.
[8] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=6K2RM6wVqKu.
[9] Shuqi Lu, Zhifeng Gao, Di He, Linfeng Zhang, and Guolin Ke. Highly accurate quantum chemical property prediction with uni-mol+, 2023.
[10] Shuqi Lu, Zhifeng Gao, Di He, Linfeng Zhang, and Guolin Ke. Data-driven quantum chemical property prediction leveraging 3d conformations with uni-mol+. Nature communications, 15 (1):7104, 2024.
[11] Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867–8887. PMLR, 2022.
[12] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592–38610. PMLR, 2023.
[13] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. arXiv preprint arXiv:2206.00133, 2022.
[14] Youzhi Luo and Shuiwang Ji. An autoregressive flow model for 3d molecular geometry generation from scratch. In International conference on learning representations (ICLR), 2022.
[15] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3d generative model for structure-based drug design. Advances in Neural Information Processing Systems, 34:6229–6239, 2021.
[16] Miha Skalic, José Jiménez, Davide Sabbadin, and Gianni De Fabritiis. Shape-based generative modeling for de novo drug design. Journal of chemical information and modeling, 59(3): 1205–1214, 2019.
[17] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. Learning a continuous representation of 3d molecular structures with deep generative models. arXiv preprint arXiv:2010.08687, 2020.
[18] Pedro O O Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Watkins, Stephen Ra, Vishnu Sresht, and Saeed Saremi. 3d molecule generation by denoising voxel grids. Advances in Neural Information Processing Systems, 36, 2024.
20


[19] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.
[20] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree transformer: Autoregressive 3d shape generation on hierarchically structured sequences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2698–2707, 2023.
[21] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020.
[22] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.
[23] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024.
[24] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11315–11325, 2022.
[25] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.
[26] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
[27] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pages 10524–10533. PMLR, 2020.
[28] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.
[29] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
[30] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.
[31] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.
[32] Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022.
[33] Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Müller, and Kristof T Schütt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):973, 2022.
[34] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling. E (n) equivariant normalizing flows. Advances in Neural Information Processing Systems, 34: 4181–4192, 2021.
[35] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and Qiang Liu. Diffusion-based molecule generation with informative prior bridges. Advances in Neural Information Processing Systems, 35:36533–36545, 2022.
[36] Shikun Feng, Yuyan Ni, Yan Lu, Zhi-Ming Ma, Wei-Ying Ma, and Yanyan Lan. Unigem: A unified approach to generation and property prediction for molecules. arXiv preprint arXiv:2410.10516, 2024.
21


[37] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. In International Conference on Learning Representations, 2021.
[38] Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure prediction by joint equivariant diffusion on lattices and fractional coordinates. In Workshop on ”Machine Learning for Materials” ICLR 2023, 2023. URL https://openreview.net/forum?id=VPByphdu24j.
[39] Benjamin Kurt Miller, Ricky T. Q. Chen, Anuroop Sriram, and Brandon M Wood. Flowmm: Generating materials with riemannian flow matching, 2024. URL https://arxiv.org/ abs/2406.04713.
[40] Chris J. Pickard. Airss data for carbon at 10gpa and the c+n+h+o system at 1gpa, 2020.
[41] Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, et al. Commentary: The materials project: A materials genome approach to accelerating materials innovation. APL materials, 1(1):011002, 2013.
[42] Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L Chevrier, Kristin A Persson, and Gerbrand Ceder. Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. Computational Materials Science, 68:314–319, 2013.
[43] Zekun Ren, Siyu Isaac Parker Tian, Juhwan Noh, Felipe Oviedo, Guangzong Xing, Jiali Li, Qiaohao Liang, Ruiming Zhu, Armin G. Aberle, Shijing Sun, Xiaonan Wang, Yi Liu, Qianxiao Li, Senthilnath Jayavelu, Kedar Hippalgaonkar, Yousung Jung, and Tonio Buonassisi. An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties. Matter, 2021. ISSN 2590-2385. doi: https://doi.org/10.1016/j.matt.2021. 11.032.
[44] Niklas Gebauer, Michael Gastegger, and Kristof Schütt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7566–7578. Curran Associates, Inc., 2019.
[45] Gongbo Zhang, Yanting Li, Renqian Luo, Pipi Hu, Zeru Zhao, Lingbo Li, Guoqing Liu, Zun Wang, Ran Bi, Kaiyuan Gao, Liya Guo, Yu Xie, Chang Liu, Jia Zhang, Tian Xie, Robert Pinsler, Claudio Zeni, Ziheng Lu, Yingce Xia, Marwin Segler, Maik Riechert, Li Yuan, Lei Chen, Haiguang Liu, and Tao Qin. Unigenx: Unified generation of sequence and structure with autoregressive diffusion, 2025. URL https://arxiv.org/abs/2503.06687.
[46] Qi Li, Rui Jiao, Liming Wu, Tiannian Zhu, Wenbing Huang, Shifeng Jin, Yang Liu, Hongming Weng, and Xiaolong Chen. Powder diffraction crystal structure determination using generative models, 2024. URL https://arxiv.org/abs/2409.04727.
[47] Qingsi Lai, Fanjie Xu, Lin Yao, Zhifeng Gao, Siyuan Liu, Hongshuai Wang, Lu Shuqi, Di He, Liwei Wang, Linfeng Zhang, Cheng Wang, and Guolin Ke. End-to-end crystal structure prediction from powder x-ray diffraction. Advanced Science, 12, 01 2025. doi: 10.1002/advs.202410722.
[48] Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, and Shuqi Lu. Pre-training protein bi-level representation through span mask strategy on 3d protein chains. arXiv preprint arXiv:2402.01481, 2024.
[49] Minyi Su, Qifan Yang, Yu Du, Guoqin Feng, Zhihai Liu, Yan Li, and Renxiao Wang. Comparative assessment of scoring functions: the casf-2016 update. Journal of chemical information and modeling, 59(2):895–913, 2018.
[50] Pdbbind+, 2025. URL https://www.pdbbind-plus.org.cn/.
22


[51] Liegi Hu, Mark L Benson, Richard D Smith, Michael G Lerner, and Heather A Carlson. Binding moad (mother of all databases). Proteins: Structure, Function, and Bioinformatics, 60(3):333–340, 2005.
[52] Vincent Le Guilloux, Peter Schmidtke, and Pierre Tuffery. Fpocket: An open source platform for ligand pocket detection. Bioinformatics, 10(1):168, 2009. ISSN 1471-2105. doi: 10.1186/ 1471-2105-10-168.
[53] Marylens Hernandez, Dario Ghersi, and Roberto Sanchez. Sitehound-web: a server for ligand binding site identification in protein structures. Nucleic acids research, 37(suppl_2): W413–W416, 2009.
[54] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123–1130, 2023.
[55] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Vijil, Aurelie Lozano, Payel Das, and Jian Tang. Protein representation learning by geometric structure pretraining. In International Conference on Machine Learning, 2022.
[56] Zuobai Zhang, Minghao Xu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, and Jian Tang. Pre-training protein encoder via siamese sequence-structure diffusion trajectory prediction. In Annual Conference on Neural Information Processing Systems, 2023.
[57] Gabriele Macari, Daniele Toti, and Fabio Polticelli. Computational methods and tools for binding site recognition between proteins and small molecules: from classical geometrical approaches to modern machine learning strategies. Journal of computer-aided molecular design, 33:887–903, 2019.
[58] José Jiménez, Stefan Doerr, Gerard Martínez-Rosell, Alexander S Rose, and Gianni De Fabritiis. Deepsite: protein-binding site predictor using 3d-convolutional neural networks. Bioinformatics, 33(19):3036–3042, 2017.
[59] Radoslav Krivák and David Hoksza. P2rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure. Journal of cheminformatics, 10:1–12, 2018.
[60] Duanhua Cao, Mingan Chen, Runze Zhang, Zhaokun Wang, Manlin Huang, Jie Yu, Xinyu Jiang, Zhehuan Fan, Wei Zhang, Hao Zhou, et al. Surfdock is a surface-informed diffusion generative model for reliable and accurate protein–ligand complex prediction. Nature Methods, pages 1–13, 2024.
[61] Yuejiang Yu, Chun Cai, Zhengdan Zhu, and Hang Zheng. Uni-dock: A gpu-accelerated docking program enables ultra-large virtual screening. 2022.
[62] Richard A Friesner, Jay L Banks, Robert B Murphy, Thomas A Halgren, Jasna J Klicic, Daniel T Mainz, Matthew P Repasky, Eric H Knoll, Mee Shelley, Jason K Perry, et al. Glide: a new approach for rapid, accurate docking and scoring. 1. method and assessment of docking accuracy. Journal of medicinal chemistry, 47(7):1739–1749, 2004.
[63] Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and David Ryan Koes. Protein–ligand scoring with convolutional neural networks. Journal of chemical information and modeling, 57(4):942–957, 2017.
[64] David Ryan Koes, Matthew P Baumgartner, and Carlos J Camacho. Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise. Journal of chemical information and modeling, 53(8):1893–1904, 2013.
[65] Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):3891–3898, 2021.
23


[66] Hannes Stärk, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola. Equibind: Geometric deep learning for drug binding structure prediction. In International conference on machine learning, pages 20503–20521. PMLR, 2022.
[67] Wei Lu, Qifeng Wu, Jixian Zhang, Jiahua Rao, Chengtao Li, and Shuangjia Zheng. Tankbind: Trigonometry-aware neural networks for drug-protein binding structure prediction. Advances in neural information processing systems, 35:7236–7249, 2022.
[68] Yangtian Zhang, Huiyu Cai, Chence Shi, Bozitao Zhong, and Jian Tang. E3bind: An endto-end equivariant network for protein-ligand docking. arXiv preprint arXiv:2210.06069, 2022.
[69] Xujun Zhang, Odin Zhang, Chao Shen, Wanglin Qu, Shicheng Chen, Hanqun Cao, Yu Kang, Zhe Wang, Ercheng Wang, Jintu Zhang, et al. Efficient and accurate large library ligand docking with karmadock. Nature Computational Science, 3(9):789–804, 2023.
[70] Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi S Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. In The Eleventh International Conference on Learning Representations.
[71] Gabriele Corso, Arthur Deng, Benjamin Fry, Nicholas Polizzi, Regina Barzilay, and Tommi Jaakkola. Deep confident steps to new pockets: Strategies for docking generalization. ArXiv, pages arXiv–2402, 2024.
[72] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022: 500902, 2022.
[73] Shuqi Lu, Xiaohong Ji, Bohang Zhang, Lin Yao, Siyuan Liu, Zhifeng Gao, Linfeng Zhang, and Guolin Ke. Beyond atoms: Enhancing molecular pretrained representations with 3d space modeling. arXiv preprint arXiv:2503.10489, 2025.
[74] Junwei Yang, Kangjie Zheng, Siyu Long, Zaiqing Nie, Ming Zhang, Xinyu Dai, Wei-Ying Ma, and Hao Zhou. Mol-ae: Auto-encoder based molecular representation learning with 3d cloze test objective. bioRxiv, pages 2024–04, 2024.
[75] Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pages 20479–20502. PMLR, 2022.
[76] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. Advances in neural information processing systems, 33:12559–12571, 2020.
[77] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127–134, 2022.
[78] Pei Zhang, Logan Kearney, Debsindhu Bhowmik, Zachary Fox, Amit K Naskar, and John Gounley. Transferring a molecular foundation model for polymer property predictions. Journal of Chemical Information and Modeling, 63(24):7689–7698, 2023.
[79] Fanmeng Wang, Wentao Guo, Minjie Cheng, Shen Yuan, Hongteng Xu, and Zhifeng Gao. Mmpolymer: A multimodal multitask pretraining framework for polymer property prediction. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 2336–2346, 2024.
[80] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: Large-scale self-supervised pretraining for molecular property prediction, 2020. URL https://arxiv. org/abs/2010.09885.
24


[81] Christopher Kuenneth and Rampi Ramprasad. polybert: a chemical language model to enable fully machine-driven ultrafast polymer informatics. Nature Communications, 14(1):4099, 2023.
[82] Changwen Xu, Yuyang Wang, and Amir Barati Farimani. Transpolymer: a transformer-based language model for polymer property predictions. npj Computational Materials, 9(1):64, 2023.
[83] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions On Graphics (TOG), 36(4):1–11, 2017.
[84] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In Proceedings of the IEEE international conference on computer vision, pages 2088–2096, 2017.
[85] Jia-Heng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun Liu, Bo Yang, and Lin Gao. Octfield: Hierarchical implicit functions for 3d modeling. arXiv preprint arXiv:2111.01067, 2021.
[86] Chao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang. Octr: Octree-based transformer for 3d object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5166–5175, 2023.
[87] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. ACM Transactions on Graphics (TOG), 42(4):1–11, 2023.
[88] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: A controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):1–20, 2024.
[89] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4209–4219, 2024.
[90] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024.
[91] Namrata Anand and Tudor Achim. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019, 2022.
[92] Kristof Schütt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377–9388. PMLR, 2021.
[93] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970–1981, 2020.
[94] Taoyong Cui, Chenyu Tang, Mao Su, Shufei Zhang, Yuqiang Li, Lei Bai, Yuhan Dong, Xingao Gong, and Wanli Ouyang. Geometry-enhanced pretraining on interatomic potentials. Nature Machine Intelligence, 6(4):428–436, 2024.
[95] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[96] Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. Unifying molecular and textual representations via multi-task language modelling. In International Conference on Machine Learning, pages 6140–6157. PMLR, 2023.
25


[97] Juzheng Zhang, Yatao Bian, Yongqiang Chen, and Quanming Yao. Unimot: Unified moleculetext language model with discrete token representation. arXiv preprint arXiv:2408.00863, 2024.
[98] Eric Nguyen, Michael Poli, Matthew G Durrant, Brian Kang, Dhruva Katrekar, David B Li, Liam J Bartie, Armin W Thomas, Samuel H King, Garyk Brixi, et al. Sequence modeling and design from molecular to genome scale with evo. Science, 386(6723):eado9336, 2024.
[99] Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, et al. Naturelm: Deciphering the language of nature for scientific discovery. arXiv preprint arXiv:2502.07527, 2025.
[100] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.
[101] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13142–13153, 2023.
[102] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 815–824, 2023.
[103] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: A deep generative network for computer-aided design models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6772–6782, 2021.
[104] Karl DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G Lambourne, Armando Solar-Lezama, and Wojciech Matusik. Fusion 360 gallery: A dataset and environment for programmatic cad construction from human design sequences. ACM Transactions on Graphics (TOG), 40(4):1–24, 2021.
[105] Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, and Shenghua Gao. Cad-mllm: Unifying multimodality-conditioned cad generation with mllm. arXiv preprint arXiv:2411.04954, 2024.
[106] Zehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar Yanardag. Text and image guided 3d avatar generation and manipulation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4421–4431, 2023.
[107] Peng-Shuai Wang, Chun-Yu Sun, Yang Liu, and Xin Tong. Adaptive o-cnn: A patch-based deep representation of 3d shapes. ACM Transactions on Graphics (TOG), 37(6):1–11, 2018.
[108] Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1179–1189, 2023.
[109] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. arXiv preprint arXiv:2408.10198, 2024.
[110] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139–1, 2023.
[111] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):1–16, 2023.
26


[112] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:73969–73982, 2023.
[113] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024.
[114] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4195–4205, 2023.
[115] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning, pages 72207229. PMLR, 2020.
[116] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19615–19625, 2024.
[117] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023.
[118] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 300–309, 2023.
[119] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12663–12673, 2023.
[120] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022.
27


A Illustration of the Generated Examples
Figure SI-1: Unconditional 3D molecular generation samples of QM9 dataset.
Figure SI-2: Unconditional 3D molecular generation samples of GEOM-DRUG dataset.
Figure SI-3: De novo crystal generation samples of MP-20 dataset.
28