SemGrasp: Semantic Grasp Generation via
Language Aligned Discretization
Kailin Li1,2, Jingbo Wang2, Lixin Yang1, Cewu Lu1†, and Bo Dai2
1 Shanghai Jiao Tong University, Shanghai, China {kailinli,siriusyang,lucewu}@sjtu.edu.cn 2 Shanghai AI Laboratory, Shanghai, China {wangjingbo,daibo}@pjlab.org.cn
Abstract. Generating natural human grasps necessitates consideration of not just object geometry but also semantic information. Solely depending on object shape for grasp generation confines the applications of prior methods in downstream tasks. This paper presents a novel semanticbased grasp generation method, termed SemGrasp, which generates a static human grasp pose by incorporating semantic information into the grasp representation. We introduce a discrete representation that aligns the grasp space with semantic space, enabling the generation of grasp postures in accordance with language instructions. A Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating object, grasp, and language within a unified semantic space. To facilitate the training of SemGrasp, we have compiled a large-scale, grasp-textaligned dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse grasps. Experimental findings demonstrate that SemGrasp efficiently generates natural human grasps in alignment with linguistic intentions. Our code, models, and dataset are available publicly at: https://kailinli.github.io/SemGrasp.
Keywords: Semantic Grasp Generation · Discrete representation · MLLM
1 Introduction
In applications such as AR/VR and embodied robotics, the ability to generate human-like grasps for a given object is of substantial value. The goal of grasping extends beyond simple object lifting; it involves alignment with human intent and preparation for subsequent manipulation tasks, such as avoiding hot water in a mug or preparing to open a bottle cap (Fig. 1 left). Hence, relying solely on the geometric information of objects is inadequate. Combining the semantic information of the object with the description of intent enables the generation of more natural and logical grasps. Typical grasp representations in previous grasp generation methods exhibit constraints in embedding semantic information. For example, methods that depict grasps through robotic hand poses [31, 44, 47, 50, 69, 73, 78, 93], MANO
† Corresponding author.
arXiv:2404.03590v1 [cs.CV] 4 Apr 2024


2 K. Li et al.
<grasp>
<grasp>
<o>
<m>
<r>
Fig. 1: Our SemGrasp methodology. The left figure shows the grasp generation process, while the right illustrates the decoding of our discrete grasp representation.
model [62] based poses [5, 6, 22, 28, 30, 46, 67, 68, 71, 75, 80, 89], contact regions [2, 19, 37, 38, 45, 81, 82, 87], and implicit forms [32, 33]. Attempts to generate dexterous grasps based on semantic cues, such as those by [28, 47, 78, 81, 93], rely on coarse affordance vectors for conditional generation or directly use visionlanguage models [17, 59] to filter sampled grasps. Nevertheless, integrating detailed semantic information or language descriptions into the grasp generation process remains challenging. When humans plan a grasping posture, they initially determine the grasp’s general orientation, guided by the object’s category and the semantics of the instruction. Subsequently, the specific manner of grasping is decided, influenced by both the manipulation intent and the object’s shape. Finally, the refinement of the grasp pose is conducted, taking into account the object’s detailed geometry and the hand-object contact state to ensure physical plausibility. Therefore, it is crucial to design a grasp representation that explicitly incorporates these three steps while implicitly embedding semantic information. In this paper, we introduce a novel grasp generation method, termed SemGrasp, that incorporates semantic information into the generation process. This approach, inspired by human grasp planning, divides the grasp representation into three interrelated components: 1) orientation, influenced by the intent and the object’s function; 2) manner, specifying the grasp taxonomy required for interaction; and 3) refinement, detailing the hand pose adjustments necessary for physical plausibility. Given that language is inherently discrete, we naturally extend this discreteness to other modalities to synchronize with the semantic space, similar to other multimodal studies [29, 41, 83]. Furthermore, human grasp poses can be categorized into 33 discrete types according to the Grasp Taxonomy [13]. By combining object shape and manipulation intent, a human grasp can be derived by adjusting these foundational types. Consequently, we employ a vector quantized variational autoencoder (VQ-VAE) [61, 72] to discretize the grasp components into tokens. This approach not only augments the alignment of grasp-language


Semantic Grasp Generation via Language Aligned Discretization 3
within the semantic space but also offers significant benefits: 1) it enhances grasp generation’s controllability and interpretability through explicit token representation; 2) it markedly reduces the dimensionality of the grasp space, simplifying the learning process of this representation. The VQ-VAE operates as an encoderdecoder structure, taking the object’s point cloud as conditional input, whereby the encoder encodes the grasp into three tokens via a codebook lookup, and the decoder reconstructs these tokens back into the original grasp. To align the discrete grasp representation of SemGrasp with semantic space more effectively, we leverage a multimodal large language model (MLLM). Inspired by the structure of LLaVA [43], our MLLM takes the discretized grasp tokens, the object features obtained via PointBERT [84], and the language description as the input. Following many MLLM works [29,43], our model is trained in two training stages: 1) multimodal alignment, wherein the MLLM is trained to predict grasp tokens based on object features and language descriptions, thereby mapping these modalities in a unified space; and 2) instruction tuning, where the MLLM is fine-tuned to enhance grasp generation for more complex outputs. Currently, well-aligned language-grasp datasets are scarce. The annotations in [28, 81] cover only simple intentions. To train SemGrasp, we collect CapGrasp, extending existing hand-object interaction datasets in three ways: 1) Low-level annotations, identifying contact states, such as which fingers touch the object and which parts of the object are being grasped. These details are deduced from the positions of hand and object. 2) High-level annotations, encompass manipulation intent and grasp force, for instance, ‘tightly touch the bottle cap to unscrew it’. Utilizing low-level information, we generate these grasp-related descriptions with GPT-4 [56]. 3) Conversational annotations: Employing GPT-4 and GPT-4v [57], we construct grasp-language mixed conversations based on dataset images or rendered visuals. These dialogues include both low and highlevel information and infer grasp details from the images. We train SemGrasp using CapGrasp. Experiments demonstrate that our method well generate the corresponding grasp pose across multiple metrics. We also verify our method’s potential value in AR/VR and embodied robotics applications. In summary, our contributions are threefold: 1) We propose SemGrasp, an innovative grasp generation method integrating semantic information. 2) We introduce a novel grasp discrete representation, efficiently and effectively expressing grasp postures and supporting the task of grasp description through language alignment. 3) We compile CapGrasp dataset. To the best of our knowledge, it is the first dataset of semantic grasps that encompass low-level, high-level, and conversational annotations.
2 Related Works
Grasp Generation Grasp generation remains a fundamental task with wide applications in robotics [1,54]. Recently, the generation of human-like grasps has attracted increasing attention. Unlike the 6DoF (degrees of freedom) paralleljaw grippers commonly used in robotics [9, 11, 12, 31, 53, 69], the higher freedom


4 K. Li et al.
in human fingers significantly complicates grasp generation. ObMan [22] leverages GraspIt! [52] for synthesizing grasps, while [46] optimize grasp based on force closure. Methods such as [5, 73, 78] validate their techniques in physical simulations. Data-driven approaches [5, 6, 22, 28, 30, 46, 48, 67, 68, 71, 75, 82, 89] employ end-to-end generative models like cVAE [66] or GAN [18]. Given the high degrees of freedom in the MANO model [62], most of these works implement post-processing to enhance contact consistency and physical plausibility. However, these methods solely emphasize geometric information of objects or incorporate basic intent features [28, 81]. In contrast, our SemGrasp integrates semantic aspects of grasping, facilitating the direct generation of grasp postures that correspond with language descriptions. Hand-Object Interaction Datasets Understanding hand-object interaction is pivotal in AR/VR, animation, and embodied AI. Existing datasets primarily concentrate on hand-object pose estimation or reconstruction. These datasets are either synthesized through rendering techniques [6, 15, 22, 40] or compiled by annotating real-world data [3, 4, 10, 16, 20, 21, 34, 36, 39, 48, 49, 58, 64, 68, 76, 81, 85, 94]. Some studies delve into the semantics of grasping. For instance, [93] introduces ‘touch codes’ to depict contact states between fingers and object parts. [81] offers annotations of grasping intent and object segmentation based on affordances. [28] segments object point clouds according to grasping semantics. Nonetheless, these datasets typically provide only basic semantic categorizations. Our CapGrasp, in contrast, delivers comprehensive annotations of hand-object interaction, encompassing detailed low-level contact state information, high-level grasp-related descriptions, and conversational annotations. Multimodel Large Language Models The development and application of Large Language Models (LLMs) have surged in recent years. Leading commercial models like GPT-4 [56] and open-source counterparts such as Llama [70] and Vicuna [90] demonstrate exceptional language understanding and generation capabilities. There’s an increasing trend of integrating LLMs into multimodal tasks, including image [26,43,92], video [41,86], 3D [24,77,83], and human posture and motion tasks [14, 29]. For example, [43] uses a vision encoder to extract image features and aligns these with the language space using projection layers. [29] interprets motion sequences as a series of tokens, and fine-tunes the T5 model [60] with LoRA [23] to facilitate various tasks like motion generation and captioning. Our work focuses on fine-tuning a model based on Vicuna [90] for tasks related to grasp generation, harnessing LLMs’ power to interpret and generate complex hand-object interactions.
3 Method
3.1 Overview
Given a specific object O, represented as a point cloud, our goal is to align the human grasp G with the associated language description L, facilitating the task of semantic grasp generation. To this end, we introduce a novel grasp generation methodology, termed SemGrasp, which fundamentally comprises two


Semantic Grasp Generation via Language Aligned Discretization 5
SG>
EG>
o> m> r>
❄
/s>
Fig. 2: Our SemGrasp pipeline. The grasp-aware language model outputs both the grasp tokens and the language conversations.
principal components: grasp discretization (Sec. 3.2) and a grasp-aware language model (Sec. 3.3). The initial phase involves the tokenization of the grasp into three interrelated tokens by training a VQ-VAE [61, 72]. As shown in Fig. 1 right, after the tokenizer is trained, the VQ-VAE is frozen, enabling the projection of <grasp> tokens from the codebook onto grasp configurations. Subsequently, as illustrated in Fig. 2, the grasp-aware language model is designed to reconcile the discrete grasp representations with the linguistic domain, trained specifically to generate corresponding <grasp> tokens. These resultant <grasp> tokens can then be reverted to the original grasp pose through the VQ-VAE decoder. The training of the grasp-aware language model is conducted utilizing our dataset, CapGrasp, which builds upon existing dataset of hand-object interactions [81], augmented through automated expansions (Sec. 3.4).
3.2 Grasp Discretization
Consistent with prior studies [30, 33], we define the grasp G = (T , θ, β) within the canonical space of the object. Here, T ∈ R4×4 represents the homogeneous transformation matrix, indicating the global rotation and translation of the hand relative to the object’s central coordinate system. The parameters θ ∈ R15×3 and β ∈ R10 denote the local hand pose and shape parameters, respectively. The hand vertices H ∈ R778×3 are computed using a differentiable layer, specifically the MANO M model [62], where H = M(G) = M(T , θ, β). In this work, to more effectively illustrate the human grasp process and align it with the semantic space, we discretize the grasp G into three components <o, m, r>, representing the orientation, manner, and refinement token, respectively, where o, m, r ∈ N. We employ a hierarchical VQ-VAE [61], encompassing the trainable codebooks Bi, encoders Ei, and decoders Di, where i ∈ {1, 2, 3}, to quantize the grasp vector into meaningful integers and subsequently reconstruct the original grasp vector from the quantized tokens (Fig. 3). The encoders progressively map the hand’s representation into the latent space, capturing grasp information from low to high levels. This structured approach enables the simulation of the grasping process through conditional probabilities: 1) The hand’s global information T is captured with the orientation


6 K. Li et al.
Fig. 3: Our grasp discretization process is based on hierarchical VQ-VAE.
token <o>, where Tˆ = D1(o, O). 2) The local hand pose θ, β is encapsulated by the manner token <m>, conditioned on <o>, with θˆ, βˆ = D2(o, m, O). 3) For the fine-tuning process, the delta parameters ∆T , ∆θ, ∆β are represented by the refinement token <r>, conditioned on <o, m>, where ∆Tˆ , ∆θˆ, ∆βˆ = D3(o, m, r, O). The final grasp is reconstructed as Gˆ = (∆Tˆ · Tˆ , ∆θˆ + θˆ, ∆βˆ + βˆ). The hat symbol ˆ· denotes the reconstructed values. Specifically, the codebook B = {bk}K
k=1, with each bk ∈ RdB , where dB represents the dimension of the VQ-VAE latent space and K the number of codebook entries. For each input vector z, the encoder E maps it to the latent space using the mapping network NE and finds the nearest codebook entry bz: z = E(z) = argmink∥NE (z)−bk∥2, where z ∈ {1, 2, . . . , K}. The decoder D then reconstructs the original vector z with the mapping network ND: zˆ = D(z) = ND(bz). The VQ-VAE is trained to minimize the reconstruction loss Lrec, the embedding loss Lemb, and the commitment loss Lcom:
Lrec = ∥H − Hˆ ∥2
2 = ∥H − M( ˆG)∥2
2 (1)
Lemb + Lcom = ∥sg[NE (z)] − bz∥2
2 + ∥NE (z) − sg[bz]∥2
2 (2)
where sg[·] denotes the stop-gradient operation.
3.3 Grasp Aware Language Model
Building upon the grasp discrete representation, we design a grasp-aware language model aimed at facilitating semantic grasp generation tasks. As depicted in Fig. 4, our model is trained to align three distinct modalities: the human grasp G, object points O, and the language description L. Grasp Modal After we train the VQ-VAE, this module is frozen. We take the encoders E as the grasp tokenizer that transfers the grasp G into the <grasp> token which contains three components: <o, m, r>. To distinct the grasp token from the language, we add special tokens <SG> and <EG> to the start and end of the grasp, respectively. The grasp token, as generated by the MLLM model, can be converted back to the human grasp utilizing the VQ-VAE decoders D.


Semantic Grasp Generation via Language Aligned Discretization 7
<mask><mask><mask>···<mask><mask><mask>
❄🔥
❄
🔥
🔥
<SG><o><m><r><EG>.</s> ❄
SO> EO>
···> OS>
Fig. 4: Grasp Aware Language Model.
Object Modal We employ PointBERT [84] to extract the object features fO from the point cloud O, where O ∈ RN×3 and fO ∈ RM×dO . Here, N denotes the number of points, M the count of point features, and dO the dimension of the object feature space. Notably, PointBERT requires the normalization of object sizes, a factor crucial for grasp generation. Thus, we incorporate the object size as a distinct token <OS> within the MLLM inputs. Subsequently, object features are projected into a unified space alongside the grasp token via a linear projection layer PO. Similar to the grasp token, special tokens <SO> and <EO> are affixed to the object feature sequence, delineating its start and end. The projected object features are treated as the system message of the GPT’s style conversational prompt. Language Modal Our language model undergoes fine-tuning based on the Vicuna-7B checkpoint [90], with a tailored prompt designed to direct the completion of specified tasks (detailed further in Appx). Textual content is tokenized into 32K word pieces employing the SentencePiece methodology [35]. Training Our model architecture, akin to LLaVA [43], aligns all modalities through projection or embedding layers into a unified semantic space, yielding X = {xi}T
i=1 ⊂ RT
dL , where dL signifies the semantic space dimensionality. An autoregressive language model is then trained to optimize the likelihood of the next tokens, predicated on preceding ones p(Xˆ |X) = Q
i p(xˆi|xˆ<i, x), terminating with the end-of-sentence token </s>. The primary objective is to diminish the negative log-likelihood loss LNLL:
LNLL = − log p(Xˆ |X) = −
X
i
log p(xˆi|xˆ<i, x) (3)
Utilizing LoRA [23], we finetune the MLLM model, adjusting approximately 0.4B parameters within the linear layers. This process unfolds in two phases: 1) multimodal alignment, wherein the MLLM is trained to predict the grasp tokens based on object features and language descriptions, thereby consolidating these modalities within a unified framework. During this phase, the object feature projection layer PO is updated. The embedding layer of the MLLM is also refined to accommodate the newly introduced special tokens; and 2) instruction tuning, throughout which the MLLM undergoes further refinement for grasp generation


8 K. Li et al.
task combined with language outputs, with the projection layer being frozen to ensure training stability.
3.4 CapGrasp Dataset
Currently, there exists no dataset with well-aligned grasp language annotations that would enable us to train SemGrasp for supporting grasp generation tasks and the downstream applications. Considering the prohibitive costs and laborintensive nature of manual semantic annotation, we design an automatic annotation methodology based on GPT-4 to augment existing hand-object interaction datasets. Our dataset, CapGrasp, encompasses low-level, high-level, and conversational annotations. Low-level Annotations Low-level annotations refer to the contact relationships between each finger and various parts of the object. According to the Grasp Taxonomy [13], we can deduce the grasp type and intent from these lowlevel annotations. For instance, if the thumb and index finger are in contact with a screw, it is inferred that the grasp type is a ‘pinch’ and the intent is to ‘screw/unscrew’. The OakInk dataset provides annotations for objects’ CAD models, hand vertices, and object part segmentation. Utilizing this information, we calculate the contact states when the distance between hand vertices and the object’s part segmentation points is less than a threshold of 3mm. High-level Annotations High-level intent is annotated from two perspectives: 1) based on low-level contact information. Given this information (i.e., the finger and object part contact), we employ GPT-4 to infer the grasping intent. For example, if all fingers are grasping the handle of a mug, GPT-4 can deduce that it is a firm grasp with possible intents such as ‘make a toast’ or ‘avoid hot beverage’. 2) Based on images or rendered views. Since the OakInk dataset includes a subset of real captured images (i.e., OakInk-image), we manually select representative frames that are clear, unobstructed, and with explicit intent. We leverage GPT-4v, a commercial visual-language model, to infer high-level information such as manipulation intent and grasp force. For grasps in OakInk without matching images (i.e., OakInk-shape), we render the image using the Blender renderer with realistic hand textures [42]. The details of the prompt are elaborated in Appx. Conversational Annotations With the aforementioned low-level and highlevel annotations, we construct conversations using the GPT-4 model. We ask GPT-4 to generate various conversational templates from different perspectives, including detailed hand-object contact information, manipulation intent, grasp force, and type. These dialogues must be consistent with the grasp and ensure logical plausibility. The prompts that guide the GPT-4 model are detailed in Appx. Considering the hallucination problem of GPT-4, to ensure the quality of our CapGrasp, we manually review these annotations to filter out intents that defy common sense and conversations that lack logical coherence. Statistically, our dataset includes approximately 1.8k object models from OakInk, about


Semantic Grasp Generation via Language Aligned Discretization 9
50,000 hand-object grasp pairs. For each pair, we offer on average 5 detailed captions and conversational annotations.
4 Metrics and Experiments
Our methodology, SemGrasp, incorporates two principal components: the grasp discrete representation and the grasp-aware language model. We assess the reconstruction accuracy of VQ-VAE to demonstrate the validity of our grasp discretization approach. Additionally, we evaluate the performance of grasp generation by our MLLM. Comparative and ablation studies underscore the effectiveness of our methodology.
4.1 Metrics
Aspect of Physical Plausibility To evaluate the physical plausibility of the predicted grasp pose ˆG, we employ several metrics: 1) Mean Per-Vertex Position Error (MPVPE, in mm) calculates the average L2 distance per vertex between the predicted hand mesh Hˆ and the ground truth H, when available. 2) Penetration Depth (PD, in cm) measures the maximum penetration depth of hand vertices into the object, indicating surface penetration. 3) Solid Intersection Volume (SIV, in cm3) quantifies the volumetric intersection by voxelizing the object mesh and calculating the volume within the hand surface. 4) Simulation Displacement (SD, in cm) tests grasp stability in PyBullet [7], measuring the object’s center displacement under steady hand conditions and gravity [22]. These metrics gauge both the quality of grasp generation and the accuracy of our grasp discrete representation. Aspect of Semantic Consistency Semantic consistency is evaluated by examining the quality of grasp generation: 1) GPT-4 assisted evaluation. For generated grasps ˆG, we first render the hand-object interaction following the same pipeline as in Sec. 3.4. Then, we use GPT-4v to score the semantic consistency of the grasp images based on input captions. Scores range from 0 to 100, with higher scores indicating better consistency. The prompts used in GPT-4 assisted evaluation are listed in Appx. 2) P-FID calculates the Fréchet Inception Distance between the point clouds of the Hˆ and H, using the pre-trained feature extractor from [55]. 3) Perceptual Score (PS) assesses the naturalness of grasps and semantic consistency, with 5 volunteers rating the generated grasps on a 5-point Likert scale. The final score is the mean Likert score.
4.2 Implementation Details
The VQ-VAE’s codebook B consists of K = 512 entries, each dimensioned at dB = 256. We employ PointBERT [84] as the point cloud feature extractor in the VQ-VAE encoder E for both hand vertices H and object vertices O. Similar to [77], PointBERT is pretrained using the ULIP-2 method [79] for enhanced geometry-language alignment. The predicted rotation from the VQ-VAE


10 K. Li et al.
decoders D uses a 6D representation [91], subsequently converted to the axisangle representation for further computation. For the MLLM, we utilize Llama structure [70] as the model backbone, finetuning based on the Vicuna-7B checkpoint [90]. To extract object feature fO, we reuse PointBERT from the VQ-VAE and freeze its parameters. fO includes M = 513 embeddings, each of dimension dO = 384. The object projection layer PO projects fO to the language space dimension of 4096. We configure the LoRA module [23] with a rank setting of r = 64, resulting in approximately 6% parameters being finetuned. The learning rate is set to 5e-4 and 3e-5 for the multimodal alignment stage and instruction tuning stage, respectively, with a cosine annealing learning rate scheduler for training stability. The batch size is 128, and the MLLM is trained over 20 epochs on 4 A100 GPUs with 80GB of memory each.
4.3 Comparisons
Discrete VQ-VAE Grasp Representation Given the discretization of grasps, three primary concerns arise: 1) Does discretization compromise reconstruction accuracy? 2) Does it affect the physical plausibility of the interaction?, and 3) Does it have the capability to embed semantic information? To answer the first two questions, we compare our method with two stateof-the-art methods, GrabNet [68] and Jiang et al . [30], on a reconstruction task. Both methods are based on cVAEs [66] for grasp generation. GrabNet employs RefineNet to refine the interaction in an end-to-end iterative manner, whereas Jiang et al . utilize test time adaptation (TTA) to optimize hand-object contact. Our method leverages a refinement token to adjust the hand pose in an endto-end manner. Compared to configurations without the refinement token, our approach with the refinement token exhibits superior performance, achieving a 26% improvement in MPVPE and a 9% improvement in SIV. Considering the TTA, an optimization-based approach, can precisely improve hand-object interaction, we also report our method’s performance with TTA in Tab. 1 for a fair comparison, which attains current SOTA results in PD and SIV. The results demonstrate that our discrete grasp representation method can accurately depict hand poses and specifically ensure the physical plausibility of interactions.
Table 1: Our discrete VQ-VAE grasp representation compared with SOTA methods.
MPVPE ↓ PD ↓ SIV ↓ SD mean. ↓ SD std. ↓
CapGrasp dataset - 0.11 0.62 0.94 1.62
GrabNet [68] w/o refineNet 18.14 0.76 5.42 1.75 2.61 GrabNet [68] 27.49 0.54 3.45 1.77 2.36 GrabNet [68] w/ TTA 27.16 0.49 2.16 1.35 1.56
Jiang et al. [30] w/o TTA 33.68 0.72 5.81 1.53 1.77 Jiang et al. [30] w/ TTA 33.84 0.58 2.78 1.36 1.55
Ours w/o finetune token 20.36 0.48 3.00 1.95 2.11 Ours 14.97 0.46 2.72 2.14 2.37 Ours w/ TTA 23.61 0.37 1.27 1.90 2.12


Semantic Grasp Generation via Language Aligned Discretization 11
o> m>
(a) Controllable generation compared with cVAE.
(b) MLLM’s qualitative results. We only visualize grasp and ignore the text outputs for better visualization. The red box indicates the failure cases.
Fig. 5: Qualitative results of our SemGrasp.
Table 2: Quantitative results of our MLLM based grasp generation method.
P-FID ↓ PD ↓ SIV ↓ SD mean. ↓ SD std. ↓ GPT-4 ↑ PS ↑
CapGrasp - 0.11 0.62 0.94 1.62 82.3 4.7
BERT [8] based 3.32 0.49 4.60 2.17 2.26 47.3 3.7 SemGrasp 2.28 0.48 4.24 2.00 2.33 74.5 4.6
Compared to previous SOTA methods, our method is competitive and exhibits advantages in certain metrics. To explore whether our discrete representation method can encode semantic information, we conduct a controllable generation task. For GrabNet, the cVAEbased method, we directly fix the sampling vector z = 0, enforcing the model to generate similar grasps. For our method, we assign the same const value to the <o, m> tokens. As depicted in Fig. 5a, take the specific category of mugs as an example, although the shape of the mug varies, our generated grasps maintain consistency in orientation and manner, showing the semantic consistency of our method. In contrast, we find that GrabNet’s prediction results are not interpretable.
Language Guided Grasp Generation We need to validate that MLLM can control grasp generation G based on textual input L. To the best of our knowledge, there are no existing works directly comparable to ours. Therefore, leveraging our discrete representation, we construct a straightforward baseline that treats this task as a classification problem. We finetune the official BERT model [8] to embed the language description in conjunction with the object feature. Subsequently, we deploy three distinct classification heads to predict the <o, m, r> tokens. We train this modified BERT with our CapGrasp following the same settings in SemGrasp. These predicted tokens are then decoded into the final grasp pose as in our SemGrasp. The outcomes of this experimental setup are documented in Tab. 2. From the results, we observe that our MLLM outperforms the baseline in both the physical plausibility and semantic consistency metrics. Showing that simply treating the task as a classification problem


12 K. Li et al.
Table 3: Representation ablation.
MPVPE ↓ PD ↓ SIV ↓ SD
mean. ↓ std. ↓
One token 29.95 0.66 5.14 1.90 2.29 <o, m> 25.73 0.58 4.32 2.14 2.50 <o, m, r × 2> 15.37 0.50 2.98 2.28 2.57 <o, m, r × 3> 15.90 0.52 3.37 1.98 2.18 Single VQ 28.02 0.68 5.31 1.81 2.00 w/o semantic 21.94 0.60 4.59 1.84 1.91
Table 4: VQ-VAE settings.
MPVPE ↓ PD ↓ SIV ↓ SD
mean. ↓ std. ↓
B entries K = 256 95.52 1.59 51.83 1.42 1.29 B entries K = 1024 39.82 0.80 5.30 3.10 3.40 B dim. dB = 512 46.32 1.11 10.86 2.00 3.09 B dim. dB = 128 38.67 0.80 6.42 2.28 2.78
Vanilla 48.48 1.09 9.42 2.24 2.87 EMA+Reset [61] 24.67 0.73 5.74 1.81 2.00
is not sufficient to generate grasp pose that aligns with the language description. On one hand, benefitted from the pretrained LLM, our SemGrasp can well understand the language instructions. On the other hand, the discrete representation is interpretable, making the MLLM more controllable. We demonstrate the qualitative results of our SemGrasp in Fig. 5b.
4.4 Ablation Studies
Ablation on Discrete Representation We conduct ablation studies to examine the design of our tokenization approach. As outlined in Sec. 3.2, we utilize three tokens—orientation, manner, and refinement—to represent <grasp> as <o, m, r>. In our evaluation, detailed in Tab. 3, we explore different token configurations: 1) Single Token: Compressing G into a single codebook significantly degrades reconstruction accuracy. 2) Two Tokens: This setup differs from the w/o refinement token setting in Tab. 1, as here we train the representation with only two tokens from scratch. 3) and 4) Multiple refinement Tokens: Iteratively predicting <r> and adjusting the hand pose step by step demonstrates that performance deteriorates when the number of <r> exceeds one. Moreover, we empirically find that predicting more tokens increases the complexity of MLLM training. 5) Single VQ-VAE: We train a single VQ-VAE to predict three grasp tokens simultaneously with a shared codebook. A single network struggles to encapsulate the intricate grasp representation. 6) w/o semantic. In this setting, we still use the hierarchical VQ-VAE to predict three tokens, but we do not assign semantic meaning to the tokens. This setup results in decreased performance. Ablation on VQ-VAE settings Our investigation into the configurations of VQ-VAE focuses on two aspects: 1) Codebook B Setting: The size of trainable parameters in the codebook has a significant impact on network performance, leading to either non-convergence or underfitting (see Tab. 4). 2) Training Strategy: VQ-VAE often suffers from codebook collapse. While methods like exponential moving average (EMA) and codebook reset (Reset) are used to mitigate this (as in [61, 88]), we find that these strategies weaken the representation effectiveness in the grasp representation task. Thus, we opt not to use the EMA strategy and allow each entry to be reset only once during training. Ablation on MLLM settings We conduct ablation studies on the MLLM configurations as presented in Tab. 5: 1) Pretrained LLM: The comparison between Llama-7B [70] and Vicuna-7B models shows Vicuna-7B as more aligned with our grasp generation needs, offering better task suitability. 2) Object size


Semantic Grasp Generation via Language Aligned Discretization 13
Table 5: Ablation study of our MLLM-based grasp generation.
P-FID ↓ PD ↓ SIV ↓ GPT-4 ↑ PS ↑
w/ Llama 2.38 0.51 4.20 58.9 3.8 w/o <SO> 3.74 0.70 8.20 43.3 3.2 w/o 2-stage 4.54 0.49 5.26 62.5 4.0 LoRA r = 16 3.76 0.48 4.38 69.2 3.3 LoRA r = 128 2.68 0.50 5.10 74.5 4.0
token <SO>. Contrary to the original ULIP’s size normalization of the object point cloud, our findings highlight the significance of object size for grasp generation tasks. 3) Training Stages: Comparing the MLLM trained in a two-stage process with a single-stage approach shows that the former not only enhances effectiveness but also stabilizes the training process. 4) and 5) LoRA rank: We explore the impact of the LoRA rank on the MLLM. We experimentally find that rank 64 is the optimal choice for our task.
5 Applications
To demonstrate the real-world applicability of the grasps generated by SemGrasp, we conducted two case studies in the fields of AR/VR and robotics to show that combined with the RL-based policies, our method can synthesize dynamic grasp motions.
5.1 Application in AR/VR
In the context of AR/VR, producing grasps that align with user intent and facilitate natural object manipulation is essential. We evaluated the practicality of grasps generated by SemGrasp using the D-grasp method [5] within the RaiSim [27] simulated environment. D-grasp, which is based on a reinforcement learning (RL) approach, focuses on creating dynamic human grasps. It calculates the next grasp action from a static reference grasp G ̄ , the target object position T ̄ O, and the current state, including the hand and object’s pose and velocity, using a policy π trained with the PPO [63] algorithm. For our experiments, SemGrasp generates the reference pose  ̄G for a specified object O and language instruction L. We utilize the publicly available Dgrasp checkpoints1 to synthesize dynamic grasps. The object is targeted to lift 10 cm upwards along the gravitational direction. We maintain the hand shape parameter β at 0, consistent with D-grasp. This process is illustrated in Fig. 6a.
5.2 Application in Embodied Robotics
Our method’s efficacy is further validated in embodied robotics. Following the UniDexGrasp2 methodology [78], which entails static reference grasp generation
1 https://github.com/christsa/dgrasp 2 https://github.com/PKU-EPIC/UniDexGrasp


14 K. Li et al.
(a) Human-like grasps motion in AR/VR (b) Dexterous grasps execution in robotics
Fig. 6: Applications of our SemGrasp in both the AR/VR environment and robotics scenario.
and goal-conditioned grasp execution, our experiments focus on the latter phase to assess the performance of SemGrasp-generated grasps. We retarget our generated grasp G ̄ to the ShadowHand S [65] and evaluate the grasp within the IsaacGym simulation [51]. Initially, SemGrasp produces the reference grasp pose  ̄G. We then establish a pipeline to adapt  ̄G for the ShadowHand model G ̄ S , beginning with aligning finger keypoints between the MANO and ShadowHand models. Given the significant differences in degrees of freedom (DoF) and morphology between the two models, we implement a fitting-based optimization approach to refine the ShadowHand-object interaction. After optimizing  ̄GS , we apply the off-theshelf UniDexGrasp’s pretrained policy to execute the dynamic grasp sequence. Details on our fitting pipeline and the integration of  ̄GS with UniDexGrasp are further elaborated in the Appx. Fig. 6b showcases the process of grasping and lifting as generated by UniDexGrasp.
6 Conclusion
We introduce SemGrasp, an approach aimed at generating semantic grasps from language instructions. We propose a novel grasp representation that emulates the natural human grasping process. The discretized representation is both interpretable and controllable, making it ideal for semantic space alignment. Leveraging this representation, we deploy MLLM to generate grasps from language instructions. Tailored for this task, we also present CapGrasp, a comprehensive dataset containing grasp-text-aligned annotations. As we explore potential applications in AR/VR and embodied robotics, we are hopeful that SemGrasp will contribute to advancements in generating more human-like, semantically coherent grasps in various contexts. Limitations Despite SemGrasp demonstrating the capability to generate static single-hand grasps from semantic cues and dynamic grasps through RL integration, exploration remains in two directions: two-hand manipulation and end-to-end semantic grasp motion synthesis. The former requires addressing both hands’ cooperation, and the latter, the continuity of motion, both contingent on the availability of extensive, high-quality motion capture or synthesis data for training. Tackling these challenges promises to advance embodied grasping, pushing toward more sophisticated and realistic manipulation.


Semantic Grasp Generation via Language Aligned Discretization 15
References
1. Bohg, J., Morales, A., Asfour, T., Kragic, D.: Data-driven grasp synthesis—a survey. IEEE Transactions on robotics (2013) 3 2. Brahmbhatt, S., Handa, A., Hays, J., Fox, D.: Contactgrasp: Functional multifinger grasp synthesis from contact. In: IROS (2019) 2 3. Brahmbhatt, S., Tang, C., Twigg, C.D., Kemp, C.C., Hays, J.: Contactpose: A dataset of grasps with object contact and hand pose. In: ECCV (2020) 4 4. Chao, Y.W., Yang, W., Xiang, Y., Molchanov, P., Handa, A., Tremblay, J., Narang, Y.S., Van Wyk, K., Iqbal, U., Birchfield, S., et al.: Dexycb: A benchmark for capturing hand grasping of objects. In: CVPR (2021) 4 5. Christen, S., Kocabas, M., Aksan, E., Hwangbo, J., Song, J., Hilliges, O.: Dgrasp: Physically plausible dynamic grasp synthesis for hand-object interactions. In: CVPR (2022) 2, 4, 13, 22 6. Corona, E., Pumarola, A., Alenya, G., Moreno-Noguer, F., Rogez, G.: Ganhand: Predicting human grasp affordances in multi-object scenes. In: CVPR (2020) 2, 4 7. Coumans, E., Bai, Y.: Pybullet, a python module for physics simulation in robotics, games and machine learning. URL http://pybullet. org (2017) 9 8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) 11 9. Eppner, C., Mousavian, A., Fox, D.: Acronym: A large-scale grasp dataset based on simulation. In: ICRA (2021) 3 10. Fan, Z., Taheri, O., Tzionas, D., Kocabas, M., Kaufmann, M., Black, M.J., Hilliges, O.: Arctic: A dataset for dexterous bimanual hand-object manipulation. In: CVPR (2023) 4 11. Fang, H.S., Wang, C., Fang, H., Gou, M., Liu, J., Yan, H., Liu, W., Xie, Y., Lu, C.: Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics (2023) 3 12. Fang, H.S., Wang, C., Gou, M., Lu, C.: Graspnet-1billion: A large-scale benchmark for general object grasping. In: CVPR (2020) 3 13. Feix, T., Romero, J., Schmiedmayer, H.B., Dollar, A.M., Kragic, D.: The grasp taxonomy of human grasp types. IEEE Transactions on human-machine systems (2015) 2, 8 14. Feng, Y., Lin, J., Dwivedi, S.K., Sun, Y., Patel, P., Black, M.J.: Posegpt: Chatting about 3d human pose. arXiv preprint arXiv:2311.18836 (2023) 4 15. Gao, D., Xiu, Y., Li, K., Yang, L., Wang, F., Zhang, P., Zhang, B., Lu, C., Tan, P.: Dart: Articulated hand model with diverse accessories and rich textures. NeurIPS (2022) 4 16. Garcia-Hernando, G., Yuan, S., Baek, S., Kim, T.K.: First-person hand action benchmark with rgb-d videos and 3d hand pose annotations. In: CVPR (2018) 4 17. Gemini: Introduction to Gemini. https : / / deepmind . google / technologies / gemini/#introduction (2023) 2
18. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS (2014) 4 19. Grady, P., Tang, C., Twigg, C.D., Vo, M., Brahmbhatt, S., Kemp, C.C.: Contactopt: Optimizing contact to improve grasps. In: CVPR (2021) 2 20. Hampali, S., Rad, M., Oberweger, M., Lepetit, V.: Honnotate: A method for 3d annotation of hand and object poses. In: CVPR (2020) 4


16 K. Li et al.
21. Hampali, S., Sarkar, S.D., Rad, M., Lepetit, V.: Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation. In: CVPR (2022) 4 22. Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M.J., Laptev, I., Schmid, C.: Learning joint reconstruction of hands and manipulated objects. In: CVPR (2019) 2, 4, 9 23. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021) 4, 7, 10 24. Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y., Li, Q., Zhu, S.C., Jia, B., Huang, S.: An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871 (2023) 4 25. Huang, J., Zhou, Y., Guibas, L.: Manifoldplus: A robust and scalable watertight manifold surface generation method for triangle soups. arXiv preprint arXiv:2005.11621 (2020) 23 26. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Liu, Q., et al.: Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045 (2023) 4 27. Hwangbo, J., Lee, J., Hutter, M.: Per-contact iteration method for solving contact dynamics. IEEE Robotics and Automation Letters (2018) 13 28. Jian, J., Liu, X., Li, M., Hu, R., Liu, J.: Affordpose: A large-scale dataset of handobject interactions with affordance-driven hand pose. In: ICCV (2023) 2, 3, 4 29. Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., Chen, T.: Motiongpt: Human motion as a foreign language. NeurIPS (2023) 2, 3, 4 30. Jiang, H., Liu, S., Wang, J., Wang, X.: Hand-object contact consistency reasoning for human grasps generation. In: ICCV (2021) 2, 4, 5, 10 31. Jin, S., Xu, J., Lei, Y., Zhang, L.: Reasoning grasping via multimodal large language model. arXiv preprint arXiv:2402.06798 (2024) 1, 3 32. Karunratanakul, K., Spurr, A., Fan, Z., Hilliges, O., Tang, S.: A skeleton-driven neural occupancy representation for articulated hands. In: 3DV (2021) 2 33. Karunratanakul, K., Yang, J., Zhang, Y., Black, M.J., Muandet, K., Tang, S.: Grasping field: Learning implicit representations for human grasps. In: 3DV (2020) 2, 5 34. Kim, J., Kim, J., Na, J., Joo, H.: Parahome: Parameterizing everyday home activities towards 3d generative modeling of human-object interactions. arXiv preprint arXiv:2401.10232 (2024) 4 35. Kudo, T., Richardson, J.: Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 (2018) 7 36. Kwon, T., Tekin, B., Stühmer, J., Bogo, F., Pollefeys, M.: H2o: Two hands manipulating objects for first person interaction recognition. In: ICCV (2021) 4 37. Lakshmipathy, A.S., Feng, N., Lee, Y.X., Mahler, M., Pollard, N.: Contact edit: Artist tools for intuitive modeling of hand-object interactions. ACM TOG (2023) 2
38. Li, H., Lin, X., Zhou, Y., Li, X., Huo, Y., Chen, J., Ye, Q.: Contact2grasp: 3d grasp synthesis via hand-object contact constraint. In: IJCAI (2023) 2 39. Li, K., Yang, L., Lin, Z., Xu, J., Zhan, X., Zhao, Y., Zhu, P., Kang, W., Wu, K., Lu, C.: Favor: Full-body ar-driven virtual object rearrangement guided by instruction text. Proceedings of the AAAI Conference on Artificial Intelligence (2024) 4


Semantic Grasp Generation via Language Aligned Discretization 17
40. Li, K., Yang, L., Zhen, H., Lin, Z., Zhan, X., Zhong, L., Xu, J., Wu, K., Lu, C.: Chord: Category-level hand-held object reconstruction via shape deformation. In: ICCV (2023) 4 41. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao, Y.: Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023) 2, 4 42. Li, Y., Zhang, L., Qiu, Z., Jiang, Y., Li, N., Ma, Y., Zhang, Y., Xu, L., Yu, J.: Nimble: a non-rigid hand model with bones and muscles. ACM TOG (2022) 8, 20 43. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. NeurIPS (2023) 3, 4, 7
44. Liu, M., Pan, Z., Xu, K., Ganguly, K., Manocha, D.: Deep differentiable grasp planner for high-dof grippers. arXiv preprint arXiv:2002.01530 (2020) 1 45. Liu, S., Zhou, Y., Yang, J., Gupta, S., Wang, S.: Contactgen: Generative contact modeling for grasp generation. In: ICCV (2023) 2 46. Liu, T., Liu, Z., Jiao, Z., Zhu, Y., Zhu, S.C.: Synthesizing diverse and physically stable grasps with arbitrary hand structures using differentiable force closure estimator. IEEE Robotics and Automation Letters (2021) 2, 4 47. Liu, Y., Yang, Y., Wang, Y., Wu, X., Wang, J., Yao, Y., Schwertfeger, S., Yang, S., Wang, W., Yu, J., et al.: Realdex: Towards human-like grasping for robotic dexterous hand. arXiv preprint arXiv:2402.13853 (2024) 1, 2 48. Liu, Y., Yang, H., Si, X., Liu, L., Li, Z., Zhang, Y., Liu, Y., Yi, L.: Taco: Benchmarking generalizable bimanual tool-action-object understanding. arXiv preprint arXiv:2401.08399 (2024) 4 49. Liu, Y., Liu, Y., Jiang, C., Lyu, K., Wan, W., Shen, H., Liang, B., Fu, Z., Wang, H., Yi, L.: Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In: CVPR (2022) 4 50. Lu, J., Kang, H., Li, H., Liu, B., Yang, Y., Huang, Q., Hua, G.: Ugg: Unified generative grasping. arXiv preprint arXiv:2311.16917 (2023) 1 51. Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., Handa, A., et al.: Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470 (2021) 14 52. Miller, A.T., Allen, P.K.: Graspit! a versatile simulator for robotic grasping. IEEE Robotics & Automation Magazine (2004) 4 53. Mousavian, A., Eppner, C., Fox, D.: 6-dof graspnet: Variational grasp generation for object manipulation. In: ICCV (2019) 3 54. Newbury, R., Gu, M., Chumbley, L., Mousavian, A., Eppner, C., Leitner, J., Bohg, J., Morales, A., Asfour, T., Kragic, D., et al.: Deep learning approaches to grasp synthesis: A review. IEEE Transactions on Robotics (2023) 3 55. Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022) 9 56. OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 3, 4 57. OpenAI: Gpt-4v(ision) system card (2023), https://openai.com/research/gpt4v-system-card 3, 20
58. Qin, Y., Wu, Y.H., Liu, S., Jiang, H., Yang, R., Fu, Y., Wang, X.: Dexmv: Imitation learning for dexterous manipulation from human videos. In: ECCV (2022) 4 59. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021) 2


18 K. Li et al.
60. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research (2020) 4 61. Razavi, A., Van den Oord, A., Vinyals, O.: Generating diverse high-fidelity images with vq-vae-2. NeurIPS (2019) 2, 5, 12 62. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: Modeling and capturing hands and bodies together. ACM TOG (2017) 2, 4, 5 63. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017) 13 64. Sener, F., Chatterjee, D., Shelepov, D., He, K., Singhania, D., Wang, R., Yao, A.: Assembly101: A large-scale multi-view video dataset for understanding procedural activities. In: CVPR (2022) 4 65. Shadowrobot: Dexterous Hand Series. https : / / www . shadowrobot . com / dexterous-hand-series/ (2005) 14
66. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional generative models. NeurIPS (2015) 4, 10 67. Taheri, O., Choutas, V., Black, M.J., Tzionas, D.: Goal: Generating 4d whole-body motion for hand-object grasping. In: CVPR (2022) 2, 4 68. Taheri, O., Ghorbani, N., Black, M.J., Tzionas, D.: Grab: A dataset of whole-body human grasping of objects. In: ECCV (2020) 2, 4, 10 69. Tang, C., Huang, D., Ge, W., Liu, W., Zhang, H.: Graspgpt: Leveraging semantic knowledge from a large language model for task-oriented grasping. IEEE Robotics and Automation Letters (2023) 1, 3 70. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023) 4, 10, 12 71. Turpin, D., Wang, L., Heiden, E., Chen, Y.C., Macklin, M., Tsogkas, S., Dickinson, S., Garg, A.: Grasp’d: Differentiable contact-rich grasp synthesis for multi-fingered hands. In: ECCV (2022) 2, 4 72. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. NeurIPS (2017) 2, 5 73. Wan, W., Geng, H., Liu, Y., Shan, Z., Yang, Y., Yi, L., Wang, H.: Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning. In: ICCV (2023) 1, 4 74. Wei, X., Liu, M., Ling, Z., Su, H.: Approximate convex decomposition for 3d meshes with collision-aware concavity and tree search. ACM TOG (2022) 23 75. Wu, Y., Wang, J., Zhang, Y., Zhang, S., Hilliges, O., Yu, F., Tang, S.: Saga: Stochastic whole-body grasping with contact. In: ECCV (2022) 2, 4 76. Xie, W., Yu, Z., Zhao, Z., Zuo, B., Wang, Y.: Hmdo: Markerless multi-view hand manipulation capture with deformable objects. Graphical Models (2023) 4 77. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., Lin, D.: Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911 (2023) 4, 9 78. Xu, Y., Wan, W., Zhang, J., Liu, H., Shan, Z., Shen, H., Wang, R., Geng, H., Weng, Y., Chen, J., et al.: Unidexgrasp: Universal robotic dexterous grasping via learning diverse proposal generation and goal-conditioned policy. In: CVPR (2023) 1, 2, 4, 13 79. Xue, L., Yu, N., Zhang, S., Li, J., Martín-Martín, R., Wu, J., Xiong, C., Xu, R., Niebles, J.C., Savarese, S.: Ulip-2: Towards scalable multimodal pre-training for 3d understanding. arXiv preprint arXiv:2305.08275 (2023) 9


Semantic Grasp Generation via Language Aligned Discretization 19
80. Yang, L., Li, K., Zhan, X., Lv, J., Xu, W., Li, J., Lu, C.: Artiboost: Boosting articulated 3d hand-object pose estimation via online exploration and synthesis. In: CVPR (2022) 2 81. Yang, L., Li, K., Zhan, X., Wu, F., Xu, A., Liu, L., Lu, C.: Oakink: A large-scale knowledge repository for understanding hand-object interaction. In: CVPR (2022) 2, 3, 4, 5, 20 82. Yang, L., Zhan, X., Li, K., Xu, W., Zhang, J., Li, J., Lu, C.: Learning a contact potential field for modeling the hand-object interaction. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) 2, 4 83. Yin, F., Chen, X., Zhang, C., Jiang, B., Zhao, Z., Fan, J., Yu, G., Li, T., Chen, T.: Shapegpt: 3d shape generation with a unified multi-modal language model. arXiv preprint arXiv:2311.17618 (2023) 2, 4 84. Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., Lu, J.: Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In: CVPR (2022) 3, 7, 9 85. Zhan, X., Yang, L., Zhao, Y., Mao, K., Xu, H., Lin, Z., Li, K., Lu, C.: Oakink2: A dataset of bimanual hands-object manipulation in complex task completion. arXiv preprint arXiv:2403.19417 (2024) 4 86. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023) 4 87. Zhang, H., Ye, Y., Shiratori, T., Komura, T.: Manipnet: neural manipulation synthesis with a hand-object spatial representation. ACM TOG (2021) 2 88. Zhang, J., Zhang, Y., Cun, X., Huang, S., Zhang, Y., Zhao, H., Lu, H., Shen, X.: T2m-gpt: Generating human motion from textual descriptions with discrete representations. In: CVPR (2023) 12 89. Zheng, J., Zheng, Q., Fang, L., Liu, Y., Yi, L.: Cams: Canonicalized manipulation spaces for category-level functional hand-object manipulation synthesis. In: CVPR (2023) 2, 4 90. Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 (2023) 4, 7, 10 91. Zhou, Y., Barnes, C., Lu, J., Yang, J., Li, H.: On the continuity of rotation representations in neural networks. In: CVPR (2019) 10 92. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023) 4 93. Zhu, T., Wu, R., Lin, X., Sun, Y.: Toward human-like grasp: Dexterous grasping via semantic representation of object-hand. In: ICCV (2021) 1, 2, 4 94. Zhu, Z., Wang, J., Qin, Y., Sun, D., Jampani, V., Wang, X.: Contactart: Learning 3d interaction priors for category-level articulated object and hand poses estimation. arXiv preprint arXiv:2305.01618 (2023) 4


Appendices
A Experiments Details
A.1 Setting Details
Dataset Split Our dataset, CapGrasp, builds upon and extends the OakInk dataset [81]. As such, we adopt the same split as OakInk, with 80% of the data allocated for training, 10% for validation, and 10% for testing. We ensure that the test set includes a wide variety of objects and language instructions, thereby allowing us to evaluate the generalization capabilities of our SemGrasp. MLLM prompt The prompt for our grasp-aware MLLM is specified in Tab. 6, guiding the model to generate coherent and plausible grasps from language instructions. GPT-4 assisted evaluation We leverage the commercial GPT-4v [57] to evaluate the quality of our SemGrasp. This involves rendered images I of the generated grasps ˆG, alongside the evaluation prompt outlined in Tab. 7 to GPT-4v. The model then returns a quality score reflecting both semantic similarity and physical reliability. To enhance the accuracy of GPT-4v, we take the following methods to improve the quality of the rendered images: 1) The grasp ˆG is mapped onto the differentiable NIMBLE model [42], which contains delicate muscle modeling and high-fidelity hand skin textures. 2) Images are rendered in Blender using the Cycles rendering engine, complemented by random lighting and camera positioning to ensure diversity. Perceptual Score We ask 5 volunteers to rate the quality of the generated grasps ˆG on a 5-point Likert scale. We randomly sample 50 predicated grasps from the test set for each experiment. The evaluation indicators involve the following three aspects: 1) Semantic coherence with the provided language instructions, 2) Physical plausibility of the hand pose, and 3) Stability of the interaction between the hand and the object. The perceptual score is the average of the ratings.
A.2 Representation Ablation Studies Details
This section elaborates on the ablation studies conducted to examine our discrete representation. Single Token Contrary to our primary model’s multi-token and hierarchical VQ-VAE structure, we explore a simplified model using a single VQ-VAE with one codebook to encapsulate the entire grasp representation. The <o, m> Setting In this variant, we devise a dual-layer hierarchical VQ-VAE specifically for grasp representation that is trained from scratch. The first codebook is for the orientation and the second codebook is for the manner. Multiple refinement Tokens This configuration introduces a delta VQVAE designed to refine the grasp pose by predicting incremental refinement


Semantic Grasp Generation via Language Aligned Discretization 21
tokens <r> conditioned on the preceding hand grasp and object point cloud. Based on this setting, we can iteratively adjust the hand pose by applying the delta parameters to the previous grasp. Single VQ-VAE Here, a unified VQ-VAE codebook is employed to simultaneously derive the three tokens (<o, m, r>), each decoded into the target pose through distinct decoding head. Without Semantic In our primary model, the reconstruction loss is composed of three parts: the orientation loss Lo, the manner loss Lm, and the re
finement loss Lr. Specifically, Lo only supervises the Tˆ and Lm mainly focuses
on the θˆ, βˆ. We simply set the items that are not supervised to zero in the loss function Eq. (4).
Lrec = Lo + Lo + Lr
= ∥M(T , 0, 0) − M(Tˆ , 0, 0)∥2
2
+ ∥M(T , θ, β) − M(Tˆ , θˆ, βˆ)∥2
2
+ ∥M(T , θ, β) − M(∆Tˆ · Tˆ , ∆θˆ + θˆ, ∆βˆ + βˆ)∥2
2 (4)
In the without semantic scenario, the <o> token is not exclusively constrained to represent orientation. We experimentally find that, during training, the three tokens collapse into a single token, which degrades performance.
B Exploratory Study
In our SemGrasp, we focus on training a grasp-aware MLLM to synchronize three distinct modalities—grasps, object models, and language instruc
<grasp>
<grasp>
<grasp>
<grasp>
Fig. 7: Results on the grasp caption task. The red text indicates the mistakes or the hallucinations of our model.


22 K. Li et al.
tions—within a unified representational space. We conduct an exploratory study to investigate the effectiveness of the alignment. Our findings indicate that the MLLM is not only capable of generating semantic grasps but also demonstrates promise in the grasp captioning task. Specifically, when provided with grasp tokens, the MLLM is able to produce corresponding language descriptions that capture both the low-level details and high-level intents of the grasps in some instances (as illustrated in Fig. 7). It is important to admit that these generated language descriptions do not always achieve the same level of accuracy as the ground truth. The MLLM occasionally struggles to capture the details of interactions or to hallucinate details in its language descriptions. This discrepancy underscores the inherent complexity of the caption task, which necessitates a comprehensive understanding of point clouds, intent interpretation, interaction reasoning, and natural language generation capabilities. However, it is a promising direction to explore the potential of the MLLM in the future when scaling up the model and the dataset.
C Applications Details
C.1 Physical-Plausible Dynamic Grasp using Human-like Hand
In our VR/AR application, we employ the open-source D-grasp method [5] to synthesize dynamic, human-like grasps. As described in the main paper, the reference pose  ̄G, corresponding to the language instruction L, is generated using our SemGrasp. This dynamic grasp policy is then applied to assess the feasibility of the generated grasps. To ensure alignment with real-world scenarios, we rotate the hand-object grasp pair so the palm faces toward the table. Given that the OakInk dataset does not provide object weight information, we assign a hypothetical weight of 300g to each object for the purpose of this evaluation. Samples of the generated dynamic grasps are illustrated in Fig. 8. In our analysis, any relative sliding between the hand and the object exceeding 4cm is classified as a failure. Based on this criterion, we report a success rate of 62.9% for our generated grasps on the test set.
Fig. 8: Synthesis of human-like grasps motion in AR/VR application.


Semantic Grasp Generation via Language Aligned Discretization 23
Fig. 9: Application in robotics. We verify our results by retargeting the generated grasp to the dexterous ShadowHand. The left image displays the adaptation outcomes, while the right image illustrates the grasp execution process.
C.2 Physical-Plausible Dynamic Grasp using ShaodowHand
Given the distinct morphological features and DoFs between the human hand and the ShadowHand, we devise a specialized pipeline for adapting the generated grasp G ̄ to the ShadowHand model G ̄ S . We manually select several corresponding keypoints on both the MANO and ShadowHand models, with a particular emphasis on the fingertips. For each MANO-based grasp  ̄G, the corresponding ShadowHand grasp  ̄GS is optimized by aligning these selected keypoints. To mitigate issues such as unnatural finger movements and potential finger collisions, we introduce a loss function that imposes angular constraints on the ShadowHand’s joints, thereby promoting physically plausible adaptations. The outcome of this fitting process is illustrated in Fig. 9 left. Following this adaptation, the refined grasp  ̄GS is executed using UniDexGrasp’s pretrained policy. To enhance the fidelity of collision detection, object meshes are preprocessed using Manifoldplus [25], followed by convex decomposition algorithms [74]. The results of these grasp executions are displayed in Fig. 9 right, showcasing the practicality and effectiveness of our methodology in the field of embodied robotics.
D CapGrasp collection
Prompts As mentioned in our main paper, we craft a set of prompts to direct both GPT-4 and GPT-4v in generating high-quality annotations automatically. For high-level details concerning manipulation intent and grasp status, two specialized prompts are utilized. These are detailed in Tabs. 8 and 9, designed to annotate high-level insights based on the contact information and images respectively. Additionally, to foster the generation of conversational content, another


24 K. Li et al.
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
USER:
ASSISTANT: <grasp>
Fig. 10: Visualization of our CapGrasp dataset.
prompt is crafted to steer GPT-4 in creating conversation templates. This specific prompt, intended to enrich our dataset with conversation annotations, is outlined in Tab. 10. Dataset Visualization Our CapGrasp dataset is showcased in Fig. 10, featuring an array of objects spanning various categories, shapes, and functionalities. Accompanying language instructions encompass a broad spectrum of grasp intentions and object interactions, with each object annotated with at least 8-10 distinct manipulation intents. This rich diversity is essential for the training of our SemGrasp, enabling the generation of grasps that are not only semantically coherent but also physically plausible across diverse scenarios.


Semantic Grasp Generation via Language Aligned Discretization 25
Table 6: System Prompt of MLLM.
– A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. The assistant can understand the information of the three-dimensional object model provided by the user, and combine the knowledge of human hand grasping to assist the user. The following is the object model information: O.
Table 7: Prompt of evaluation.
– Your task is to evaluate the alignment between a hand pose and its corresponding textual description, focusing on the grasp’s intent, specific object parts engaged by th e hand, and the contact dynamics between fingers and the object. Disregard the background and any textures on both the hand and object. Assessments should consider the physical feasibility of the grasp. Scores will range from 0 to 100, where 100 signifies perfect alignment and a physically plausible grasp, while 0 indicates a significant misalignment or a grasp that seriously defies physical principles. Just directly give the final score, such as: 95. Here is the description L and the grasp image I:
Table 8: Prompt of contact based on highlevel annotations.
– Given a formatted input describing which part of an object is grasped by a human hand and how many fingers are contacting the object, your task is to generate a set of grasp purposes or add more grasping details, such as the grasping method, the grasping force, etc. Please provide a comprehensive list (8 - 10 phrases) of potential grasp purposes or additional grasping details for each object and its corresponding grasped part. – For example: #input: [{OBJ: "mug", PART: "handle", FINGERS: 5}, {OBJ: "pen", PART: "", FINGERS: 3}] #output: [{"to drink", "to make a toast"}, {"to write", "to underline or highlight text"}] Please note that some PART may be empty, indicating that the entire object is grasped.
Table 9: Prompt of image-based high-level annotations.
– Deduce the manipulation intent and the grasp force status from an image depicting hand-object interaction. Your response should focus on the object’s contact part, affordance, hand grasp types, and hand fingers status to support your answer. Please ignore the background and object texture when deducing. Provide a clear and concise answer of the manipulation intent and grasp force status, following the example: {"intent": "to cut something", "status": "firmly grasping"}. Your analysis should be thorough and accurate, considering all relevant aspects of the hand-object interaction to support your deductions effectively.
Table 10: Prompt of conversation templates generation.
– Please provide conversation templates related to the topic of "human grasping the object." The conversation should incorporate the optional elements: 1. {finger}: e.g. "four fingers", "only one finger" 2. {status}: e.g. "firmly contacting", "softly touching" 3. {intent}: e.g. "pour water", "cut something", "toast", "transfer food onto a plate" 4. {object}: e.g. "mug", "bottle" Along with the required element of <grasp> as a state noun. Build one round of conversation using these elements, allowing for flexibility and creativity in the conversation templates. You should focus on creating dialogue that reflects human interaction related to grasping objects, considering various scenarios and details provided by the optional elements. Here is some examples: {"USER": "Try to {status} the {object} using {finger}". "ASSISTANT": "Sure, here is the <grasp>."} {"USER": "{finger} are {status} the {object} {intent}". "ASSISTANT": "This sounds like a typical <grasp>."}