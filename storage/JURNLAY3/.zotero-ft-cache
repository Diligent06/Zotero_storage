Task-Oriented 6-DoF Grasp Pose Detection in Clutters
An-Lan Wang1,†, Nuo Chen1,†, Kun-Yu Lin1, Yuan-Ming Li1 and Wei-Shi Zheng1,2,B
Abstract— In general, humans would grasp an object differently for different tasks, e.g., “grasping the handle of a knife to cut” vs. “grasping the blade to hand over”. In the field of robotic grasp pose detection research, some existing works consider this task-oriented grasping and made some progress, but they are generally constrained by low-DoF gripper type or non-cluttered setting, which is not applicable for human assistance in real life. With an aim to get more general and practical grasp models, in this paper, we investigate the problem named Task-Oriented 6-DoF Grasp Pose Detection in Clutters (TO6DGC), which extends the task-oriented problem to a more general 6-DOF Grasp Pose Detection in Cluttered (multi-object) scenario. To this end, we construct a large-scale 6-DoF taskoriented grasping dataset, 6-DoF Task Grasp (6DTG), which features 4391 cluttered scenes with over 2 million 6-DoF grasp poses. Each grasp is annotated with a specific task, involving 6 tasks and 198 objects in total. Moreover, we propose One-Stage TaskGrasp (OSTG), a strong baseline to address the TO6DGC problem. Our OSTG adopts a task-oriented point selection strategy to detect where to grasp, and a task-oriented grasp generation module to decide how to grasp given a specific task. To evaluate the effectiveness of OSTG, extensive experiments are conducted on 6DTG. The results show that our method outperforms various baselines on multiple metrics. Real robot experiments also verify that our OSTG has a better perception of the task-oriented grasp points and 6-DoF grasp poses.
I. INTRODUCTION
Given the single-view point cloud of a cluttered scene, the general grasp pose detection problem requires a model to detect several grasp poses to grasp individual objects stably, which has achieved tremendous progress in recent years. Today’s state-of-the-art grasping algorithms have shown great reliability and generalizability in grasping objects with high degree-of-freedom (DoF) gripper [1], [2], [3], [4], [5], [6], under complicated environments. Several recent works start on solving grasp pose detection in some more challenging grasping scenarios, e.g., reactive grasping [7], [8], [9], [10] and target-referenced grasping [11], [12], [13]. Despite these successes, there is still a significant gap between how robots pick and how humans grasp. The abovementioned methods treat grasp as the final goal, focusing on how to grasp an object stably and reliably in different scenarios. However, when humans grasp an object, we generally want the grasp can help us finish a particular task. In other words, the grasp action is not the final goal, but a starting line towards further operations.
1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China. Email: {wanganlan, chenn65, linky5,liym266}@mail2.sysu.edu.cn
2Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China. † Equal Contribution
BCorresponding author: zhwshi@mail.sysu.edu.cn
Existing Task-Oriented Grasp Dataset
(d) Our 6-DoF Task Grasp Dataset
(c) Existing 6-DoF Grasp in Clutters dataset
Planar Gripper
(a) 6-DoF, Single Object (b) Planar, Cluttered
Fig. 1. Examples of several existing datasets and our 6-DoF Task Grasp (6DTG) dataset. Compared with the existing task-oriented grasping datasets (a) and (b), 6DTG uses 6-DoF grasp poses and cluttered scenes. Compared with the existing 6-DoF Grasping in clutters dataset (c), 6DTG provides the task annotation for each grasp. Different grasp pose color represents different task in our 6DTG dataset. Best viewed in color.
Recent works [14], [15], [16], [17], [18], [19] provide a way to narrow this gap by investigating the task-oriented grasp pose detection problem, where a robot is required to detect a grasp pose to pick up an object to finish a particular task. However, these works are generally constrained by noncluttered setting [15], [19] or low-DoF gripper [14], [18], as shown in Figure 1 (a) and 1 (b). To perform household work in real life, an assistive robot must be able to perform 6-DoF task-oriented grasping in clutters. Such as grasping a mug to fetch some water from a dinner table with a bowl, knife, and mug on top of it. To bridge this gap, we investigate the problem named Task-Oriented 6-DoF Grasp pose detection in Clutters (TO6DGC). In this problem, a robot needs to detect several 6-DoF grasp poses to grasp an object (located in a cluttered scene) to finish specific tasks. This problem is more realistic and practical compared with previous task-oriented grasp pose detection problems, which is a step closer to how humans grasp. TO6DGC problem has a higher requirement on datasets, i.e., (1) each object should be able to perform different tasks and (2) each stable grasp needs to be annotated with its task label. Previous datasets, e.g., Graspnet-1Billion [1] as
arXiv:2502.16976v1 [cs.RO] 24 Feb 2025


shown in Figure 1(c), is difficult to satisfy these requirements. Therefore, by utilizing existing 3D objects and grasp annotation datasets [20], [21], we construct a new largescale dataset, named 6-DoF Task Grasp (6DTG) (as shown in Figure 1(d)). 6DTG has several characteristics: (1) Finegained grasp task annotation. All grasps are annotated with their corresponding task to support task-oriented grasping. (2) 6-DoF grasp pose and cluttered scenes. This makes our dataset more practical compared with existing task-oriented datasets. (3) Densely annotated grasp poses. Dense grasp annotation has been proved to be important for training a robust grasp pose detection network [1], [21], [3], [22].
Previous 6-DoF task-oriented grasping methods [15], [19], generally focus on single object scenarios, adopting a twostage pipeline. They first detect task-irrelevant (stable) grasps and then use an evaluation model to judge whether each grasp pose is suitable for the desired task, as shown in Figure 2(a). Such a pipeline is time-consuming (detect and evaluate all stable grasps) and suffers from error accumulation, leading to poor evaluation precision. These methods perform even worse in solving the TO6DGC problem, as shown in the experiments. In this work, we further propose a strong baseline One-Stage TaskGrasp (OSTG) to address the TO6DGC problem, which directly detects the task-oriented 6-Dof Grasp pose in a one-stage manner, as shown in Figure 2(b). Specifically, Our OSTG first adopts a taskoriented point selection module to localize the points that can be grasped to finish the desired task, namely where to grasp, as we find that the task characteristics of a grasp pose is partially related to the grasp point. Based on the selected task-oriented points, we propose a novel task-guided grasp pose detection module that directly detects task-oriented 6DoF grasp rotation, i.e., how to grasp. We conduct extensive experiments on the 6DTG dataset, and the results demonstrate that our proposed OSTG model outperforms various baselines by a large margin. Real robot experiments also verify the effectiveness of our OSTG model.
II. RELATED WORKS
A. 6-DoF Grasp Pose Detection in Clutters.
6-DoF (or full-DoF) grasp pose detection in clutters, is a fundamental problem in robotics [23], [24], [25], [26], and is an important step toward Artificial General Intelligence [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44]. Two different approaches are explored in this field. The first approach [45], [46] adopts a sampling-evaluation strategy, which uniformly samples grasp candidates in the scene and then evaluates each grasp candidate using a deep network. The second approach [47], [1], [48], [2] adopts an end-to-end network. However, all above-mentioned methods focus on grasping objects stably and reliably, without considering the task associated with each detected grasp pose. In this work, we extend these to the task-oriented setting, which is more natural and practical.
Task-Guided Grasp Pose Detection
Task-Irrelevant Grasp Pose Detection
Task
Single-Object Point Features
Scene Point Features
(b) Ours
(a) Previous
Evaluation Model
Object + Task
Fig. 2. Previous methods [17], [19] focus on single object scenarios, and use a two-stage pipeline, i.e., generate task-irrelevant (stable) grasps firstly and then use an evaluation model to evaluate whether the grasp is suitable for a particular task. In contrast, we propose a novel one-stage task-guided grasp pose detection model to detect task-oriented grasp in a holistic way.
B. Task-Oriented Grasp Pose Detection
For task-oriented grasp pose detection, previous research can be mainly divided into two categories: planar-based [14], [16], [18] and 6-DoF-based [49], [15], [17], [19]. Research in the first category mainly took RGB (or RGB-D) images as inputs and outputs a set of rotated bounding boxes to represent the grasp poses. Due to the limitation of low DoF, their applications are restricted. Another line of research focuses on 6-DoF (full-DoF) grasp poses. However, they mainly focus on how to grasp single-object with specific tasks, which strictly restricts their application, e.g., GraspGPT [19]. In this work, we investigate the problem named task-oriented 6-DoF grasp pose detection in clutters, which is more realistic and practical than all previous ones, facilitating the development of task-oriented grasp pose detection.
C. Grasp Point Sampling Strategies
To detect available 6-DoF grasps in cluttered scenes, direct regression in high-dimensional discontinuous grasp space is quite challenging [50]. Several works propose to sample grasp points first and then detect grasp poses based on the sampled points. GPD [46] and PointNetGPD [51] use a simple uniform sampling strategy to select points. ContactGraspNet [3] uses the contact point of the gripper and object to effectively represent the grasp. GSNet [2] proposes the concept of graspable point sampling, achieving outstanding performance in cluttered scene 6-DoF grasping. In this work, we propose a task-oriented point selection strategy to handle the task-oriented 6-DoF grasp pose detection in clutters problem.
III. TASK-ORIENTED 6-DOF GRASP POSE DETECTION
IN CLUTTERS
A. Problem Definition
In general 6-DoF grasp pose detection in clutters, given the single-view point cloud P of a clutter, the model needs to detect several grasps G to grasp individual objects stably. Each grasp g ∈ G is represented by (R, T ) ∈ SE(3), where R ∈ SO(3) and T ∈ R3 are the rotation and translation of grasp g. SE is the Special Euclidean group, and SO is the Special Orthogonal group. For the task-oriented setting, we additionally provide an object class o and a task class t, the model needs to detect several task-oriented grasps G∗, which can be formulated as a posterior probability : p (G∗|P, o,t),


TABLE I
OBJECT CATEGORIES AND CORRESPONDING TASKS IN OUR DATASET
Mug Bottle Knife Hat Bowl Scissor Tasks Grasp, Wrap, Grasp, Wrap, Handover, Grasp, Grasp, Handover,
Pour, Contain Contain Cut Wear Wrap Cut
TABLE II
COMPARISON OF OUR 6DTG WITH PREVIOUS TASK-ORIENTED
GRASPING DATASETS. OUR DATASET IS THE FIRST TO PROVIDE 6-DOF
TASK-ORIENTED GRASP ANNOTATION IN CLUTTERS.
Datasets Object Objects Tasks Grasps 6-DoF Clutters
Category Number Grasp SG14000 [49] 5 44 7 12K ✓ TaskGrasp [15] 75 191 56 250K ✓ GATER [16] - 83 10 12K GraspCLIP[18] 28 96 38 - ✓ AGD [17] 6 203 6 100K ✓
6DTG(Ours) 6 198 7 2,000K ✓ ✓
where G∗ contains all successful grasps g∗ that can grasp object o to finish the task t. This problem is more challenging than previous task-oriented settings (i.e., cluttered scenes vs. single-object, 6-DoF grasp vs. planar grasp ).
B. 6DTG Dataset
- Data Collection (1) Objects and stable grasps prototype. In this work, our focus lies specifically on kitchen and household objects that can be grasped to operate different tasks. Taking this into account, we choose ShapeNetSem [20] and ACRONYM [21] as the objects and stable grasp prototype source, respectively. After human selection, We choose 6 object categories and 198 objects, as shown in Table I. (2) Scene generation. Following ACRONYM [21], we generate cluttered scenes that multiple objects are placed on top of a support table using trimesh. Firstly, we randomly select 3-6 objects that cover different object categories. Secondly, given the support table, we sequentially sample locations from a 2D Gaussian (centered around the table’s center) to place the selected objects on top of it, while ensuring the resulting configurations of all objects are collisionfree. It’s worth mentioning that, for small objects like knives and scissors, we additionally place a small cube under them to lift it for 5cm. This adjustment is made to facilitate grasping, as directly picking up scissors from the tabletop is challenging for a parallel-yaw gripper. For each generated scene, we render two point-cloud observations using different camera poses.
- Annotations
Next, we illustrate how to generate object-level and scenelevel grasp task labels , as shown in Figure 3. (1) Object-level. Inspired by AGD [17] and 3D AffordanceNet [52], we first annotate all successful grasps of objects with different task labels. The task classes we use are the same as 3D AffordanceNet [52], with details of the tasks each object can perform outlined in Table I. For each object, we first use a pre-trained 3D AffordanceNet [52] to detect the object affordance part, and then each grasp is labeled according to the grasp point. Applying task labels according
(a) Object-level Task-Oriented Grasp Annotation
(b) Scene-level Task-Oriented Grasp Annotation
Fig. 3. Visualizations of (a) Object-level and (b) scene-level task-oriented grasp annotation. Different colored grippers represent different tasks. Best viewed in color.
to the grasp point can help identify the task property of a grasp, but cannot serve as the final judgment. Therefore, we further manually verify whether the grasp for the object is suitable for the task. Finally, we got about 300K object-level task annotated grasps, an example is shown in Figure 3(a). (2) Scene-level. As our dataset is collected from simulation, it’s easy to use the object-level grasp task labels to label the scene-level grasp, using the object poses. It’s worth noting that grasps colliding with any scene geometry is removed. An example is shown in Figure 3(b).
- Data Triplet Generation
After the above processes, we generate data triplets (P, o,t) by scanning all rendered scenes, where P, o,t is the point cloud, object class, and task class. Totally, we have 16042 triplets for training and 6841 triplets for test.
C. Comparison with Related Datasets
In Table II, we compare our 6DTG with existing taskoriented datasets. Our dataset is the first task-oriented dataset that focuses on both cluttered scenes and 6-DoF grasp poses. Additionally, we emphasize that our dataset provides a much larger amount of grasp annotation, compared with the previous datasets, whether planar-based or 6-DoF-based.
IV. METHOD
Existing task-oriented grasping methods, generally focus on 1) planar-based grasp in clutters, and 2) 6-DoF-based grasp for a single object. These methods can not handle the challenging TO6DGC problem.
A. Grasp Representation
In the TO6DGC problem, each successful grasp in the high dimensional SE(3) space further corresponds to a task. So it is difficult for a learning-based model to generate the distribution of successful 6-DoF task-oriented grasps. Therefore, we introduce the grasp representation in our work, which can help solve this problem. Specifically, we decompose a 6-DoF grasp pose into (1) the corresponding task-oriented grasp point, (2) the 3-DoF grasp rotation, and (3) the grasp width of a parallel-yaw gripper. The grasp rotation Rg is further decomposed into an approaching direction a ∈ R3, and a baseline direction b ∈ R3, as shown in Figure 5.


Point Clouds
N×3
N′ × C
Point Encoder
Point Decoder
Objectness Detection
Target Object Detection
M×C
Task-Guided Grasp Pose Detection
Task(Cut)
Object(Scissors)
Objectness
Y/N
Feature
Extractor Target Objectness
Y/N
Stable Grasp Pose Detection
Taskness Detection
Taskness
Y/N
Ltgask
Lsgtable
Ltgask: Task Grasp Loss
Lsgtable: Stable Grasp Loss
Task-Oriented Point Selection
Fig. 4. Overview of our proposed One-Stage TaskGrasp (OSTG) model. In the figure, we provide an example of a model detecting grasp poses to grasp the scissors in the clutters that can finish the cut task. First of all, the point encoder-decoder processes N × 3 points and outputs N′ ×C-dim point features. Supervised by the point-wise labels, the Task-oriented Point Selection module selects M points according to objectness, target objectness, and taskness, in a step-by-step manner. These task-related grasp point features are then fed into the Task-Guided Grasp Pose Detection module to generate task-oriented grasps. We additionally add a stable grasp loss to supervise the model. Best viewed in color.
Therefore, we can first predict a suitable grasp point for the desired task and then estimate the corresponding 3-DoF rotation and grasp width, which greatly eases the TO6DGC. Based on this, we propose our One-Stage Task Grasp (OSTG) model. Different from previous two-stage methods that first generate all stable grasps and then evaluate each grasp, our OSTG directly generates task-oriented grasp poses in one stage.
B. Model Overview
Figure 4 illustrates the proposed One-Stage TaskGrasp (OSTG) model. Given the point cloud P, task t, and object o, we first use a point set backbone network PointNet++ [53] to process the point cloud, which tasks in the raw point cloud with size N × 3, and outputs features in the shape of N′ × C. Subsequently, the Task-Oriented Point Selection module learns the point that can be grasped to finish the desired task, namely where to grasp. Finally, these selected task-related grasp point features, together with the task class one-hot vector, are fed into a Task-guided Grasp Pose Generation (TGPG) module, which directly generates task-oriented grasp poses, i.e., how to grasp. We additionally add a stable grasp detector to predict stable grasp poses for efficient training.
C. Task-oriented Point Selection
First of all, we introduce the Task-oriented Point Selection module of our approach, which learns the point that can be grasped to finish the desired task, namely where to grasp. The Task-oriented Point Selection (TPS) module samples the task-related points in a step-by-step manner. In specific, the TPS module samples points according to objectness (i.e., whether the point belongs to an object), target objectness (i.e., whether the point belongs to the target object), and taskness (i.e., whether the point in the target object is suitable for the desired task).
Formally, given the point features outputted by the backbone fori, we add three binary classification heads ho, hto, htask to select points, which can be formulated as:
fo = ho( fori), fto = hto( fo, o), ftask = htask( fto,t), (1)
where fo, fto, and ftask are the selected point features according to objectness, target objectness, and taskness respectively, and o, t is the target object and task. This point selection operation is finished step-by-step, and points are sampled (or oversampled) to a fixed size after each head to support parallel computing. Such a step-by-step strategy has several benefits: (1) The points filtered by objectness or target objectness are not taken into further consideration, (2) As the point number decreases, the computation also reduces.
D. Task-guided Grasp Pose Detection
In this section, we propose a Task-guided Grasp Pose Detection module to learn the grasp rotation based on the selected task-oriented point feature and the task information. Specifically, the TGPD module takes two inputs, i.e., the selected task-oriented point features ftask and the target task t. To guide the grasp parameter prediction, we first concatenate the task embedding to the point feature, which is given as follows:
f ̃task = Concat( ftask,t). (2)
After getting the concatenated feature, we then use three heads to predict two grasp rotation vectors, i.e., the approaching vector a ̃ and the baseline vector b ̃, and the corresponding grasp confidence. As our grasp representation defined, the two vectors a and b should be orthonormal, we use a GramSchmidt orthonormalization to process a ̃ and b ̃, and then compute the grasp rotation matrix. Only using task-oriented grasps as supervision, it is challenging to train the overall model as the ground-truth task-oriented grasp pose number is limited. Therefore, we


Baseline
Approaching Vector
b
a
Vector
Task-Oriented Grasp Point
Fig. 5. We represent a grasp rotation using an approaching vector and baseline vector. To calculate the grasp loss, we further represent a grasp pose using five points as shown above. Best Viewed in color.
additionally add a task-irrelevant (stable) grasp pose detection module. This module takes in the point features from the feature extractor, outputs all the grasp poses, and is supervised with all available grasps.
E. Training Losses
The overall training losses can be divided into two parts, i.e., the point selection loss Lpoint and grasp loss Lgrasp.
Point Selection Loss. As the task-oriented point selection module contains three steps, i.e., objectness, target objectness, and taskness. Each step is supervised with its groundtruth label and contributes a loss item:
Lpoint = Lo + Lto + Ltask. (3)
Grasp Loss. Following previous work [3], we project a gripper grasp pose into 5 points, as shown in Figure 5. Then, we calculate the loss of predicted grasps and their corresponding ground-truth grasp, which is defined as
Lg = 1
n
n
i∑
sbimuin||gpred
i − ggt
u ||, (4)
where the subtraction of two grasp poses is defined as the distance of the five corresponding points in 3D space. Therefore, the grasp loss is defined as:
Lgrasp = Ltask
g + Lstable
g , (5)
where Ltgask is the task-oriented grasp loss and Lstable
g is the stable grasp loss. Grasp Loss. The training loss function is defined as:
Loverall = Lpoint + Lgrasp. (6)
V. EXPERIMENTS
A. Baselines
Previous task-oriented grasping methods, generally focus on 1) planar-based grasp in clutters, and 2) 6-DoF-based grasp for a single object. It’s challenging to adapt these methods directly to solve the TO6DGC problem (6-DoF and Clutters). We design two baselines.
Two-stage Baseline 1. For this baseline, we adapt previous evaluation-based methods [15], [19], which first generate all stable grasps and then evaluate each grasp. In practice, for simplicity, we directly use the ground-truth stable grasp as the stable grasp, which can be seen as an upper bound. We use TaskGrasp [15] to evaluate whether each grasp is suitable for the task t and object o. This baseline is constructed to
verify whether previous single-object two-stage methods can handle the multi-object (cluttered) scenario. Additionally, we add a Random baseline that randomly selects a task class from all task classes.
One-stage Baseline 2. In this setup, we use the ContactGraspnet [3] as the model to generate grasps. Different from its origin implementation, we simply concatenate a task onehot and an object one-hot to the point features before it is fed to the final task-oriented grasp pose prediction head, which is supervised with the corresponding task-oriented grasp poses. We also keep the origin stable grasp prediction head, supervised with all stable grasp poses in the scene. We construct this baseline to verify whether a current state-ofthe-art stable 6-DoF grasp pose detection model can handle the TO6DGC problem with a simple modification.
B. Metrics
For the one-stage methods, we combine previous 6-DoF grasping [3] and task-oriented [17] works, and evaluate the performance using two metrics. (i) Coverage Rate. This metric only required the model to output the grasp poses that grasp the task-oriented points, without requiring the grasp pose to grasp the object stably. (ii) Success Rate, which considers the grasp quality of each task-oriented-point grasp pose. This metric considers whether the grasp pose can grasp the object for the given task, Success Rate is set to 1 if at least one detected grasp pose satisfies the task-oriented requirement. To determine whether a predicted grasp pose g ̃ satisfies the task-oriented requirement, we calculate the grasp pose distance [1] between g ̃ and all ground-truth taskoriented grasp poses. The grasp pose distance is defined as:
D(G1, G2) = (dt (G1, G2), dα (G1, G2)), (7)
where G1 and G2 are two grasp poses, dt (G1, G2) = ||t1 − t2|| denotes the translation distance and dα (G1, G2) =
arccos 1
2 (trace(R1, RT
2 ) − 1) is the rotation distance. ti and Ri are the translation vector and rotation matrix. Based on this, one predicted grasp g ̃ is seen as positive if it has a D < (thd,thα ) with at least one ground-truth task-oriented grasp. In this paper, we set thd = 3 cm and thα = 30 degree, as previous work [1] done. For the two-stage methods, we compute the average grasp task classification precision as the Success Rate.
C. Comparison with Baselines
In this section, we compare our OSTG model with several baselines on the proposed 6DTG dataset. As depicted in Table III, the previous two-stage evaluation-based method exhibits poor performance on our 6DTG datasets, even provided with the ground-truth stable grasp. This is because TaskGrasp [15] is designed to evaluate grasp poses of singleobject point clouds. But in our 6DTG dataset, the point cloud of cluttered scenes contains multi-object and background points, which is much more challenging than single-object. Additionally, it is worth noting that such evaluation-based methods take a long time to evaluate compared with our One-Stage TaskGrasp (OSTG), i.e., 8 hours vs. 40 minutes.


TABLE III
COMPARISONS WITH SEVERAL CONSTRUCTED BASELINES ON 6DTG
DATASET. † USES GROUND-TRUTH STABLE GRASP AS INPUT.
Methods Coverage Success Rate
Two-Stage† Random - 11.73
Baseline 1 - 25.32
One-Stage Baseline 2 5.39 66.81
Ours 43.58 72.13
TABLE IV
ABLATION STUDY OF THE MAIN COMPONENTS OF OUR METHOD ON
6DTG DATASET.
Model TPS TGPD Coverage Success Rate Baseline 0.2 61.42 + TPS ✓ 40.19 62.27 Ours ✓ ✓ 43.58 72.13 Ours w/o TPS ✓ 5.39 66.81
Compared with the one-stage Baseline 2, OSTG obtains significant improvement, i.e., 38.19% in Coverage Rate and 5.32% in Success Rate. The results demonstrate that our OSTG model effectively learns where and how to grasp, which is attributed to our proposed task-oriented point selection and task-guided grasp pose detection module.
D. Ablation Study
Effect of main components in OSTG. In Table IV, we analyze the effect of each component of our proposed method. We directly use a state-of-the-art 6-DoF grasp pose detection model Contact-Graspnet as our baseline. By adopting a Task-oriented Point Selection module (TPS), we obtain a huge improvement in terms of coverage rate, i.e., 40.17%, which demonstrates the effectiveness of our proposed point selection strategy. Then, by introducing the Task-guided Grasp Pose Detection module (TGPD), our model obtains an improvement in Success Rate, i.e., 9.86%. In addition, if the TPS module is removed from the full model (the model degrades to the above-mentioned Baseline 2), the performance drops both in Coverage and Success Rate but still achieves better performance than the baseline. Qualitative analysis. Here, to verify the effectiveness of the proposed Task-oriented Point Selection (TPS) module, we employ t-SNE [54] to project the point features into 2D space. For every point class, we randomly choose 100 points for visualization, the results are shown in Figure 6. As we can see, “Ours w/o TPS” can hardly separate points that belong to different classes, resulting in all points being mixed within the 2D space. In contrast, the point features extracted by our full model are grouped by the object labels, which suggests that our model has learned roughly distinct regions in the point feature space to correspond to each object point.
E. Real-World Experiment
As the contributed dataset 6DTG is collected from simulation environments. We conduct real-world robot grasping
Ours Full
Ours w/o TPS
Fig. 6. Analysis of the Task-oriented Point Selection (TPS) module using t-SNE [54]. For each class, we randomly choose 100 point features.
TABLE V
REAL-WORLD EXPERIMENTS.
Mug Knife Scissor
Tasks Grasp Pour Grasp Cut Grasp Cut
Success/Total 8/10 6/10 5/10 4/10 4/10 3/10
experiments to verify whether our model trained from simulation dataset can transfer well to real-world grasping. Specifically, we construct cluttered scenes by randomly placing some objects from knives, Scissors, Mugs, Bottles, and bowls, which are all unknown to the model. Then, we use the model to generate task-oriented grasps given a specific task and a target object. To perform the grasp, we use a fixed Franka Emika robot hand with a parallel-yaw gripper, and a third-view Kinect Azure RGB-D camera is placed on the right front of the robot. As shown in Table V, for easy tasks like ”grasping an object”, our model predicts execution grasp with a high success rate. For more challenging tasks, e.g., grasping the knife for cutting, our model suffers a performance decrease. Overall, the results are consistent with previous experiments, demonstrating the effectiveness of our model, trained on a simulation dataset. The setup and several evaluation examples are attached in the accompanying video.
VI. CONCLUSION
In this work, we investigate the problem named taskoriented 6-DoF grasp pose detection in clutters (TO6DGC). This problem considers 6-DoF grasping and cluttered scenarios compared with previous task-oriented grasp pose detection settings. To facilitate the development of TO6DGC, we contribute a new dataset, named 6-DoF Task Grasp (6DTG), which features over 2 million 6-DoF task-oriented grasp poses. We also propose a method, One-Stage Task Grasp (OSTG), and construct several baselines, providing convenience for future research. Our OSTG directly detects task-oriented grasp poses by adapting a task-oriented point sampling strategy and a task-oriented grasp detection module, outperforming all baselines on multiple metrics. Real robot experiments demonstrate the effectiveness of OSTG.
VII. ACKNOWLEDGMENTS
This work was supported partially by NSFC(92470202, U21A20471), National Key Research and Development Program of China (2023YFA1008503), Guangdong NSF Project (No. 2023B1515040025).


REFERENCES
[1] H.-S. Fang, C. Wang, M. Gou, and C. Lu, “Graspnet-1billion: A largescale benchmark for general object grasping,” in CVPR, 2020. [2] C. Wang, H.-S. Fang, M. Gou, H. Fang, J. Gao, and C. Lu, “Graspness discovery in clutters for fast and accurate grasp detection,” in CVPR, 2021. [3] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, “Contactgraspnet: Efficient 6-dof grasp generation in cluttered scenes,” in ICRA, 2021. [4] X.-M. Wu, J.-F. Cai, J.-J. Jiang, D. Zheng, Y.-L. Wei, and W.-S. Zheng, “An economic framework for 6-dof grasp detection,” in ECCV, 2025. [5] Y.-K. Wang, C. Xing, Y.-L. Wei, X.-M. Wu, and W.-S. Zheng, “Singleview scene point cloud human grasp generation,” in CVPR, 2024. [6] B. Lim, J. Kim, J. Kim, Y. Lee, and F. C. Park, “Equigraspflow: Se (3)-equivariant 6-dof grasp pose generative flows,” in CoRL, 2024. [7] N. Marturi, M. Kopicki, A. Rastegarpanah, V. Rajasekaran, M. Adjigble, R. Stolkin, A. Leonardis, and Y. Bekiroglu, “Dynamic grasp and trajectory planning for moving objects,” Autonomous Robots, vol. 43, pp. 1241–1256, 2019. [8] I. Akinola, J. Xu, S. Song, and P. K. Allen, “Dynamic grasping with reachability and motion awareness,” in IROS, 2021. [9] W. Yang, C. Paxton, A. Mousavian, Y.-W. Chao, M. Cakmak, and D. Fox, “Reactive human-to-robot handovers of arbitrary objects,” in ICRA, 2021. [10] J. Liu, R. Zhang, H.-S. Fang, M. Gou, H. Fang, C. Wang, S. Xu, H. Yan, and C. Lu, “Target-referenced reactive grasping for dynamic objects,” in CVPR, 2023. [11] A. Kurenkov, J. Taglic, R. Kulkarni, M. Dominguez-Kuhne, A. Garg, R. Martı ́n-Martı ́n, and S. Savarese, “Visuomotor mechanical search: Learning to retrieve target objects in clutter,” in IROS, 2020. [12] Z. Liu, Z. Wang, S. Huang, J. Zhou, and J. Lu, “Ge-grasp: Efficient target-oriented grasping in dense clutter,” in IROS, 2022. [13] E. Li, H. Feng, S. Zhang, and Y. Fu, “Learning target-oriented pushgrasping synergy in clutter with action space decoupling,” RAL, 2022. [14] K. Fang, Y. Zhu, A. Garg, A. Kurenkov, V. Mehta, L. Fei-Fei, and S. Savarese, “Learning task-oriented grasping for tool manipulation from simulated self-supervision,” IJRR, 2020. [15] A. Murali, W. Liu, K. Marino, S. Chernova, and A. Gupta, “Same object, different grasps: Data and semantic knowledge for task-oriented grasping,” in CoRL, 2021. [16] M. Sun and Y. Gao, “Gater: Learning grasp-action-target embeddings and relations for task-specific grasping,” RAL, 2021. [17] W. Chen, H. Liang, Z. Chen, F. Sun, and J. Zhang, “Learning 6dof task-oriented grasp detection via implicit estimation and visual affordance,” in IROS, 2022. [18] C. Tang, D. Huang, L. Meng, W. Liu, and H. Zhang, “Task-oriented grasp prediction with visual-language inputs,” IROS, 2023. [19] C. Tang, D. Huang, W. Ge, W. Liu, and H. Zhang, “Graspgpt: Leveraging semantic knowledge from a large language model for taskoriented grasping,” RAL, 2023. [20] M. Savva, A. X. Chang, and P. Hanrahan, “Semantically-enriched 3d models for common-sense knowledge,” in CVPR Workshops, 2015. [21] C. Eppner, A. Mousavian, and D. Fox, “Acronym: A large-scale grasp dataset based on simulation,” in ICRA, 2021. [22] D. Zheng, X.-M. Wu, Z. Liu, J. Meng, and W.-s. Zheng, “Diffuvolume: Diffusion model for volume based stereo matching,” IJCV, 2025. [23] F. Su ́arez-Ruiz, X. Zhou, and Q.-C. Pham, “Can robots assemble an ikea chair?” Science Robotics, 2018. [24] N. T. Dantam, Z. K. Kingston, S. Chaudhuri, and L. E. Kavraki, “Incremental task and motion planning: A constraint-based approach.” in Robotics: Science and systems, vol. 12. Ann Arbor, MI, USA, 2016, p. 00052. [25] G.-H. Xu, Y.-L. Wei, D. Zheng, X.-M. Wu, and W.-S. Zheng, “Dexterous grasp transformer,” in CVPR, 2024. [26] C. Gao, Z. Xue, S. Deng, T. Liang, S. Yang, L. Shao, and H. Xu, “Riemann: Near real-time se (3)-equivariant robot manipulation without point cloud segmentation,” arXiv preprint arXiv:2403.19460, 2024. [27] L. Shao, T. Migimatsu, and J. Bohg, “Learning to scaffold the development of robotic manipulation skills,” in ICRA, 2020. [28] Y. Avigal, L. Berscheid, T. Asfour, T. Kr ̈oger, and K. Goldberg, “Speedfolding: Learning efficient bimanual folding of garments,” in IROS, 2022. [29] R. Papallas and M. R. Dogar, “To ask for help or not to ask: A predictive approach to human-in-the-loop motion planning for robot manipulation tasks,” in IROS, 2022.
[30] A.-L. Wang, K.-Y. Lin, J.-R. Du, J. Meng, and W.-S. Zheng, “Eventguided procedure planning from instructional videos with text supervision,” in ICCV, 2023, pp. 13 565–13 575. [31] A.-L. Wang, B. Shan, W. Shi, K.-Y. Lin, X. Fei, G. Tang, L. Liao, J. Tang, C. Huang, and W.-S. Zheng, “Pargo: Bridging vision-language with partial and global views,” arXiv preprint arXiv:2408.12928, 2024. [32] Y.-M. Li, W.-J. Huang, A.-L. Wang, L.-A. Zeng, J.-K. Meng, and W.-S. Zheng, “Egoexo-fitness: towards egocentric and exocentric full-body action understanding,” in ECCV, 2024. [33] N. Chen, X.-M. Wu, G. Xu, J.-J. Jiang, Z. Chen, and W.-S. Zheng, “Motiongrasp: Long-term grasp motion tracking for dynamic grasping,” RAL, 2024. [34] Y.-M. Li, A.-L. Wang, K.-Y. Lin, Y.-M. Tang, L.-A. Zeng, J.-F. Hu, and W.-S. Zheng, “Techcoach: Towards technical keypoint-aware descriptive action coaching,” arXiv preprint arXiv:2411.17130, 2024. [35] K.-Y. Lin, J.-R. Du, Y. Gao, J. Zhou, and W.-S. Zheng, “Diversifying spatial-temporal perception for video domain generalization,” NeruIPS, vol. 36, 2024. [36] J. Tang, W. Zhang, H. Liu, M. Yang, B. Jiang, G. Hu, and X. Bai, “Few could be better than all: Feature sampling and grouping for scene text detection,” in CVPR, 2022. [37] Z. Zhao, J. Tang, B. Wu, C. Lin, S. Wei, H. Liu, X. Tan, Z. Zhang, C. Huang, and Y. Xie, “Harmonizing visual text comprehension and generation,” arXiv preprint arXiv:2407.16364, 2024.
[38] J. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao et al., “Mtvqa: Benchmarking multilingual text-centric visual question answering,” arXiv preprint arXiv:2405.11985, 2024.
[39] J. Tang, C. Lin, Z. Zhao, S. Wei, B. Wu, Q. Liu, H. Feng, Y. Li, S. Wang, L. Liao et al., “Textsquare: Scaling up text-centric visual instruction tuning,” arXiv preprint arXiv:2404.12803, 2024.
[40] W. Zhao, H. Feng, Q. Liu, J. Tang, S. Wei, B. Wu, L. Liao, Y. Ye, H. Liu, W. Zhou et al., “Tabpedia: Towards comprehensive visual table understanding with concept synergy,” arXiv preprint arXiv:2406.01326, 2024.
[41] J. Lu, H. Yu, Y. Wang, Y. Ye, J. Tang, Z. Yang, B. Wu, Q. Liu, H. Feng, H. Wang et al., “A bounding box is worth one token: Interleaving layout and text in a large language model for document understanding,” arXiv preprint arXiv:2407.01976, 2024.
[42] B. Shan, X. Fei, W. Shi, A.-L. Wang, G. Tang, L. Liao, J. Tang, X. Bai, and C. Huang, “Mctbench: Multimodal cognition towards text-rich visual scenes benchmark,” arXiv preprint arXiv:2410.11538, 2024. [43] H. Feng, Q. Liu, H. Liu, J. Tang, W. Zhou, H. Li, and C. Huang, “Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding,” Science China Information Sciences, 2024.
[44] J.-R. Du, J.-C. Feng, K.-Y. Lin, F.-T. Hong, Z. Qi, Y. Shan, J.-F. Hu, and W.-S. Zheng, “Weakly-supervised temporal action localization by progressive complementary learning,” TCSVT, 2024. [45] J. Varley, J. Weisz, J. Weiss, and P. Allen, “Generating multi-fingered robotic grasps via deep learning,” in IROS, 2015. [46] A. Ten Pas, M. Gualtieri, K. Saenko, and R. Platt, “Grasp pose detection in point clouds,” IJRR, 2017. [47] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection,” IJRR, 2018. [48] P. Ni, W. Zhang, X. Zhu, and Q. Cao, “Pointnet++ grasping: Learning an end-to-end spatial grasp generation algorithm from sparse point clouds,” in ICRA, 2020. [49] W. Liu, A. Daruna, and S. Chernova, “Cage: Context-aware grasping engine,” in ICRA, 2020. [50] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Variational grasp generation for object manipulation,” in CVPR, 2019. [51] H. Liang, X. Ma, S. Li, M. G ̈orner, S. Tang, B. Fang, F. Sun, and J. Zhang, “Pointnetgpd: Detecting grasp configurations from point sets,” in ICRA, 2019. [52] S. Deng, X. Xu, C. Wu, K. Chen, and K. Jia, “3d affordancenet: A benchmark for visual object affordance understanding,” in CVPR, 2021. [53] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical feature learning on point sets in a metric space,” NeruIPS, vol. 30, 2017. [54] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal of machine learning research, vol. 9, no. 11, 2008.