UniGrasp: Learning a Unified Model to Grasp with Multifingered Robotic Hands
Lin Shao1, Fabio Ferreira∗1,2, Mikael Jorda∗1, Varun Nambiar∗1, Jianlan Luo3, Eugen Solowjow4, Juan Aparicio Ojea4, Oussama Khatib1, Jeannette Bohg1
Abstract— To achieve a successful grasp, gripper attributes such as its geometry and kinematics play a role as important as the object geometry. The majority of previous work has focused on developing grasp methods that generalize over novel object geometry but are specific to a certain robot hand. We propose UniGrasp, an efficient data-driven grasp synthesis method that considers both the object geometry and gripper attributes as inputs. UniGrasp is based on a novel deep neural network architecture that selects sets of contact points from the input point cloud of the object. The proposed model is trained on a large dataset to produce contact points that are in force closure and reachable by the robot hand. By using contact points as output, we can transfer between a diverse set of multifingered robotic hands. Our model produces over 90% valid contact points in Top10 predictions in simulation and more than 90% successful grasps in real world experiments for various known two-fingered and three-fingered grippers. Our model also achieves 93%, 83% and 90% successful grasps in real world experiments for an unseen two-fingered gripper and two unseen multi-fingered anthropomorphic robotic hands.
I. INTRODUCTION
The ability of a robot to grasp a variety of objects is of tremendous importance for many application domains such as manufacturing or warehouse logistics. It remains a challenging problem to find suitable grasps for arbitrary objects and grippers; especially when objects are only partially observed through noisy sensors. The most successful approaches in recent literature are data-driven methods where large-capacity models are trained on labeled training data to predict suitable grasp poses, e.g. [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. The main objective of these types of approaches is to generalize to objects that were not part of the training data. All of these models are trained for one particular end-effector, which is typically a two-finger gripper. There are only few exceptions that
This work has been partially supported by JD.com American Technologies Corporation (JD) under the SAIL-JD AI Research Initiative and by the International Center for Advanced Communication Technologies (InterACT). This article solely reflects the opinions and conclusions of its authors and not JD or any entity associated with JD.com.
∗The authors contributed equally. Lin Shao, Fabio Ferreira, Mikael Jorda, Varun Nambiar, Oussama Khatib, Jeannette Bohg are with Stanford Artificial Intelligence Lab (SAIL), Stanford University, Stanford, CA, USA. [lins2,fabiof,mjorda,vnambiar,khatib,bohg] @stanford.edu
Fabio Ferreira is with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Germany. fabioferreira@mailbox.org
Jianlan Luo is with Dep. of ME and Dep. of EECS, University of California, Berkeley, CA, USA. jianlanluo@cs.berkeley.edu Eugen Solowjow, Juan Aparicio Ojea are with Siemens Corporate Technology, Berkeley, CA, USA. [eugen.solowjow, juan.aparicio]@siemens.com
Fig. 1. UniGrasp takes as input a kinematic description of a robotic hand and a point cloud of an object. Given this input, UniGrasp is trained on a large dataset to produce contact points on the object surface that are in force closure and reachable by the robotic hand. UniGrasp produces valid contact point sets not only on novel objects but also generalizes to new multifingered hands that it has not been trained on.
consider more dexterous hands e. g. [4, 10, 11, 12, 14, 13]. Of those, [10, 12, 13] go beyond a small set of pre-grasp shapes. [7] is the only approach we are aware of that considers two different grippers in one model but still trains separate grasp synthesis models for each one.
In this paper, we go beyond generalization to unseen objects. We propose UniGrasp, a model that additionally generalizes to novel gripper kinematics and geometries with more than two fingers (see Fig. 1). Such a model enables a robot to cope with multiple, interchangeable or updated grippers without requiring retraining. UniGrasp takes as input a point cloud of the object and a robot hand specification in the Unified Robot Description Format (URDF) [15]. It outputs a set of contact points that are in force closure and reachable by the robot hand. This contact-based grasp representation allows to infer precision grasps that exploit the dexterity of the multi-fingered hand. It also alleviates the need to define a finite set of grasp pre-shapes and approach directions.
This work makes the following contributions: (1) Given a point cloud from an object and a URDF model of a gripper, we map gripper and object features into separate lower-dimensional latent spaces. The resulting features are concatenated and form the input to the part of the model that generates sets of contact points. (2) We propose a novel, multi-stage model that selects sets of contact points from an object point cloud. These contact points are in force closure
arXiv:1910.10900v2 [cs.RO] 7 Sep 2020


and reachable by the gripper to produce a 6D grasp. (3) In simulation and the real world, we show that both, our mapping scheme and multi-stage model adapts well to a varying number of robot hands. The selected contact point sets are typically clustered in a local neighborhood. Such an attribute is well suited for execution on a real robot because precisely aligning the robotic fingers with an isolated point may be hard to achieve. (4) Finally, we publish a new largescale dataset comprised of tuples of annotated objects with contact points for various grippers, covering many popular commercial grippers.
II. RELATED WORK
In the following, we review related work that shares some of the key assumptions with our work, e.g. the object to be grasped is novel and only observed through a noisy depth camera from a single point of view. There are grasp synthesis methods for multi-fingered hands [16, 17, 18] that do not rely on learning approaches. Different from our approach, they typically rely on full observations of the objects and take several seconds of computation. For a broader review of the field on data-driven grasp synthesis, we refer to [1]. Furthermore, we briefly discuss learning-based methods for processing point clouds.
A. Data-Driven Grasp Synthesis for Novel Objects
Learning to grasp novel objects from visual data has been an active field of research since the seminal paper by Saxena et al. [19]. Since then, many approaches have been developed that use more sophisticated function approximators (CNNs) to map from input data to a good grasp [3, 4, 5, 6, 7, 8, 9, 10, 11, 20, 14, 21]. These approaches differ in the input data format (e.g. 2D images, depth maps or point clouds), the grasp representation (e.g. grasping points, oriented 2D rectangles, a 2D position and wrist orientation or a full 6D pose) and whether they learn to grasp from real world examples or synthetic data. In our work, we take as input a point cloud of a segmented object and generate a set of contact points, one per finger of the hand. We train our model on synthetically generated data. The key difference to previous work is that we train one model that generalizes over many robot hands, even if they were not part of the training data. All of the aforementioned prior work is trained for one particular hand. Typically, this is a two-fingered gripper with exceptions including [4, 10, 14, 11, 12, 13]. [7] is the only approach we are aware of that considers two different grippers (a suction cup and a parallel jaw gripper). However, the authors still train separate grasp synthesis models for each gripper. Another key difference to previous work is that we adopt contact points to represent a grasp. Contact points are rarely used in recent work because it is difficult to precisely make contact with these points under noise in perception and control [1]. For this reason, the dominant grasp representation in related work has been the pose of the end-effector, e.g. [4, 8, 7, 11]. This representation allows to reduce the complexity of precise control to simply close the fingers after having reached a particular end-effector pose. However, this requires to define a finite set of pre-grasp shapes which limits the dexterity of
a multi-fingered robot hand, e.g. [4, 11, 12, 14, 13]. Also the resulting grasps are typically power grasps which are great for pick-and-place. However, for assembly or similar tasks, precision grasps are more desirable. A model that outputs contact points per finger basically defines such a precision grasp. Through inverse kinematics, it yields an end-effector pose in SE(3) and a finger configuration that exploits the full dexterity of the hand. Of the aforementioned related works, [10, 12, 13] use a multi-fingered robot hand and go beyond a small set of pre-grasp shapes. [10] suggest to use a CNN that produces heatmaps which indicate suitable locations for fingertip and palm contacts. These contacts are used as a seed for a grasp planner to determine the final grasp pose for the reconstructed object. In follow-up work [22], a CNN completes object shapes from partial point cloud data to provide the grasp planner with a more realistic object model. [12] propose to train a deep conditional variational auto-encoder to predict the contact locations and normals for multi-fingered grasps given an RGB-D image of an object. [13] propose a method that directly optimizes grasp configuations of a multifingered hand by backpropagating through the network with respect to these parameters. All these methods work for a specific hand while our method not only generalizes over unseen objects but also over novel robotic grippers.
B. Representation Learning for Point Clouds
Recently, various approaches for processing sparse point clouds have been proposed in related work. [23] propose PointNet to learn object features for classification and segmentation. PointNet++ [24] applies PointNet hierarchically for better capturing local structures. In our approach, we adopt PointNet [23] to extract features of robotic hands and PointNet++ to extract object features. [25] train an autoencoder network to reconstruct point clouds. The learned representations enable shape editing via linear interpolation inside the embedding space. We train a similar auto-encoder network to learn a representation of various robotic hands.
III. TECHNICAL APPROACH
Given a point cloud of an object and a URDF of a gripper, we aim to select a set of contact points on the surface of the object such that these contact points satisfy the force closure condition and are reachable by the gripper without collisions as shown in Fig. 2. If the gripper has N fingers, our model UniGrasp selects a set of N contact points from the object point cloud. In this section, we first discuss how we learn a uniform representation of N -fingered robotic hands. This representation can be constructed from a URDF file which describes the kinematics and geometry of the hand. Furthermore, we describe how N contact points are generated on the object surface. We conclude this section by describing the training of the neural network model named Point Set Selection Network (PSSN). We report how we generate a large-scale annotated data set with objects being grasped by a diverse set of two and three-fingered robotic hands in Sec. IV. Because we have annotated data only for two and three-fingered robotic hands, our trained network model works for two and three-fingered


Fig. 2. Overview of UniGrasp for a 3-fingered gripper. First, we individually map gripper and object features into a lower-dimensional latent space. Their representations are concatenated and fed into our multi-stage Point Set Selection Network (PSSN) that generates contact points. The contact points are in force closure and yield a collision-free grasp for the 3-fingered gripper.
robotic hands. Conceptually, our model could be extended to hands with more fingers if the training data was available.
A. Gripper Representation
We aim to find a compact representation of the gripper geometry and kinematics as input to UniGrasp. In this section, we first describe how we encode the geometry of robotic hands. Then we describe how we project a novel robot hand into the learned latent space and construct the input feature (see Fig. 3).
1) Unsupervised Learning of a Gripper Representation: We use an autoencoder to learn a low-dimensional latent space that encodes the geometry of robotic hands in a specific joint configuration. For training, we synthesize 20k point clouds of 2000 procedurally generated grippers each in different joint configurations. The generated 2K grippers are two and three-fingered grippers. Their diameter varies from 10 to 30cm. These grippers have prismatic and revolute joints. The number of joints varies from one to twelve. Each point cloud consists of 2048 points. We use PointNet [23] as encoder without the transformation modules. This yields a k-dimensional feature vector that forms the basis for the latent space. The decoder transforms the latent vector using 3 fully connected layers. The first two layers use a ReLU activation function. The last layer produces a 2048×3 output, the reconstructed point cloud of the gripper. We use the Chamfer distance [26] to measure the discrepancy between the input and reconstructed point cloud.
2) Robotic Hand Representation: The autoencoder described in the previous section allows us to encode a robot hand in a specific joint configuration. However, we also want to encode the gripper kinematics and range of joints. In the following, we describe how we parse the URDF [15] file and construct a gripper feature for the example of a 3-fingered hand with 3 DoF. Let us assume, each finger of the hand has one revolute joint. We denote the joints as θ1, θ2 and θ3. Each joint has joint limits. Let Li and Hi represent the minimum and maximum joint angle. There are 23 = 8 joint configurations that outline the boundaries of the configuration space of the hand, e.g. (L1, L2, L3) (H1, L2, L3) (L1, H2, L3). We refer to these as boundary configurations. Let Mi = 0.5Hi+0.5Li. Then (M1, M2, M3) describes the joint configuration of the hand where each joint angle takes the mean value of joint limits. We refer to this as central configuration. We generate point clouds sampled on the surface of the gripper model under all boundary and the central configurations. For the above example of a 3DoF gripper, there are 9 point clouds in total. They represent the kinematic and geometric attributes of this specific robotic hand.
Fig. 3. Top: We train an autoencoder to learn a lower-dimensional representation of our grippers and their random configurations by minimizing the Chamfer distance between the reconstructed and the ground truth gripper point clouds. Bottom: The trained encoder is used to generate a feature representation of the geometry and kinematics of the input gripper.
We feed the 9 point clouds into the autoencoder to extract features (see Fig. 3). Note that robotic hands with a different number of DoF have a different number of boundary configurations. To get a fixed-size feature, we apply three pooling operations (max-, min-, and mean-pooling) among the batch dimension. The output features of those three operations are concatenated to get a final robotic hand feature.
B. Contact Point Set Selection
We use PointNet++ [24] to extract features of the object point cloud represented by S0 = {pi}L
i=1 where L denotes the number of points. The object feature has a dimension of L×64 and the gripper feature has a dimension of 1×768. We repeat the gripper feature along the first dimension by L and change it to be L × 768. We then concatenate gripper feature and object features along the second dimension leading to a new dimension of L × 832. We apply 1D convolution of the feature of L × 832 and the new feature is L × 64 denoted as F0 which is the input to the point set selection network. We aim to select N points from this point cloud such that these N points form force-closure and are reachable by the robotic hand without collisions. The task of training a neural network for point set selection is new and challenging. Fig. 2 give a high-level overview of the multi-stage point selection process in PSSN. In Stage One, our model first selects one point pa from the original point cloud S0. Conditioned on the point pa, the network continues to select the second point pb in Stage Two. If the robotic hand is two-fingered, the point selection procedure ends. The two points (pa, pb) are the two contact points to grasp. For a three fingered robotic hand, the third point pc will be selected during Stage Three. In this paper, we only report results for two and three-fingered hands. For robotic hands with more fingers, our model can conceptually continue to select contact points by adding more stages. In each Stage n, the neural network will select the next point conditioned on the previously selected n − 1 points. For one object and an N-fingered gripper, we denote the list of all valid point sets as V = {(pˆN
i=1)}. An item in this list is a set containing N points. A set is said to be valid if it forms force closure and is reachable by the associated robotic hand. However, it is challenging to find these valid point sets among the vast number of possible contact point sets in the


Fig. 4. The above scheme describes the multi-stage process shown in Fig. 2 in more detail. Given a feature matrix as input (comprised of gripper and object features), PSSN successively expands into n stages to generate the set of contact point coordinates of an n-fingered gripper. The two top rows describe the general flow of information from left to right by repeatedly using the modules from the bottom row. Solid lines indicate inputs to modules while dotted lines signify a read (no edge label) or write (update-feat) access to previous stage elements. Stage One is primarily used to find a representation of the subset of features that are most promising contact point sets. This representation is used as a template for the following stages (here: Two and Three), which each maintain their own copy of it. These subsequent stages inflate their input before deflating it into feature coordinate vectors (in purple). The coordinates indicate a) which template-copy elements should form the input for the next stage and more importantly b) the indices of data points in the point cloud representing the final grasp locations. Each stage adds one coordinate.
object point cloud. To add robustness to the point selection procedure, we adopt an approach akin to beam search [27] where during each stage the network selects multiple, most promising contact points. The detailed architecture of PSSN is shown in Fig. 4. In the following, we describe each stage in detail. Stage One: We first select K1 points denoted as S1 from the original point cloud S0 = {pi}L
i=1 as follows. The feature F0 with a shape of (L, 64) are fed into a 1d convolution and soft-max layer to calculate the probability of each point being valid. The model then selects the K1 points with the highest scores and gathers the corresponding features denoted as F1 with a dimension of (K1, 64). Stage Two: This stage aims to select points from S0 that conditioned on the points in S1 yield valid point sets. As a first step, Stage Two selects K2 points from S0 in the same way as Stage One. This set of points is referred to as S2. The network collects the corresponding point features in F2 which is of dimension (K2, 64). Then, feature maps F1 and F2 are copied and put into a reshape-1d-cnn layer visualized in Fig. 4. The network performs a pairwise copy and concatenation to produce a combined feature map F3 of dimension (K2, K1, 64). The feature map is then sent to 1d convolution and soft-max layer to get the final score matrix of dimension (K2, K1). We collect the top K3 elements denoted as S3. For two fingered robotic hands, these two points are the contact points predicted by PSSN. Stage Three: In this stage, the network selects a third contact point. For this purpose, the network accesses the features of each element in the set S3. Each index in the set S3 refers to two previously selected points of Stage One and Stage Two. The network copies the corresponding features
and concatenates them. The new feature map has a dimension of (K3, 128) and is fed into a 1d convolutional layer with an output of dimension (K3, 64) denoted as F4. We select the third point index from the already reduced set S1. For this purpose, the network simultaneously gathers the feature map F4 with dimension (K3, 64) and feature map F1 with dimension (K1, 64). These two feature maps are fed into a reshape-1d-cnn layer to get a pairwise concatenation with a dimension of (K3, K1, 64) denoted as F5. The feature map F5 is input to a 1d convolution and soft-max to calculate the score matrix of dimension (K3, K1). Each element (u, v) in this matrix corresponds to the uth element in S3 and the vth point in set S4. Note that the uth element in the set S3 refers to two points selected in the previous two stages. Based on the score, the network determines the K4 top contact points sets. These form the candidate grasps for the three-fingered robotic hand. In practice, we find it is easier to train the PSSN when rejecting a large number of invalid point sets. The following heuristics reduce the number of candidate triplets from billions to millions. As a first step, PSSN predicts a normal per point in this point cloud. We denote these predicted normal vectors {pˆni}L
i=1. Then, we leverage simple heuristics [28] to reject invalid point sets based on point positions and the predicted point normals. For two-fingered robotic hands, the angle between the two contact point normals needs to be larger than 120◦ to be in force closure [29]. For threefingered robotic hands, three contact points form a triangle. We set a constraint that each side of the triangle needs to be larger than 1 cm to prevent our model from selecting points which are too close to each other. In addition, we prefer contact points that form a more regular triangle and


we set another two constraints. The maximum internal angle of the triangle needs to be less than 120◦. For each point, the angles between its normal vector and two incident edge directions are less than 90◦. It indicates that the point normal is pointing outwards of the triangle. Generally three contact points in force closure fulfill those constraints above.
C. Loss Function and Optimization
Given the ground truth point normals {pni}L
i=1, the loss of predicted point normals is defined to be
Lpn = −
L
∑
i=1
(pˆni · pni)
where · represents the dot product. [30] found that formulating grasp success prediction as a ranking problem yields higher accuracy than formulating it as a binary classification problem. We therefore formulate point selection at every stage as a learning-to-rank [31] problem. For an object point cloud and a gripper, every point set which is in force closure and reachable by this specific robot hand is annotated as positive. All other point sets are labeled negative. Therefore, each point set has a binary label y. In Stage n (where n ≤ N), the network predicts a list of Kn point sets and their corresponding probability yˆ of whether this set is valid or not. Based on their predicted probabilities, we have a ranked list of these point sets. We use a variant of the ListNet [32] loss to increase the yˆ of point sets with positive labels.
Ln = −
Kn
∑
j=1
yj log( exp(yˆj)
∑Kn
j=1 exp(yˆj ) )
We use the Adam optimizer [33] for training the network, set the learning rate to 10−4, and split the data into 80/20 for training and test sets. We train the neural network stage by stage. First, we only train Stage One. Only the loss of Stage One is computed and the gradients are back-propagated to the weights of the layers in Stage Two. After training Stage One, we fix the weights in Stage One and continue to train the layers in Stage Two. Only the loss of Stage Two is computed. The training procedure continues until Stage N where in this paper N = 2, 3.
IV. GRASP DATASET GENERATION
To train the UniGrasp model, we require data that consists of object point clouds annotated with sets of contact points that are in force closure and reachable by specific grippers. We generate this data set in simulation as commonly done for other data-driven approaches [4, 34, 6, 7]. To construct this dataset, we select 1000 object models that are available in Bullet [35] and scale each object up to five different sizes to yield 3275 object instances. We use 12 different robotic hands. Nine of these hands are two-fingered robotic grippers and three of them are three-fingered 1. The data generation process is visualized in Fig. 5.
1We use commercially available grippers such as the Sawyer [36] and Panda [37] parallel-yaw gripper, Robotiq 3-Finger [38], Kinova KG-3 [39], and Barrett BH8-282 gripper [40]
Fig. 5. Overview of our dataset generation process. Point clouds of different Bullet [35] object meshes were generated based on images rendered from 8 different viewpoints around the objects. All point sets passing a forceclosure check were then added to the list of valid points. If these point sets are also reachable and collision-free by a specific gripper, then it is added to the final dataset used to train UniGrasp.
We place the object on a horizontal plane and render the object from eight viewpoints to generate RGB-D images. We reconstruct a point cloud from those RGB-D images and down-sample them to 2048 points. We find 2048 points are dense enough to represent the geometry of the object given GPU memory constraints. Given a robotic hand with N fingers (N= 2,3), we generate all possible sets of N points that are in force closure and reachable by the given robotic hand. For an object represented by a point cloud of 2048 points and a three
fingered robotic hand, the total number of combinations is
(2048 3
) ≈ 1.4 × 109. We assume a friction coefficient of 0.5 for two-fingered grippers and of 0.65 for three-fingered grippers and use a polygonal approximation of the friction cone with 16 faces. We first utilize the heuristics described in Section III-B to reject large amounts of point sets. We use FastGrasp [41] to evaluate the point sets and compute the grasp quality score Q−
l . We label a point set to be in
force closure for two-fingered grippers, if Q−
l > 0 and a point set to be in force closure for three-fingered grippers, if Q−
l > 0.0001.
If a set of N points is in force closure, we use inverse kinematics for each N -fingered hand to determine (a) whether the points are reachable and (b) whether the grasp is collisionfree - both specific to the hand. All sets of contact points that are in force closure, reachable and collision-free are labeled as positive for a specific gripper and negative otherwise.
V. EXPERIMENTS
In this section, we extensively evaluate the proposed grasp synthesis method in terms of the accuracy of contact point selection for various known N-fingered robotic hands both in simulation and in real world experiments. We demonstrate the validity of the learned gripper embedding space and show that our model can produce valid contact points for different novel robotic hands, both in simulation and the real world experiments.
A. Robotic Hand Presentation Learning
For training the autoencoder to represent the gripper, we split the gripper point clouds described in Sec.III into training and test set with a ratio of nine to one. We use five consecutive layers of 1D convolution in PointNet [23]. The feature channel dimensions of each convolution layers are (64,64,128,128,256,256) starting from input layers. The final feature embedding space has a dimension of 256.


Fig. 6. Interpolation in Gripper Feature Space. The top row indicates prismatic joint movement, the middle row represents revolute joint movement. The bottom row shows the geometry changes between two types of 3-fingered robotic hands.
The training and test Chamfer Distance losses [26] are 4.87 × 10−5 and 4.44 × 10−5 respectively. To demonstrate that the learned representation is able to capture robotic hand attributes like prismatic and revolute joint movement as well as variety in geometry, we show the linear interpolation ability between two points in the latent space. We take two point clouds P c1 P c2 and feed them into the autoencoder. We extract the corresponding features from the encoder denoted as F1 and F2. We generate new features F3 = 0.8F 1 + 0.2F 2 and F4 = 0.2F 1 + 0.8F 2 by interpolation between F1 and F2. F1 − F4 features are put into the decoder to reconstruct point clouds. The results are shown in Fig. 6 and indicate the meaningful interpolation not only in configuration space but also in shape space.
B. Grasp Point Set Selection Performance
In simulation, PSSN selects K1 = 1024 in Stage One, K2 = 1024 and K3 = 1024 in Stage Two and K4 = 1024 in Stage Three. We run six experiments to demonstrate the strength of our proposed method in predicting valid contact points for up to three-fingered robot hands. 1) Evaluation Metrics: We adopt two evaluation metrics to reflect prediction accuracy. Top1 refers to the prediction accuracy of the highest ranked point to be within 5mm of a valid point set V, i.e. of a point set that forms a force closure grasp, is reachable and collision-free by the input hand. Top10 refers to the percentage of test cases where at least one valid point set of the top 10 predicted ones is within 5mm of a valid grasp. We allow the 5mm relaxation for two reasons. a) There is always annotation noise e.g. in the point normal or force closure estimation. b) In the real world, fingertips often have a width of 10mm. With a 5mm tolerance the fingertip still covers the valid contact point. A similar relaxation is also used in previous work [21]. Neighbor reports the ratio of valid point sets over all possible point sets within a local neighborhood of our prediction. We set the radius of this local neighborhood around each contact point to be 5mm. A high number shows that there are many valid points around a predicted contact point. This is helpful during grasp acquisition in the real world where noise in the point clouds and in robot actuation may lead to alignment errors between the robot fingers and contact points. 2) Baseline: There is no previous work that has proposed a model for grasping novel objects with novel grippers. Therefore, we propose the following baseline that is close to our approach on data generation but is not based on learning. Given a partial point cloud of an object, we apply the same
aforementioned heuristics to reject contact point sets that are unlikely to be feasible grasps. We run FastGrasp [41] to evaluate the remaining point sets. FastGrasp requires as input the object’s center of mass and the surface normals at the contact points. As we do not assume access to these properties in UniGrasp, we approximate them for FastGrasp. The object’s center of mass is the average position of all points in the point cloud. To estimate the normal of each point, we compute the covariance matrix of the nearest 30 points. We adopt the eigenvector with the smallest eigenvalue as normal. The normals are oriented towards the estimated center. Given this information, we iterate over contact point sets with estimated point normals and center of mass until FastGrasp returns a point set that is in force closure. If this point set is also reachable and collision-free, we consider the grasp successful. If not, we continue iterating over candidate point sets. If FastGrasp fails to return a point set which has positive label in the dataset within 10 second, we consider the object grasping as a failure. 3) Results: To test our model’s generalization ability to novel objects, we train a neural network on the training dataset and report its performance on the test dataset. The results are given in Fig. 7. We parse the URDF of various grippers and generate their features as described in Sec. III. During each training stage, our model selects point sets and ranks them. For example, given a two-fingered robotic hand, the network has two prediction stages. For two-fingered grippers, our model achieve 92.8% and 83.7% Top1 accuracies for Saywer and Franka, respectively. We also test the grasping performance for three-fingered robotic hands. We train our point set selection network on the objects with annotation of Kinova, Robotiq and Barrett hand, respectively. Our model achieves 77.1%, 86.9% and 89.6% Top1 test accuracy. We run the baseline using the two-fingered Sawyer gripper and the three-fingered Robotiq-3F gripper and compare it with UniGrasp on the same hands. Our UniGrasp method achieves 86.9% and 89.6% average grasp success rates, respectively. The baseline only achieves 66.2% and 20.9% average grasp success rates. Therefore, UniGrasp significantly improves over the baseline. To test our model’s generalization ability to novel robotic hands, we first train the neural network model on the dataset annotated with all two-fingered grippers except the Sawyer and Robotiq-2F. In the test stage, we first parse the URDF file of these two new grippers and extract corresponding gripper features. We then give these features as input to PSSN and evaluate the grasp performance on the test dataset annotated for each gripper. Our model achieves an accuracy of 86.6% and 75.8% on the Top1 prediction. There are on average 43% and 37% valid points that are Neighbors of the predicted contact point. This shows that our model clusters valid contact point sets in regions on the object surface. This is beneficial for real-world grasp executions under perception and actuation noise as for example discussed in [42]. Fig. 8 illustrates that our model generates distinct contact point sets on the same example object for various robotic hands. This qualitatively shows that our model generates


Accuracy Stage One Train Stage One Test Stage Two Train Stage Two Test Stage Three Train Stage Three Test Neighbor
Top1 Top10 Top1 Top10 Top1 Top10 Top1 Top10 Top1 Top10 Top1 Top10 T Sawyer 99.8 100.0 99.6 100.0 93.6 97.6 92.8 96.8 - - - - 53 U Robotiq-2F - - 93.8 100.0 - - 75.8 91.7 - - - - 37 U Sawyer - - 97.4 100.0 - - 86.6 95.4 - - - - 43 B Sawyer - - - - - - 66.2 88.8 - - - - T Franka 99.0 99.8 98.7 100.0 84.7 95.6 83.7 91.0 - - - - 43 T Kinova-3F 96.6 99.7 96.8 99.8 91.6 96.3 91.6 96.3 76.3 89.5 77.1 90.7 26 T Barrett 98.4 100.0 98.7 99.8 95.6 98.3 97.1 98.9 90.6 96.4 89.6 96.3 40 T Robotiq-3F 98.2 99.8 98.4 99.6 95.6 98.3 96.3 98.4 86.6 94.2 86.9 95.1 36 B Robotiq-3F - - - - - - - - - - 20.9 41.0 
Fig. 7. Summary of our grasping evaluation in simulation for two-fingered and three-fingered grippers. We report prediction accuracies of PSSN at different stages for different grippers. We distinguish between two different evaluation strategies and denote them by T and U respectively. A gripper prefixed with T (e.g. T Sawyer) indicates the gripper was trained and evaluated on train-test-splits dataset while both splits contained samples of that gripper (among other grippers). In contrast, U-prefixed grippers denote that samples of the gripper were not used during training. The gripper prefixed with B presents the grasp performance of that gripper using baseline method. The column Neighbor shows the percentage of valid grasps among all the existing point sets in a neighborhood of the selected grasp.
Fig. 8. Examples of contact point selection results on one object with two-fingered grippers and Kinova-kg3. The black point cloud represents the object in top view. Red, yellow and green spheres indicate the contact point predicted by our PSSN in Stage One, Two and Three respectively. Blue points represent the gripper. For two-fingered grippers, left gripper represents the full-closed status and right gripper represents the full-open status. Our method generates contact point sets that are adapted to robotic hand attributes.
Accuracy Stage One Test Stage Two Test Neighbor
Top1 Top10 Top1 Top10 G1/G2 82.3 95.2 26.6 30.7 8.8 G1/G1 89.9 98.0 55.8 60.8 27.5 G2/G1 70.6 85.9 54.3 61.7 32 G2/G2 77.1 92.6 73.8 81.2 40
Fig. 9. G1 and G2 denote the left and middle gripper shown in Fig. 8, respectively. G1/G2 means the input gripper is G1 and the test ground truth data is G2. G1/G1 means the input gripper is G1 and the test ground truth data is G1.
contact points that are specific to the robotic hand attributes. We also conducted an experiment that quantitatively analysis how much the gripper features matter in generating valid contact points. For this, we denote the left two-fingered gripper G1 and the right two-fingered gripper G2 in Fig. 8. We send the gripper feature of G1 and evaluate the predicted results using annotated data of G2 (denoted as G1/G2). We run four experiments which are G1/G1, G1/G2, G2/G1, G2/G2. In Fig. 9, we show that the grasp success rate drops by 30-20% when the test hand does not match the input hand. This demonstrates that PSSN leverages the gripper embedding to generate contact points which are specific to the input gripper.
C. Real World Experiments
We evaluate the performance of our model in real world experiments on 22 objects which are shown in the corresponding video. The object set includes eight objects from Dexnet2.0 [6], three objects from the YCB object data set [43] and two deformable objects. We use a laptop with Intel Core i9 CPU and Nvidia 1070 GPU to run our neural network model. Our hand-eye camera setup uses an RGB-D
camera to capture the depth images of a given object. We use a single depth image for each grasping experiment. The depth image is inpainted [44] using OpenCV to remove invalid values. Utilizing the camera matrix, we can reconstruct a 3D point cloud. We feed the gripper description and object point cloud into our UniGrasp model. UniGrasp outputs each finger’s contact point. We solve the inverse kinematics for the robotic gripper using RBDL [45] which takes around 0.4 milliseconds, and command the robot to approach the object using a Cartesian space controller. We close the fingers and lift the object. PSSN simultaneously selects 256 point sets. We start to go through this list from top1 and use an inverse kinematics solver to compute the joint configuration for reaching the point set. Although UniGraps is trained to output reachable contact points, there is no guarantee. However, if the inverse kinematics solver does not generate a solution or there is a collision, we can select the next best set of contact points in the output list. However, this did not occur in our real-world experiments. We apply our model in the real world using the Kinova KG-3 and Robotiq-3F grippers that our model was trained on. To test the generalization of our model over grippers, we remove one finger of Kinova KG-3 in the URDF file to create a novel two-fingered gripper. We also apply UniGrasp to the Schunk SVH five-fingered robotic hand [46]. Our UniGrasp model produces three contact points for the thumb, index and ring finger. After solving inverse kinematics for those three fingers, we control the five fingers by having the middle finger mimic the index and the pinkie mimic the ring finger. Finally, we also apply our method to the Allegro hand (from Wonik Robotics) using the thumb, index and middle finger, and we compare our method to the baseline in the case of the Allegro hand. The results are shown in Fig. 10. Our method achieves 92% and 95% successful grasps for Kinova-KG3 and Robotiq-3F grippers. It also generates 93% for the novel two-fingered gripper. Although in our experiment the thumb of the Schunk hand was unfortunately broken, our model achieves 83% successful grasps. For the Allegro hand, we achieve a 90% success rate while the baseline only achieves 40% of successful grasps.
VI. CONCLUSION
We present a novel data-driven grasp synthesis approach named UniGrasp that generates valid grasps for a wide


Trials SuccessRate(%) RunTime(s) T Kinova-KG3 64 92 0.13 T Robotiq-3F 65 95 0.20 U Kinova-2F 60 93 0.06 U Schunk SVH 60 83 0.21 U Allegro Hand 60 90 0.22 B Allegro Hand 60 40 4.87
Fig. 10. Summary of our grasping evaluation in the real world for various grippers. The prefix T and U are the same as in Fig. 7. The prefix B represents the baseline described in Sec. V. RunTime represents computation time spent on PSSN or the average time spent on heuristics+FastGrasp for the baseline.
range of grippers from two-fingered parallel-yaw grippers to articulated multi-fingered grippers. UniGrasp takes point clouds of the object and the URDF of robotic hand as inputs. The outputs are the contact points on the surface of objects. We show in quantitative experiments that our method predicts over 90% valid contact points for various known two- and three-fingered grippers in Top10 predictions and over 90% valid grasps in real world experiments. Our model also generates 93%, 90%, and 83% successful grasps for novel two-fingered, four-fingered and five-fingered robotic hands in real-world experiments. In future work, we aim to extend UniGrasp to n-fingered hands where n > 3. For this, we will also reconsider the heuristics used for rejecting contatc point sets.
REFERENCES
[1] J. Bohg, A. Morales, T. Asfour, and D. Kragic, “Data-driven grasp synthesisa survey,” IEEE Transactions on Robotics, vol. 30, no. 2, pp. 289–309, April 2014. [2] A. Sahbani, S. El-Khoury, and P. Bidaud, “An overview of 3d object grasp synthesis algorithms,” Robotics and Autonomous Systems, vol. 60, no. 3, pp. 326 – 336, 2012, autonomous Grasping. [3] I. Lenz, H. Lee, and A. Saxena, “Deep learning for detecting robotic grasps,” The International Journal of Robotics Research, vol. 34, no. 4-5, pp. 705–724, 2015. [4] D. Kappler, J. Bohg, and S. Schaal, “Leveraging big data for grasp planning,” in Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE, 2015, pp. 4304–4311. [5] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours,” in Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 2016, pp. 3406–3413.
[6] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics,” in Proceedings of Robotics: Science and Systems, 2017.
[7] J. Mahler, M. Matl, V. Satish, M. Danielczuk, B. DeRose, S. McKinley, and K. Goldberg, “Learning ambidextrous robot grasping policies,” Science Robotics, vol. 4, no. 26, p. eaau4984, 2019. [8] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, “Learning handeye coordination for robotic grasping with deep learning and large-scale data collection,” The International Journal of Robotics Research, vol. 37, no. 4-5, pp. 421–436, 2018. [9] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine, “Scalable deep reinforcement learning for vision-based robotic manipulation,” in Proceedings of The 2nd Conference on Robot Learning, ser. Proceedings of Machine Learning Research, A. Billard, A. Dragan, J. Peters, and J. Morimoto, Eds., vol. 87. PMLR, 29–31 Oct 2018, pp. 651–673. [10] J. Varley, J. Weisz, J. Weiss, and P. Allen, “Generating multi-fingered robotic grasps via deep learning,” in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Sep. 2015, pp. 4415–4420.
[11] P. Schmidt, N. Vahrenkamp, M. W ̈achter, and T. Asfour, “Grasping of unknown objects using deep convolutional neural networks based on depth images,” in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 6831–6838. [12] M. Veres, M. Moussa, and G. W. Taylor, “Modeling grasp motor imagery through deep conditional generative models,” IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 757–764, 2017. [13] Q. Lu, K. Chenna, B. Sundaralingam, and T. Hermans, “Planning multifingered grasps as probabilistic inference in a learned deep network,” in Robotics Research. Springer, 2020, pp. 455–472. [14] H. Liang, X. Ma, S. Li, M. G ̈orner, S. Tang, B. Fang, F. Sun, and J. Zhang, “PointNetGPD: Detecting grasp configurations from point sets,” in IEEE International Conference on Robotics and Automation (ICRA), 2019.
[15] “Unified robot description format (urdf),” 2019. [Online]. Available: http: //wiki.ros.org/urdf [16] K. Hang, J. A. Stork, and D. Kragic, “Hierarchical fingertip space for multi
fingered precision grasping,” in 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2014, pp. 1641–1648.
[17] M. Li, K. Hang, D. Kragic, and A. Billard, “Dexterous grasping under shape uncertainty,” Robotics and Autonomous Systems, vol. 75, pp. 352–364, 2016. [18] Y. Fan, T. Tang, H.-C. Lin, and M. Tomizuka, “Real-time grasp planning for multi-fingered hands by finger splitting,” in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 40454052. [19] A. Saxena, J. Driemeyer, and A. Y. Ng, “Robotic grasping of novel objects using vision,” The International Journal of Robotics Research, vol. 27, no. 2, pp. 157173, 2008. [20] D. Morrison, P. Corke, and J. Leitner, “Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach,” in Proceedings of Robotics: Science and Systems (RSS), 2018.
[21] Q. V. Le, D. Kamm, A. F. Kara, and A. Y. Ng, “Learning to grasp objects with multiple contact points,” in 2010 IEEE International Conference on Robotics and Automation. IEEE, 2010, pp. 5062–5069. [22] J. Varley, C. DeChant, A. Richardson, J. Ruales, and P. Allen, “Shape completion enabled robotic grasping,” in Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on. IEEE, 2017, pp. 2442–2447.
[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on point sets for 3d classification and segmentation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 652–660.
[24] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical feature learning on point sets in a metric space,” in Advances in Neural Information Processing Systems, 2017, pp. 5099–5108. [25] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning representations and generative models for 3D point clouds,” in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. Stockholmsm ̈assan, Stockholm Sweden: PMLR, 10–15 Jul 2018, pp. 40–49. [26] H. Fan, H. Su, and L. Guibas, “A point set generation network for 3d object reconstruction from a single image,” in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 2463–2471.
[27] Wikipedia contributors, “Beam search — Wikipedia, the free encyclopedia,” 2019. [Online]. Available: https://en.wikipedia.org/wiki/Beam search [28] C. Borst, M. Fischer, and G. Hirzinger, “Grasping the dice by dicing the grasp,” in Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453), vol. 4. IEEE, 2003, pp. 36923697. [29] V.-D. Nguyen, “Constructing force-closure grasps,” The International Journal of Robotics Research, vol. 7, no. 3, pp. 3–16, 1988. [30] D. Kappler, S. Schaal, and J. Bohg, “Optimizing for what matters: the top grasp hypothesis,” in 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016, pp. 2167–2174. [31] R. K. Pasumarthi, S. Bruch, X. Wang, C. Li, M. Bendersky, M. Najork, J. Pfeifer, N. Golbandi, R. Anil, and S. Wolf, “Tf-ranking: Scalable tensorflow library for learning-to-rank,” in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 2970–2978. [32] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li, “Learning to rank: from pairwise approach to listwise approach,” in Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 129–136.
[33] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Int. Conf. on Learning Representations (ICLR), 2015.
[34] J. Mahler, F. T. Pokorny, B. Hou, M. Roderick, M. Laskey, M. Aubry, K. Kohlhoff, T. Kr ̈oger, J. Kuffner, and K. Goldberg, “Dex-net 1.0: A cloudbased network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards,” in Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 2016, pp. 1957–1964.
[35] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation for games, robotics and machine learning,” http://pybullet.org, 2016–2019. [36] R. Robotics, “Sawyer 2-finger gripper,” 2019. [Online]. Available: https: //www.rethinkrobotics.com/sawyer/ [37] F. E. GmbH, “Panda 2-finger gripper,” 2019. [Online]. Available: https: //www.franka.de/panda/ [38] R. Inc., “Robotiq 3-finger gripper,” 2019. [Online]. Available: https: //robotiq.com/products/3- finger- adaptive- robot- gripper [39] K. Inc., “Kinova kg-2 and kg-3 gripper,” 2019. [Online]. Available: https://www.kinovarobotics.com/en/products/accessories/grippers [40] B. Technology, “Barrett bh8-282 3-fingered gripper,” 2019. [Online]. Available: https://www.barrett.com/about- barretthand [41] F. T. Pokorny and D. Kragic, “Classical grasp quality evaluation: New theory and algorithms,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Tokyo, Japan, 2013. [42] M. A. Roa and R. Su ́arez, “Computation of independent contact regions for grasping 3-d objects,” IEEE Transactions on Robotics, vol. 25, no. 4, pp. 839850, 2009. [43] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M. Dollar, “Benchmarking in manipulation research: Using the yale-cmu-berkeley object and model set,” IEEE Robotics Automation Magazine, vol. 22, no. 3, pp. 36–52, Sep. 2015. [44] A. Telea, “An image inpainting technique based on the fast marching method,” Journal of graphics tools, vol. 9, no. 1, pp. 23–34, 2004. [45] M. L. Felis, “Rbdl: an efficient rigid-body dynamics library using recursive algorithms,” Autonomous Robots, pp. 1–17, 2016. [46] Schunk, “Schunk svh 5-finger,” 2019. [Online]. Available: https://schunk.com/