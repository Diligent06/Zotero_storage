IRASim: Learning Interactive Real-Robot Action Simulators
Fangqi Zhu1,2, Hongtao Wu1†∗, Song Guo2∗, Yuxiao Liu1, Chilam Cheang1, Tao Kong1 1ByteDance Research, 2Hong Kong University of Science and Technology zhufangqi00@gmail.com, wuhongtao.123@bytedance.com, songguo@cse.ust.hk Gen-IRASim.github.io
Abstract
Scalable robot learning in the real world is limited by the cost and safety issues of real robots. In addition, rolling out robot trajectories in the real world can be time-consuming and labor-intensive. In this paper, we propose to learn an interactive real-robot action simulator as an alternative. We introduce a novel method, IRASim, which leverages the power of generative models to generate extremely realistic videos of a robot arm that executes a given action trajectory, starting from an initial given frame. To validate the effectiveness of our method, we create a new benchmark, IRASim Benchmark, based on three real-robot datasets and perform extensive experiments on the benchmark. Results show that IRASim outperforms all the baseline methods and is more preferable in human evaluations. We hope that IRASim can serve as an effective and scalable approach to enhance robot learning in the real world. To promote research for generative real-robot action simulators, we open-source code, benchmark, and checkpoints at https: //gen-irasim.github.io.
1 Introduction
The field of embodied AI has witnessed remarkable progress in recent years. Real robots are now able to complete a wide variety of tasks [1, 2, 3, 4, 5]. However, real robots are costly, unsafe, and require regular maintenance which may restrict scalable learning in the real world. And rolling out robot trajectories in the real world can be time-consuming and labor-intensive, although it is necessary for model evaluation and reinforcement learning. While efforts have been made to create powerful physical simulators [6, 7, 8, 9], they are still not visually realistic enough. Also, they are not scalable for the reason that it takes effort to build new environments in simulation. What if we can create an interactive real-robot action simulator that can simulate robot trajectories in a way that is both accurate and visually indistinguishable from the real world? With such a simulator, agents can interactively control virtual robots to interact with diverse objects in various scenes in the simulator. It enables robots to improve policies by learning from simulated experiences without safety concerns and maintenance efforts. And the improved policy can consequently produce a large amount of simulated but realistic "real-robot" trajectories for training. Furthermore, the simulator can be leveraged as a dynamics model for imagining outcomes of different proposed candidate actions for model-based reinforcement learning.
Recent advances in generative models showcase extraordinary performance in generating realistic texts [10], images [11], and videos [12]. Inspired by these successes, we take the first step to leverage generative models in building a real-robot action simulator. To this end, we propose IRASim, a novel method that generates extremely realistic videos of a robot executing an action trajectory,
*Corresponding authors. †Project lead.
Preprint. Under review.
arXiv:2406.14540v1 [cs.RO] 20 Jun 2024


Initial Frame Input Trajectory
IRASim
Prediction / Ground-truth
Figure 1: Overview of IRASim. IRASim is an interactive real-robot action simulator that allows agents to input an action trajectory to control the "real robot" in an initial frame.
starting from a given initial frame (Fig. 1). We refer to this task as the trajectory-to-video task. And in this paper, we focus on predicting videos of robot arms completing manipulation tasks. The trajectory-to-video task differs from the general text-to-video task. While various videos can meet the text condition in the text-to-video task, the predicted video in our trajectory-to-video task must strictly and accurately follow the input trajectory. More importantly, a challenge of this task is that each action in the trajectory provides an exact description of the robot’s movement in each frame. This contrasts with the text-to-video task, where textual descriptions offer a general condition without specific frame-by-frame details. Another challenge is that the trajectory-to-video task features rich robot-object interactions, which must adhere to physical laws. For instance, when the robot picks up a bowl and moves, the bowl should move together with the robot. In terms of data, training a trajectory-to-video model only requires trajectory-video pairs, which is very scalable – even failure trajectories can be used for training.
To tackle the trajectory-to-video task, IRASim leverages an innovative frame-level condition method to achieve precise frame-by-frame alignment between actions and video frames. We use the powerful Diffusion Transformer [13] as the backbone of IRASim to improve the modeling of robot-object interactions. IRASim can generate realistic videos of high-resolution (up to 288 × 512). To generate long-horizon videos, IRASim can be rolled out in an autoregressive manner and maintain consistency between the generated video clips.
To validate the effectiveness of the proposed method, we develop a new benchmark, IRASim Benchmark, based on three real-robot manipulation datasets: RT-1 [1], Bridge [14], and LanguageTable [4]. We perform extensive experiments on this benchmark. Results show that IRASim outperforms comparing baseline methods and is more preferable than other methods in human evaluation on all three datasets. To showcase the interactive capability of our method, we simulate teleoperating the robots in RT-1 [1] and Bridge [14] to move in 3D space and Language-Table [4] in 2D spaces with a vive controller and keyboard, respectively. Unlike previous work that uses texts as the interface between users and the simulator [15], IRASim adopts trajectories as a more robot-centric interface for building simulators. To the best of our knowledge, it is the first work that models the visual dynamics of complicated 7-Dof actions in the real world. We hope this work can serve as a starting point for developing generative realistic real-robot action simulators. To summarize, the contribution of this paper is threefold:
• We propose a novel method, i.e. IRASim, capable of generating high-resolution and long-horizon videos for the trajectory-to-video task. It achieves precise alignments between actions and video frames and accurately adheres to physical laws.
2


• We introduce IRASim Benchmark, a new benchmark based on three real-robot datasets for the trajectory-to-video task. We aim to drive progress on generative real-robot action simulators.
• We perform extensive experiments on the proposed benchmark to show the performance of IRASim. Results demonstrate that our method is able to produce accurate videos that are almost visually indistinguishable from the real world.
2 Related Work
World Models. Learning a world model (or dynamics model) [16, 17] which predicts future observations based on the current observation and actions has been increasingly popular recently [18, 19, 20, 21, 22, 23, 15, 24, 25, 26, 27]. VLP [25] exploits text-to-video models as dynamics models to generate video plans for robots. DreamerV3 [20] and DayDreamer [21] leverage recurrent state space models (RSSMs) [28] to learn a latent representation of states through modeling a world model for reinforcement learning. GAIA-1 [22] uses a generative model that leverages videos, texts, and actions to generate photorealistic driving scenes. Recently, Genie [24] learns an interactive environment from unlabelled internet videos which allows users to act in the environments via latent actions. Probably, the most relevant work is UniSim [15] which builds a universal simulator for modeling real-world interaction. It generates videos from texts and demonstrates controlling a robot arm to move in 2D space with language instructions. Our method can be considered as a world model for robot manipulation. It takes as inputs real-robot physical actions instead of text [15] or latent actions [24]. Besides 2D actions in previous work [18, 19, 15], IRASim also deals with complex 7-Dof robot actions in 3D space which involves translation, rotation, and gripper actions.
Video Models. Video models generate video frames either unconditionally or with conditions including classes, initial frames, texts, strokes, and/or actions [18, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 22, 40, 15, 12, 41, 24, 42]. VideoGPT [30] leverages a simple GPT-like architecture to autoregressively generate VQ-VAE [43] encoded tokens for video generation. Recently, diffusion models [44, 45] are becoming more and more popular in video generation [31, 32, 33, 39, 38, 15, 12]. A popular choice of architecture is U-Net [31, 39] which has also been widely used in image diffusion models [11]. Sora [12] showcases extraordinary video generation capability with Diffusion Transformers [13]. Our method also leverages Diffusion Transformers as the backbone [41, 13]. A relevant line of work is to control video synthesis with motions. These methods use either userspecified strokes [46, 47], bounding boxes [40], or human poses [48, 36] as conditions. Our method differs from previous work in that we seek to model complex 3D real-world actions in the video to learn a real-robot action simulator.
Scaling Real-World Robot Learning. Rolling out policies in the real world is essential in scaling up robot learning. Firstly, it is necessary for model evaluation [1, 2, 3, 4, 5]. Secondly, as real-robot data are scarce for the reason that data collection often requires costly human demonstrations, an alternative is to roll out a policy to collect data (e.g. dataset augmentation (DAgger) [49, 5, 50]). Finally, real-robot reinforcement learning requires rolling out robots in the real world to collect trajectories [51, 52, 53]. However, policy rollout in the real world is time-consuming. And human supervision is often needed to ensure safety which can be labor-intensive. Moreover, scaling up real-world evaluation would necessitate building and maintaining a large number of robots. To tackle this challenge, recent work [54] shows a correlation between evaluation in a physical simulator and on real robots. Our method aims to build a real-robot action simulator as an efficient and scalable alternative for real-world policy rollout.
3 Methods
3.1 Problem Statement
We define the trajectory-to-video generation task as predicting the video of a robot that executes a trajectory given the initial frame I1 and the action trajectory a1:N−1:
I2:N = f (I1, a1:N−1) (1)
where N denotes the number of frames in the video; ai denotes the action at the i-th timestep. In this paper, we focus on predicting videos for robot arms. A typical action space for robot arms contains 7
3


degrees of freedom (DoFs), i.e. 3 DoFs for describing translation in the 3D space, 3 DoFs for 3D rotation, and 1 DoF for the gripper action.
3.2 Preliminaries
Diffusion Models. Before delving into our method, we briefly review preliminaries of diffusion models [44, 45]. Diffusion models [44] typically consist of a forward process and a reverse process. The forward process gradually adds Gaussian noises to data x0 over T timesteps. It can be formulated
as q (xt|x0) = N xt; √αtx0, 1 − αtI , where xt is the diffused data at the t-th diffusion timestep and αt is a constant defined by a variance schedule. The reverse process starts from xT ∼ N (0, I) and gradually remove noises to recover x0. It can be mathematically expressed as pθ(xt−1|xt) = N (xt−1; μθ(xt, t), Σθ(xt, t)), where μθ(·) and Σθ(·) denote the mean and covariance functions, respectively, and can be parameterized via a neural network.
In the training phase, we sample a timestep t ∈ [1, T ] and obtain xt = √αtx0 + √1 − αtεt via the reparameterization trick [45] where εt ∈ N (0, I). We leverage the simplified training objective to train a noise prediction model εθ as in DDPM [45]:
Lsimple(θ) = ||εθ(xt, t) − εt||2 (2)
In the inference phase, we generate x0 by first sampling xT from N (0, I) and iteratively compute
xt−1 = xt − √1 − αtεθ(xt, t)
√αt
(3)
until t = 0. For conditional diffusion processes, the noise prediction model εθ can be parameterized as εθ(xt, t, c) where c is the condition that controls the generation process. Throughout the paper, we use superscript and subscript to indicate the timestep of a frame in the input video and the diffusion timestep, respectively.
Latent Diffusion Models. Directly diffusing the entire video in the pixel space is time-consuming and requires substantial computation to generate long videos with high resolutions [31]. Inspired by [11, 41], we perform the diffusion process in a low-dimension latent space z instead of the pixel space for computation efficiency. Following [39], we leverage the pre-trained variational autoencoder (VAE) in SDXL [55] to compress each frame Ii in the video to a latent representation with the VAE encoder zi = Enc(Ii) where i ∈ {1, 2, ..., N }. The latent representation can be decoded back to the pixel space with the VAE decoder Ii = Dec(zi).
3.3 IRASim
IRASim is a conditional diffusion model operating in the latent space of the VAE introduced in Sec. 3.2. The condition c includes the latent representation of the initial frame of a video z1 = Enc(I1) and an action trajectory a1:N−1. The diffusion target is the latent representations of the subsequent N − 1 frames of the video in which the robot executes the action trajectory, i.e. x = z2:N . Inspired by Sora’s remarkable capability of understanding the physical world [12], we similarly adopt Diffusion Transformers (DiT) [13] as the backbone of IRASim. In the design of IRASim, we aim to address three key aspects: 1) consistency with the initial frame 2) adherence to the given action trajectory and 3) computation efficiency. In the following, we describe details of IRASim and discuss pivotal design choices to achieve the aforementioned objectives.
Tokenization. Each latent representation zi = Enc(Ii) contains P tokens of D dimensions, where P denotes the number of patches per frame. By sequencing the latent representations of all frames by timestep order, the video is tokenized to N × P tokens. Spatial and temporal positional embeddings are added to the tokens to allow awareness of patch positions within frames and timesteps in the video, respectively. The VAE is frozen throughout the training process.
Spatial-Temporal Attention Blocks. Standard transformer blocks apply Multi-Head Self-Attention (MHA) to all tokens in the input token sequence, resulting in quadratic computation cost. We thus leverage the memory-efficient spatial-temporal attention mechanism [56, 24, 41] in the transformer block of IRASim to reduce the computation cost (Fig. 2). Specifically, each block consists of a spatial attention block and a temporal attention block. In the spatial attention block, MHA is confined to
4


(a) IRASim (b) Frame-level Adaptation
Input Tokens cS1 c#$
Scale
FFN
Scale & Shift
Scale
MHA
Scale & Shift
α%%, ... , α%&
γ%%, ... , γ%$
β%%, ... , β%$
γ'%, ... , γ'$
β'%, ... , β'$
α'%, ... , α'$
(N, P, D)
(P, N, D)
...
Scale
FFN
Scale & Shift
Scale
MHA
Scale & Shift
α(
γ(, β(
γ), β)
α)
c* Input Tokens
Scale
FFN
Scale & Shift
Scale
MHA
Scale & Shift
(N, P, D)
(P, N, D)
Scale
FFN
Scale & Shift
Scale
MHA
Scale & Shift
c+,
α(
γ(, β(
γ), β)
α)
α%
γ%, β%
γ', β'
α'
Diffusion Timestep
t
Lx
Encode, Patchify
Spatial Attention Block
Temporal Attention Block
Noise Prediction ε̂
Noisy Video Frames
First Video Frame
Action Trajectory
a!:#$!
Embed
MLP
Linear, Unpatchify
(c) Video-level Adaptation
Figure 2: Network Architecture of IRASim. (a) shows the general diffusion transformer architecture of IRASim. The input to IRASim includes the initial frame and the entire trajectory. (b) Frame-level adaptation (Frame-Ada). (c) Video-level adaptation (Video-Ada).
tokens within a frame to model intra-frame interaction. In the temporal attention block, MHA is confined to tokens at an identical patch position across all the frames to model inter-frame interaction. For a sequence of N × P tokens, spatial attention operates on the 1 × P tokens within each frame; temporal attention operates on the N × 1 tokens across the N timesteps. Compared to attending over all the N × P tokens at a time, the spatial-temporal attention greatly decreases the computation cost which makes generating long and high-resolution videos feasible.
Initial Frame Condition. The initial frame condition is achieved by treating the initial frame as the ground-truth portion in the input video sequence [12]. That is, during training, we only add noise to the tokens corresponding to the 2nd to the N-th frames z2:N , while keeping those of the initial frame z1 intact as it does not need to be predicted (Fig. 2). And the diffusion loss is only computed upon the 2nd to the N-th frames. This condition approach ensures consistency with the initial frame by enabling the predicted frames to interact with it via attention mechanism.
Trajectory Condition. A naive approach to impose the trajectory condition is to encode the trajectory as one embedding and append it to the input token sequence as an in-context condition [13]. However, considering Diffusion Transformers [13] demonstrate that adaptive normalization performs better than in-context condition, we adopt this design in IRASim to achieve trajectory condition.
• Video-Level Condition. Similar to using a text embedding to condition the generation of the entire video in the text-to-video task, we use a linear layer to encode the entire trajectory into a single embedding for condition. The embedding is then added to the embedding of the diffusion timestep t for generating the scale parameters γ and α and the shift parameters β for each spatial and temporal attention block. These parameters control the video generation via shifting the distribution of the token embeddings in the transformer block. The overall framework is shown in Figure 2(c). See Appendix C.1 for more details.
• Frame-Level Condition. Unlike the text-to-video task where the text describes the entire video, the trajectory in the trajectory-to-video task is a finer description. Each action in the trajectory defines how the robot should move in each frame. And thus, each generated frame must match with its corresponding action in the trajectory. To achieve this precise frame-level alignment, we condition the generation of each frame by its corresponding action. Instead of encoding the entire action trajectory into a single embedding, we use a linear layer to encode each action into an individual embedding. The diffusion timestep embedding is added to each action embedding to
5


generate the scale and shift parameters for each individual frame in the spatial block. The scale and shift parameters of the temporal block for all frames share the same conditioning embedding which is derived similarly as in video-level condition. See Appendix C.2 for more details.
Output. The output layer contains a linear layer which outputs the noise prediction εˆ = εθ(xt, t, c) εˆ is used to compute the L2 loss with the ground-truth noise during training (Eq. 2). Note that IRASim only predicts the mean of the noise but not the covariance as in [13] – we empirically found that this improves video generation quality. During inference, we sample xT from N (0, I) and gradually denoise it via Eq. 3 to obtain the predicted latent representation of the 2nd to the N-th frames ˆz2:N = x0. The predicted video frames can be decoded with the VAE decoder Iˆ2:N = Dec(ˆz2:N ).
4 Experiments
In this section, we perform extensive experiments on IRASim Benchmark which includes three challenging real-robot datasets to verify the performance of IRASim. We aim to answer three questions: 1) Is IRASim effective on solving the trajectory-to-video task on various datasets with different action spaces? 2) How do different components contribute to the final performance of IRASim? 3) Can we leverage IRASim as a robot action simulator to allow humans to control robots in an image? More details and results can be found in the Appendix.
4.1 IRASim Benchmark
IRASim Benchmark is designed for the trajectory-to-video generation task. We source data from three publicly available robot manipulation datasets: RT-1 [1], Bridge [14], and Language-Table [4]. We split each dataset into training, validation, and test sets. The dataset statistics are shown in Table 3 in Appendix B. In RT-1 and Bridge, a robot arm with a gripper moves in the 3D space to perform manipulation which interacts with objects in the scene. The action spaces of RT-1 and Bridge consist of 1) 6-Dof arm action in the 3D space T ∈ SE(3) and 2) a continuous gripper action g ∈ [0, 1]. In Language-Table, a robot arm moves in a 2D plane to move blocks with a cylindrical end-effector. The action space is 2-Dof translation in the 2D space p ∈ R2. See Fig. 1 and 3 for example images of these datasets. We convert the arm action of all datasets to relative delta actions.
4.2 Experiment Setup
We perform experiments on IRASim Benchmark. During training, we sample video clips containing 16 continuous frames from episodes using a sliding window. We resize videos, and the resolutions after resizing for RT-1, Bridge, and Language-Table are 256×320, 256×320, and 288×512, respectively. We perform experiments on video generation on short trajectories and long trajectories. Short trajectories, which are segments of complete episodes, consist of 16 frames and 15 actions. The video can be generated in one diffusion generation process. For long trajectories, we utilize complete episodes from the dataset. Long videos can be rolled out in an autoregressive manner. The initial frame of the first diffusion process is the given ground-truth frame, while the initial frame of each subsequent diffusion process is the last output frame from the previous process.
IRASim Variants. We follow standard transformers which scale the hidden size, number of heads, and number of layers together. In particular, we perform experiments on four configurations: IRASim-S, IRASim-B, IRASim-L, and IRASim-XL. Details of these models are shown in Tab. 7 in Appendix E. If not specified otherwise, throughout the paper, we report the results of IRASimXL which contains 679M trainable parameters in total. We denote IRASim with frame-level and video-level adaptation as IRASim-Frame-Ada and IRASim-Video-Ada, respectively.
Baselines. To evaluate the effectiveness of IRASim, we compare with two state-of-the-art baseline methods, i.e. VDM [31] and LVDM [39]. Both methods are diffusion models based on a U-Net architecture. LVDM diffuses videos in a latent space while VDM operates in the pixel space. These methods demonstrate strong capabilities on the text-to-video task. To impose trajectory conditions on video generation, we encode the trajectory into an embedding to condition the diffusion process in both methods. This is similar to the text embedding used for text-to-video generation in the original papers [31, 39]. LVDM is configured such that its number of parameters is similar to IRASim. As
6


Short Trajectories Long Trajectories
(b)
(a)
Figure 3: Qualitative Results. We show video generation of IRASim with (a) short trajectories and (b) long trajectories on the test set of RT-1, Bridge, and Language-Table. Ground-truths are in blue boxes. Predictions are in orange boxes. Initial ground-truth video frames are in green boxes.
VDM performs diffusions in the pixel space, it requires more computation resources than LVDM and IRASim despite having a smaller number of 44M parameters. More details about the implementation of baselines can be found in Appendix D.
Metrics. Following [36], we evaluate with two types of metrics: computation-based and modelbased. Computation-based metrics includes PSNR [57] and SSIM [58]. Model-based metrics includes includes Latent L2 loss, FID [59] and FVD [60]. Latent L2 loss and PSNR measure the L2 distance between the predicted video and the ground-truth video in the latent space and pixel space, respectively. SSIM evaluates the similarity between videos in terms of image brightness, contrast, and structure. FID and FVD assess video quality by analyzing the similarity of video feature distributions. Unlike the text-to-video task where a variety of videos may meet with a single text condition, the variety is much smaller in the trajectory-to-video task as the robot in the predicted video must strictly
7


Table 1: Quantitative Results on Video Generation of Short Trajectories. We prioritize Latent L2 loss and PSNR as primary evaluation metrics.
Dataset Method Computation-based Model-based
PSNR ↑ SSIM ↑ Latent L2 ↓ FID ↓ FVD ↓
RT-1
VDM [31] 13.762 0.554 0.4983 41.23 371.13 LVDM [39] 25.041 0.815 0.2244 4.26 30.72 IRASim-Video-Ada 25.446 0.823 0.2191 4.34 29.27 IRASim-Frame-Ada 26.048 0.833 0.2099 5.60 25.58
Bridge
VDM [31] 18.520 0.741 0.3709 39.82 127.25 LVDM [39] 23.546 0.810 0.2155 10.59 35.06 IRASim-Video-Ada 24.733 0.827 0.2021 10.30 23.03 IRASim-Frame-Ada 25.275 0.833 0.1947 10.51 20.91
Language-Table
VDM [31] 23.067 0.857 0.3204 64.63 136.56 LVDM [39] 28.254 0.889 0.1704 6.85 24.34 IRASim-Video-Ada 23.893 0.859 0.2028 7.05 73.84 IRASim-Frame-Ada 28.818 0.888 0.1660 6.38 48.49
0% 20% 40% 60% 80% 100% % win rate
IRASim-Frame-Ada
VvDsM IRASim-Frame-Ada
LVvDs M
IRASim-Frame-Ada
vs
IRASim-Video-Ada
RT-1
100%
60% 12% 28%
38% 32% 30%
0% 20% 40% 60% 80% 100% % win rate
Bridge
100%
72% 16% 12%
36% 30% 34%
0% 20% 40% 60% 80% 100% % win rate
Language-Table
100%
68% 10% 22%
66% 20% 14%
Win Tie Loss
Figure 4: Human Preference Evaluation. We perform a user study to evaluate the human preference between IRASim-Frame-Ada and other baseline methods.
follows the input trajectory. Thus, we prioritize Latent L2 loss and PSNR as primary evaluation metrics. In Sec. 4.3, we will later show that Latent L2 loss and PSNR match with human preference the most among all the metrics.
4.3 Results
Video Generation of Short Trajectories. Qualitative results are shown in Fig. 3(a) and Fig. 7. Quantitative results are shown in Tab. 1. As shown in Fig. 3(a) and Fig. 7, IRASim-Frame-Ada can generate videos that are almost visually indistinguishable from the ground-truth. IRASim-Frame-Ada performs the best among all the comparing methods in terms of Latent L2 loss and PSNR. It achieves the highest SSIM on RT-1 and Bridge and is comparable with the best baseline method LVDM on Language-Table. In all three datasets, it outperforms IRASim-Video-Ada in all the computation-based metrics. This indicates that frame-level condition enhances consistency between each frame and its corresponding action in the trajectory, as shown in Fig. 7 in the Appendix A.1. IRASim-Frame-Ada also surpasses the two baseline methods based on U-Nets on all three datasets on Latent L2 loss. This demonstrates the superiority of transformer-based model, especially in handling complex 3D actions and robot-object interaction. VDM fails to generate realistic videos despite consuming more computation costs during training. This indicates the effectiveness of performing diffusion in latent space.
Human Preference Evaluation. We also perform a user study to help understand human preferences between IRASim-Frame-Ada and other methods. We juxtapose the videos predicted by IRASim-Frame-Ada and the comparing method and ask humans which one they prefer. The groundtruth is also provided as a reference. IRASim-Frame-Ada beats all the comparing methods in all
8


Table 2: Quantitative Results on Video Generation of Long Trajectories.
RT-1 Bridge Language-Table
Latent L2 ↓ PSNR ↑ Latent L2 ↓ PSNR ↑ Latent L2 ↓ PSNR ↑
LVDM [39] 0.2567 23.573 0.2534 21.792 0.1776 26.215 IRASim-Video-Ada 0.2519 23.984 0.2385 22.868 0.2112 22.551 IRASim-Frame-Ada 0.2408 24.615 0.2306 23.260 0.1730 26.773
150K
180K
210K
240K
270K
300K
Training Steps
0.20
0.22
0.24
0.26
0.28
Latent L2
RT-1
150K
180K
210K
240K
270K
300K
Training Steps
0.18
0.20
0.22
0.24
0.26
0.28
Bridge
150K
180K
210K
240K
270K
300K
Training Steps
0.20
0.22
0.24
0.26
0.28
Language-Table
IRASim-S (33M) IRASim-B (132M) IRASim-L (461M) IRASim-XL (679M)
Figure 5: Scaling. IRASim scales elegantly with the increase of model sizes and training steps.
three datasets (Fig. 4). This result aligns with the Latent L2 loss and PSNR which justifies the reason for using them as the primary evaluation metrics.
Video Generation of Long Trajectories. Qualitative results are shown in Fig. 3(b) and Fig. 8. Quantitative results are shown in Tab. 2. We compare IRASim with the best baseline method LVDM [39]. IRASim-Frame-Ada consistently outperforms the comparison methods in all three datasets on Latent L2 loss and PSNR. Fig. 3(b) and Fig. 8 show that it retains the powerful capability of generating visually realistic and accurate videos as in the short trajectory setting.
Scaling. We follow [13] and train IRASim-Frame-Ada of different model sizes ranging from 33M to 679M. Results are shown in Fig. 5. On all three datasets, IRASim scales elegantly with the increase of model sizes and training steps. This indicates strong potential for increasing model sizes and training steps to further improve the performance.
Application To showcase the application of IRASim, we perform experiments on controlling the "real robots" in the three datasets of IRASim Benchmark. For Language-Table [4] with a 2D translation action space, we use the arrow keys from the keyboard to input action trajectories for better accessibility. However, we note that IRASim can handle any 2D translational trajectories as input. For RT-1 [1] and Bridge [14] with a 3D action space, we use the Vive controller to record action trajectories as input. Fig. 6 shows that IRASim can be used as an interactive real-robot action simulator in various ways. In particular, IRASim is able to robustly handle multimodality in generation – Fig. 6(a) shows generating videos with an identical initial frame but different trajectories.
5 Limitations and Conclusion
In this paper, we present IRASim, a novel method that generates videos of a robot that executes an action trajectory given the initial frame. Results show that our method is able to generate long-horizon and high-resolution videos that are almost visually indistinguishable from ground-truth videos.
Similar to other generative methods, one limitation of our method is hallucination. Also, the inference speed of IRASim is not real-time, despite having a high throughput – it costs only 8GB memory during inference. Finally, IRASim currently does not support flexible input resolutions and action spaces, restricting its capability to fully utilize robot data of various resolutions and action spaces. In the future, we will investigate incorporating different robot data in training and accelerate the
9


Keyboard Inputs: ↑↑→→→↓↓↓←←←←←←←
Keyboard Inputs: ↑↑→→→↓↓↓↓↓↓←←←←
(a)
(b)
Figure 6: Application. We showcase controlling (a) the virtual robot in Language-Table with arrow keys on a keyboard and (b) the robots in RT-1 and Bridge with a vive controller. Predictions are in orange boxes. Initial Frames are in green boxed.
inference speed via methods including diffusion distillation [61, 62]. Also, we plan to explore using IRASim as a real-robot simulator for collecting simulated trajectories via policies to improve policies with methods including DAgger [49] and RL. We hope that IRASim can serve as a starting point for the development of generative realistic real-robot action simulators and an effective method to accelerate robot learning in the real world for the community.
References
[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023.
[2] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to robotic
10


control. In Proceedings of The 7th Conference on Robot Learning, volume 229, pages 21652183, 06–09 Nov 2023.
[3] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation, 2023.
[4] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023.
[5] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991–1002. PMLR, 2022.
[6] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning, 2021.
[7] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, et al. Orbit: A unified simulation framework for interactive robot learning environments. IEEE Robotics and Automation Letters, 2023.
[8] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11097–11107, 2020.
[9] Zoey Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, and Abhishek Gupta. Urdformer: A pipeline for constructing articulated simulation environments from real-world images. arXiv preprint arXiv:2405.11656, 2024.
[10] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[11] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.
[12] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024.
[13] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
[14] Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: A dataset for robot learning at scale. In Conference on Robot Learning, pages 1723–1736. PMLR, 2023.
[15] Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. In The Twelfth International Conference on Learning Representations, 2024.
[16] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1), 2022.
[17] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
[18] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. Advances in neural information processing systems, 29, 2016.
11


[19] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786–2793. IEEE, 2017.
[20] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.
[21] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on Robot Learning, pages 2226–2240. PMLR, 2023.
[22] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.
[23] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024.
[24] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. arXiv preprint arXiv:2402.15391, 2024.
[25] Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan Wahid, brian ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy Zeng, and Jonathan Tompson. Video language planning. In The Twelfth International Conference on Learning Representations, 2024.
[26] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models, 2024.
[27] Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. In The Twelfth International Conference on Learning Representations, 2024.
[28] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 2555–2565. PMLR, 2019.
[29] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019.
[30] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.
[31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633–8646, 2022.
[32] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.
[33] Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022.
[34] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.
[35] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563–22575, 2023.
12


[36] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model, 2023.
[37] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10459–10469, 2023.
[38] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[39] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023.
[40] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024.
[41] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024.
[42] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models, 2024.
[43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.
[44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256–2265. PMLR, 2015.
[45] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.
[46] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory, 2023.
[47] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023.
[48] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring human dance generation in real world. arXiv preprint arXiv:2307.00040, 2023.
[49] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011.
[50] Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Hgdagger: Interactive imitation learning with human experts. In 2019 International Conference on Robotics and Automation (ICRA), pages 8077–8083. IEEE, 2019.
[51] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1–40, 2016.
13


[52] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on robot learning, pages 651–673. PMLR, 2018.
[53] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.
[54] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024.
[55] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.
[56] Mingxing Xu, Wenrui Dai, Chunmiao Liu, Xing Gao, Weiyao Lin, Guo-Jun Qi, and Hongkai Xiong. Spatial-temporal transformer networks for traffic flow forecasting. arXiv preprint arXiv:2001.02908, 2020.
[57] Alain Horé and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, pages 2366–2369, 2010.
[58] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, 2004.
[59] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6629–6640, Red Hook, NY, USA, 2017. Curran Associates Inc.
[60] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges, 2019.
[61] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14297–14306, 2023.
[62] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024.
[63] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
[64] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2022.
14


A Additional Qualitative Results
In this section, we present additional qualitative results on comparing IRASim with baseline methods.
A.1 Video Generation of Short Trajectories
Results are illustrated in Fig. 7. These results demonstrate that IRASim-Frame-Ada surpasses other methods in aligning frames with actions and modeling the interaction between robots and objects.
A.2 Video Generation of Long Trajectories
Results are illustrated in Fig. 8. IRASim-Frame-Ada generates consistent and long-horizon videos, accurately simulating the entire trajectory. Additionally, IRASim-Frame-Ada maintains its superior performance in frame-action alignment and robot-object interaction as observed in the short-trajectory setting.
A.3 Scaling
Results are shown in Fig. 9. IRASim-Frame-Ada consistently improves the quality of the generated video in terms of reality and accuracy with the increase of model size.
15


Ground-truth
(c)
(b)
(a) VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
Ground-truth
VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
Ground-truth
Figure 7: Additional Results on Video Generation of Short Trajectories. We compare the results of different methods on (a) RT-1, (b) Bridge, and (c) Language-Table. Differences between IRASim-Frame-Ada and other methods are highlighted in green and red boxes.
16


Ground-truth
(c)
(b)
(a) VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
Ground-truth
VDM
LVDM
IRASim-Video-Ada
IRASim-Frame-Ada
Ground-truth
Figure 8: Additional Results on Video Generation of Long Trajectories. We compare the results of different methods on (a) RT-1, (b) Bridge, and (c) Language-Table. Differences between IRASim-Frame-Ada and other methods are highlighted in green and red boxes. Note that the input trajectory is the entire trajectory of an episode.
17


Ground-truth
(c)
(b)
(a) IRASim-S
IRASim-B
IRASim-L
IRASim-XL
IRASim-S
IRASim-B
IRASim-L
IRASim-XL
Ground-truth
Ground-truth
IRASim-S
IRASim-B
IRASim-L
IRASim-XL
Figure 9: Additional Results on Scaling. We compare the results of IRASim-Frame-Ada with different model sizes on (a) RT-1, (b) Bridge, and (c) Language-Table.
18


B Datasets
Table 3: Dataset Statistics. An "episode" is a single trial where the robot completes a task. A "sample" is a clip from an episode.
Datasets RT1 [1] Bridge [14] Language2Table [4]
Data Split Episode Sample Episode Sample Episode Sample
Train 82,069 2,314,893 25,460 482,701 170256 1483133 Validation 2,167 4,810 1,737 2,905 4,446 5,119 Test 2,167 4,799 1,738 2,946 4,562 5,243
We provide details on the three publicly available robot manipulation datasets used in IRASim Benchmark: RT1 [1], Bridge [14], and Language-Table [4]. A summary of the dataset statistics is presented in Table 3. Each training sample consists of a 4-second video clip containing 16 frames, extracted from an episode with a continuous sliding window. For testing and validation, frames are sampled at 16-frame intervals to reduce the number of evaluation videos and, consequently, lower evaluation costs. The original resolution for RT-1 is 256 × 320, for Bridge it is 480 × 640, and for Language-Table it is 360 × 640. To ensure efficient training, we resize the Bridge videos to a resolution of 256 × 320 and the Language-Table videos to 288 × 512.
C IRASim Model Details
In this section, we introduce more details about two types of trajectory condition methods in Sec. 3.3: Video-Level Condition and Frame-Level Condition.
C.1 Video-Level Conditioning
In video-level condition (Fig. 2(c)), we first obtain the conditioning embedding cST by adding the diffusion timestep embedding to the trajectory embedding. We then use cST to regress the scale parameters γ and α, as well as the shift parameters β. Specifically, the computation of the spatial block is as follows: x = x + (1 + α1) × MHA(γ1 × LayerNorm(x) + β1) (4)
x = x + (1 + α2) × FFN(γ2 × LayerNorm(x) + β2) (5)
where x, with a shape of (N, P, D), denotes the token embeddings. x is reshaped as (P, N, D) before entering the temporal block. The computation of the temporal block is: x = x + (1 + α3) × MHA(γ3 × LayerNorm(x) + β3) (6)
x = x + (1 + α4) × FFN(γ4 × LayerNorm(x) + β4) (7) Note that layer normalization is performed before scaling and shifting.
C.2 Frame-Level Condition
In frame-level condition (Fig 2(b)), spatial attention blocks and temporal attention blocks are conditioned differently. The derivation of the conditioning embedding for temporal attention blocks cT is the same as in video-level condition, where we add the diffusion timestep embedding to the trajectory embedding. Different frames are conditioned differently in spatial attention blocks. We denote the conditioning embedding of spatial attention blocks for the i-th frame as ci
S. To derive ci
S, the i-th action in the trajectory is first encoded to an embedding through a linear layer. The diffusion timestep embedding is then added to the encoded embedding to obtain ci
S. We use c1
S, . . . , cN
S and
cT to regress the corresponding scale parameters γ and α, as well as the shift parameters β. While the computation of the temporal blocks is the same as the video-level condition (Eq. 6 and 7), the computation of spatial blocks is different: xi = xi + (1 + αi
1) × MHA(γi
1 × LayerNorm(xi + βi
1)), (8)
xi = xi + (1 + αi
2) × FFN(γi
2 × LayerNorm(xi + βi
2)). (9)
where αi1, γi1, βi1, αi2, γi2, βi2 denote the scale and shift parameters for the i-th frame. They are
regressed from ci
S.
19


D Baselines Details
In this section, we detail the baseline implementation. For VDM [31], we leverage the implementation provided in 1, which utilizes a 3D U-Net architecture for controllable video generation. We use only the model component from this code and keep the training setting consistent with IRASim. LVDM [39] employs the same model architecture as VDM. It performs diffusion in the latent space while VDM performs diffusion in the pixel space. We use an MLP to encode the trajectory into an embedding. It is then concatenated with the embedding of the diffusion timestep to form the conditioning embedding. This is similar to the original methods in the paper where the text embedding is concatenated with the diffusion timestep embedding to form the conditioning embedding. The initial frame condition method of VDM and LVDM is the same as IRASim as described in Sec. 3.3. LVDM and IRASim share the same VAE model and training setting. Given that the resolution of Language-Table [4] is up to 288 × 512, we resize the video to 144 × 256 in the training of VDM to make the computational cost affordable. During evaluation, we resize the generated video back to 288 × 512 for comparison with other methods. For RT-1 and Bridge, the training of VDM is performed at a resolution of 256 × 320. The training hyperparameters for VDM and LVDM are shown in Tab. 4 and 5. More training hyperparameters that share with IRASim can be found in Tab. 6.
Table 4: Hyperparameters for VDM
Hyperparameter Value
Base channels 64 Channel multipliers 1,2,4,8 Num attention heads 8 Attention head dimension 32 Conditioning embedding dimension 768 Input channels 3 Parameters 40M
Table 5: Hyperparameters for LVDM
Hyperparameter Value
Base channels 288 Channel multipliers 1,2,4,8 Num attention heads 8 Attention head dimension 32 Conditioning embedding dimension 768 Input channels 3 Parameters 687M
E Training Details
For all models, we use AdamW [63] for training. We use a constant learning rate of 1e-4 and train for 300k steps with a batch size of 64. The gradient clipping is set to 0.1. We found the training of IRASim very stable – no loss spikes were observed even without gradient clipping. However, loss spikes often occur in LVDM and VDM when gradient clipping is not used. Following [13], we utilize the Exponential Moving Average (EMA) technique with a decay of 0.9999. All other hyperparameters are set the same as [13]. Tab. 6 lists further hyperparameters. All models are trained from scratch. We utilize PNDM [64] with 50 sampling steps for efficient video generation during evaluation. IRASim generates a 16-frame video with a duration of approximately 4 seconds, requiring only 30 seconds on an A100 GPU using 8GB of memory. Although there is still significant room for latency improvement, our method features high throughput and is memory-friendly during inference.
For scaling results in Fig. 5, the configurations of four different sizes of IRASim are shown in Tab. 7. We study the scale performance of IRASim-Frame-Ada since it performs best.
The information about computing resources for training our IRASim is provided in Tab. 8.
F Evaluation Details
We evaluate the video quality generated by IRASim and the baselines under two settings: short trajectories and long trajectories. In the short trajectory setting, the input consists of one initial frame and a short trajectory containing 15 actions, resulting in the generation of 15 subsequent frames. These short trajectories are sampled from episodes using a sliding window with an interval of 16. In the long trajectory setting, the input comprises one initial frame and a complete long trajectory, with the output being the generated subsequent frames. The average lengths of the long trajectories are 42.5, 33.4, and 23.7 frames for RT-1, Bridge, and Language-Table, respectively. These lengths
1https://github.com/lucidrains/video-diffusion-pytorch
20


also represent the average number of frames for the generated long videos, which are produced in an autoregressive manner, as detailed in Sec. 4.2. The statistics of the generated short and long videos used for evaluation are presented in Tab. 3.
In all metric calculations, we ignore the initial frame and only evaluate the quality of the generated frames. For PSNR and SSIM, we refer to skimage 2 for calculation. For FID and FVD, we refer to 3 and 4 for calculation, splitting the generated videos into frames and using their codebases to compute the FID and FVD values. However, we do not calculate FID and FVD metrics for long videos because we find that these metrics do not reflect human preferences well, even in the short trajectory setting. This could be because FID and FVD essentially calculate the similarity between the distributions of two datasets, whereas the trajectory2video task is a reconstruction task, making reconstruction loss a more suitable evaluation metric.
Table 6: Hyperparameters for training IRASim
Hyperparameter Value
Layers 28 Hidden size 1152 Num attention heads 16 Patch size 2 Input channels 4 Dropout 0.1 Optimizer AdamW(β = 0.9, β = 0.999) Learning rate 0.0001 Batch size 64 Gradient clip 0.1 Training steps 3000000 EMA 0.9999 Weight decay 0.0 Prediction target ε Parameters 679M
Table 7: Model Sizes. We use IRASim as an abbreviation of IRASim-Frame-Ada for brevity.
Model Layers Hidden size Num attention heads Parameters
IRASim-S 12 384 6 33M IRASim-B 12 768 12 132M IRASim-L 24 1024 16 461M IRASim-XL 28 1152 16 679M
Table 8: Compution resources for training IRASim
Dataset Concurrent GPUs GPU Hours GPU type
RT-1 32 2381 A800 (40 GB) Bridge 32 2371 A800 (40 GB) Lanaguge-Table 32 2369 A100 (80 GB)
G Human Preference Evaluation
Five participants took part in the human evaluation. For each participant, we randomly sampled 10 ground-truth video clips from the test set for each of the 3 datasets. And for each video clip, we
2https://scikit-image.org/docs/stable/api/skimage.metrics.html 3https://github.com/mseitzer/pytorch-fid 4https://github.com/universome/stylegan-v
21


juxtapose the predictions of IRASim-Frame-Ada with those of VDM, LVDM, and IRASim-VideoAda (Fig. 10). Thus, a participant evaluated 90 pairs of video clips. Note that the orders of the juxtaposition are random for different clips. See the caption of Fig. 10 for more details. We compare the results of all evaluated video clips and calculate the win, tie, and loss rates. The screenshot of the GUI used in the human evaluation is shown in Fig. 10. The full text of the instruction given to participants is as follows:
Evaluation Instructions
You are asked to choose the more realistic and accurate video from two generated videos (shown above). The ground-truth video is given as a reference (shown below). Please carefully examine the given videos. If you can find a significant difference between the two generated videos, you may choose which one is better immediately. If not, please replay the videos more times. If you are still not able to find differences, you may choose the "similar" option. Please do not guess. Your decision needs solid evidence.
Figure 10: Screenshot of the GUI in Human Preference Evaluation. The two videos in the upper row are generated by IRASim-Frame-Ada and a comparing method, arranged in a random left-right order. The video in the lower row is the ground-truth video.
22