Language-Driven 6-DoF Grasp Detection
Using Negative Prompt Guidance
Toan Nguyen1 , Minh Nhat Vu2,3,∗ , Baoru Huang4 , An Vuong1 , Quan Vuong5 , Ngan Le6 , Thieu Vo7 , and Anh Nguyen8
1 FPT Software AI Center, Vietnam 2 TU Wien, Austria 3 AIT GmbH, Austria, ∗Corresponding author 4 Imperial College London, United Kingdom 5 Physical Intelligence, United States 6 University of Arkansas, United States 7 Ton Duc Thang University, Vietnam 8 University of Liverpool, United Kingdom
Abstract. 6-DoF grasp detection has been a fundamental and challenging problem in robotic vision. While previous works have focused on ensuring grasp stability, they often do not consider human intention conveyed through natural language, hindering effective collaboration between robots and users in complex 3D environments. In this paper, we present a new approach for language-driven 6-DoF grasp detection in cluttered point clouds. We first introduce Grasp-Anything-6D, a largescale dataset for the language-driven 6-DoF grasp detection task with 1M point cloud scenes and more than 200M language-associated 3D grasp poses. We further introduce a novel diffusion model that incorporates a new negative prompt guidance learning strategy. The proposed negative prompt strategy directs the detection process toward the desired object while steering away from unwanted ones given the language input. Our method enables an end-to-end framework where humans can command the robot to grasp desired objects in a cluttered scene using natural language. Intensive experimental results show the effectiveness of our method in both benchmarking experiments and real-world scenarios, surpassing other baselines. In addition, we demonstrate the practicality of our approach in real-world robotic applications. Our project is available at https://airvlab.github.io/grasp-anything.
Keywords: Language-Driven 6-DoF Grasp Detection, Diffusion Models
1 Introduction
Grasp detection stands as a foundational and enduring challenge in the field of robotics and computer vision [8, 23]. This task involves identifying a suitable configuration for the robotic hand that stably grasps the objects, facilitating the effective manipulation capability in the robot’s operating environment. Traditional grasp detection methods have predominantly focused on ensuring the


2 Nguyen et al.
stability of the detected grasp pose, while often neglecting the human intention. This limitation underscores a large gap between current approaches and real-world user-specified requirements [72]. The integration of human intention conveyed through natural language, is therefore crucial to help robots perform complex tasks more flexibly. This enables users to communicate task specifications more intuitively and comprehensively to the intelligent robot, facilitating a more effective human-robot collaboration.
Grasp the mug. Get the sunglasses. Hand me the spoon. Pick up the green pencil. Give me the wrist-watch.
Fig. 1: We tackle the task of language-driven 6-DoF grasp detection in cluttered 3D point cloud scenes.
In recent years, thanks to advancements in large language models [10, 11, 46] and large vision-language models [27, 28, 51], there has been a surge of interest in language-driven robotics research [5, 6, 13, 40, 52, 69, 84]. This research field focuses on developing intelligent robots that can understand and respond to human linguistic commands. For example, SayCan [6] and PaLM-E [10] are robotic language models designed to provide instructions for robots operating in realworld environments. Trained on large-scale data, RT-1 [5] and RT-2 [84] are robotic systems capable of performing low-level actions in response to natural language commands. While significant progress has been made in the field, it is noteworthy that only a few works have addressed the task of language-driven grasp detection [39, 52, 64, 65, 67, 72]. Furthermore, these methods still exhibit considerable shortcomings. Particularly, while the authors in [39,64] solely focus on single-object scenarios, the works in [65, 67, 72] restrict grasp detection to 2D configurations. These limitations prevent the robot from capturing the complexity of real-world 3D and multi-object scenarios. In this research, we address these limitations by training a new system that detects language-driven 6-DoF grasp poses, with a focus on grasping objects within diverse and complex scenes represented as 3D point clouds. We first introduce a new dataset, namely Grasp-Anything-6D, as a largescale dataset for language-driven 6-DoF grasp detection in 3D point clouds. Our dataset builds upon the Grasp-Anything dataset [72] and incorporates a stateof-the-art depth estimation method [4] to support 2D to 3D projection, and manual correction to ensure the dataset quality. Specifically, Grasp-Anything6D provides one million (1M) 3D point cloud scenes with comprehensive object grasping prompts and dense 6-DoF grasp pose annotations. With its extensive volume, our dataset enables the capability of 6-DoF grasp detection using language instructions directly from the point cloud. Empirical demonstrations show that our dataset successfully facilitates grasp detection in diverse and complex scenes, both in vision-based experiments and real-world robotic settings.


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 3
With the new dataset in hand, we propose a new diffusion model to address the challenging problem of language-driven 6-DoF grasp detection called LGrasp6D. We opt for diffusion models due to their recent impressive results in various generation tasks [19, 43, 60], including image synthesis [12, 42], video generation [47, 78], and point cloud generation [32, 37]. However, the application of diffusion models to grasp detection remains under-explored [39, 68]. Unlike previous works that mostly focus on language-driven grasp detection in 2D image [65,67,72] or in 3D point cloud with single object [39,64], our work proposes a new diffusion model for language-driven 6-DoF grasp detection in cluttered 3D point cloud environments. In practice, language-driven 6-DoF grasp detection is a fine-grained task driven by the language, e.g., “Grasp the blue cup” and “Grasp the black cup” are for two different objects in the scene. Therefore, we introduce a new negative prompt guidance learning strategy to tackle this fine-grained nature. The main motivation of this strategy is to learn a negative prompt embedding that can encapsulate the notion of other undesired objects in the scene. When being applied in the generation process, the learned negative prompt embedding explicitly guides the grasp pose toward the desired object while avoiding unwanted ones. Our LGrasp6D method is an end-to-end pipeline that enables humans to command the robot to grasp desired objects in a cluttered scene using a natural language prompt. Figure 1 illustrates examples of our language-driven grasp detection in 3D point clouds. To summarize, our contributions are three-fold:
– We propose Grasp-Anything-6D, a large-scale dataset for language-driven 6-DoF grasp detection in 3D point clouds. – We propose a new diffusion model that learns and applies negative prompt guidance, significantly enhancing the grasp detection process. – We demonstrate that our dataset and the proposed method outperform other approaches and enable successful real-world robotic manipulation.
2 Related Works
Robot Grasp Detection. Several works for robot grasp detection addressed the task on 2D images [20, 26, 53, 82]. Thanks to recent advancements in 3D perception [21, 41, 48, 49], 6-DoF grasp detection in 3D point clouds is gaining increasing interest in both computer vision and robotics communities. In general, two main lines of approaches have been employed for this problem. The first line [18, 29, 35, 36] involves sampling various grasp candidates across the input point cloud, followed by validation using a grasp evaluator network. The primary drawback of methods in the first line lies in their inefficiency in terms of speed, attributed to their multi-stage structure. In contrast, the second line of research detects the grasp poses in an end-to-end manner [16,41,50,74], achieving a more favorable balance in terms of the time-accuracy tradeoff. For instance, Qin et al. [50] presented a novel gripper contact model and a single-shot neural network to predict amodal grasp proposals, while Wang et al. [74] proposed the concept


4 Nguyen et al.
of graspness to detect the scene graspable areas. However, most of the existing 6DoF grasp detection methods do not take into account language as the input. In this work, we follow the end-to-end approach. Our method integrates language instructions into the grasp detection process, ensuring that the detected grasp pose is aligned with the user-specified requirements.
Language-Guided Robotic Manipulation. Amidst the remarkable strides of large language models [7, 11, 46] and large vision-language models [27, 28, 51], several recent works have harnessed language semantics for multiple tasks of robot manipulation [5,17,40,54,84]. For instance, the authors in [17] presented a framework that learns meaningful skills from language-based expert demonstrations. Nguyen et al. [40] utilized language to detect open-vocabulary affordance for 3D point cloud objects. More recently, the authors in [84] proposed a family of models that learn generalizable and semantically aware policies derived from fine-tuning large vision-language models trained on web-scale data. Besides, the task of language-guided grasp detection is also under active exploration. However, approaches in this research direction present several limitations. Specifically, the works in [39, 64] only addressed single-object scenarios. The authors in [65,67,71,72] exclusively detected 2D rectangle grasp poses. More recently, the method in [52] required multiple viewpoints of the scene to build the language field, which is not always obtainable. In contrast to these works, our method is capable of detecting language-driven 6-DoF grasp poses in cluttered single-view point cloud scenes, making it well-suitable for real-world robotic applications. Diffusion Probabilistic Models. Diffusion models are a class of neural generative models, based on the stochastic diffusion process in Thermodynamics [59]. In this setting, a sample from the data distribution is gradually noised by the forward diffusion process. Then, a neural network learns the reverse process to gradually denoise the sample. First introduced by [59], diffusion models have been further simplified and accelerated [19, 60], and improved significantly [2,43,62,77]. In recent years, many works have explored applying diffusion models for various generation problems, such as image synthesis [12, 79], scene synthesis [25, 73], and human motion generation [66, 76]. In robotics, diffusion models have also been applied to many problems ranging from policy learning [9, 81], task and motion planning [31, 68] to robot design [75]. However, few works have adopted diffusion models for the task of grasp detection [39, 68]. Notably, none of them consider the task of language-driven grasping in 3D cluttered point clouds. To address this challenging task, we propose a novel diffusion model that incorporates a new negative prompt guidance learning approach. This strategy assists in guiding the generation process toward the desired grasp distributions while steering away from unwanted ones. The effectiveness of our proposed approach is demonstrated through comprehensive experiments.
3 The Grasp-Anything-6D Dataset
Our Grasp-Anything-6D dataset is built upon the Grasp-Anything dataset [72]. Leveraging foundation models [45,56], Grasp-Anything is a large-scale dataset for


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 5
2D language-driven grasp detection. This dataset consists of 1M RGB images and ≈3M objects, substantially surpassing prior datasets in diversity and volume. To bring the problem from 2D to 3D, we first leverage the state-of-the-art depth estimation method ZoeDepth [4] to estimate the depth map given the input RGB images of Grasp-Anything. Subsequently, we perform projection and manual verification to ensure the quality of our dataset.
3D Scenes and 6-DoF Grasps Construction. For a given 2D scene in the Grasp-Anything dataset [72], we first employ ZoeDepth [4] to get the depth map for the image and establish the 3D point cloud scene with the camera model assumption of a 55-degree field of view and central principal point. We select the field of view of 55 degrees because it leads to 3D scenes representing real object scales. Next, to bring a 2D grasp configuration to 3D, we first infer the 3D position using the center of the 2D rectangle grasp in the image. Since in the Grasp-Anything dataset, the position of the 2D grasp may not necessarily be integers, we employ bilinear interpolation to calculate its corresponding 3D position by considering the 3D coordinates of neighboring pixels. The position determines the translation part of the grasp representation. For the rotation part, we utilize the angle of the 2D rectangle grasp and map it to 3D to rotate the 6-DoF grasp pose accordingly. The width of the 6-DoF grasp is derived from the width of the 2D grasp. Adhering to the Robotiq 2F-140 gripper specifications [55], we establish the maximum grasp width as 202.1 mm, and discard any grasps exceeding this threshold. The overview of our 3D scenes and 6-DoF grasps construction process is illustrated in Figure 2. We maintain the same scene description and grasping prompts as in the Grasp-Anything dataset. Additionally, we infer the 3D masks on the point cloud scene for every object in the grasp list using the corresponding segmentation masks in 2D.
Depth Estimation
2D image and grasps Depth map
3D Construction & Post-Processing
3D point cloud and grasps
Fig. 2: Overview of Grasp-Anything-6D dataset construction pipeline.
Post-Processing. After converting the 2D scenes and grasps to 3D, we manually check for the collision of the 6-DoF grippers and point cloud scenes, as well as whether the grippers can stably grasp the objects. These problems may occur since the depth estimation network [4] may not always bring good results. Concretely, we remove the grasp poses that collide with the point cloud scene and those whose closing volume between the fingers does not intersect the object determined by its 3D mask. As a result, our Grasp-Anything-6D dataset consists of 1M point cloud scenes, with comprehensive grasping prompts, and 200M corresponding dense and high-quality 6-DoF grasp poses.


6 Nguyen et al.
4 Grasp Detection using Negative Prompt Guidance
4.1 Motivation
Diffusion models have recently shown remarkable performance across various generation tasks. This makes it a promising choice for our problem, where grasp detection can be viewed as a generation process conditioned on both the point cloud scene and the language prompt. The main contribution of our diffusion model is a novel negative prompt guidance learning strategy. This is motivated by the notion that generating a grasp for a specific object can benefit significantly from guidance away from unwanted objects in the scene. Our LGrasp6D leverages this by integrating learning the negative prompt embedding into the training process alongside the conventional denoising objective. Our target for the negative prompt embedding is to capture the notion of other undesired objects in the scene. The learned negative prompt guidance is then applied in the sampling to assist the grasp detection process.
4.2 Language-Driven 6-DoF Grasp Detection
MLP
Multi-Head Cross-Attention
Q
Text Encoder Scene Encoder
"Bring me the vase."
K, V
MLP
...
Scene tokens
MLP
Position encoder
Element-wise sum
Concatenation
Frozen
Fig. 3: Overview of our denoising network. In addition to predicting the noise, our denoising network is trained to learn the negative prompt embedding, which is supervised by the text embeddings associated with other unwanted objects in the same scene.
Forward Process. We use the se(3) Lie algebra [57] to represent the translation and rotation of our grasp poses. We use the se(3) representation since it allows us to conveniently perform the operators of addition and multiplication by a scalar required by our forward and reverse diffusion processes. The grasp pose is then represented as the concatenation of se(3) vector and the grasp width. Note that one can easily convert between the se(3) and 4 × 4 transformation matrix representation using the logarithm map and exponential map [44]. Given a target grasp pose g0 in the training dataset, in the forward process, we obtain a sequence of perturbed grasp poses by gradually adding to it small amounts


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 7
of Gaussian noise in T steps. The noise step sizes are specified by a predefined variance schedule {βt ∈ (0, 1)}T
t=1. The forward process is formulated as:
q (gt|gt−1) = N gt; p1 − βtgt−1, βtI . (1)
The perturbed pose at any arbitrary time step t can be obtained by:
gt = √α ̄tg0 + √1 − α ̄tε, (2)
where α ̄t = Qt
i=1 αt with αt = 1 − βt and ε ∼ N (0, I). When T → ∞, gT is equivalent to N (0, I) [19]. Denoising Network. Our denoising network approximates the added noise described in the forward process by incorporating both the conditions of the point cloud scene and the textual prompt specifying the target object. Additionally, our network learns a vector representation serving as a negative prompt guidance. In our framework, this representation is guided by the available textual prompts associated with other objects within the scene. The details of our denoising network are shown in Figure 3. The denoising network first encodes the grasp pose gt at a specific time step t using a grasp encoder MLP. The scene encoder encodes the point cloud scene S to ns scene embedding tokens. In our framework, we use PointNet++ [49] as the underlying architecture for the scene encoder. For the textual prompt, we employ a pretrained text encoder to get a text embedding t. We use sinusoidal positional embedding [19] to embed the time step t to a high-dimensional vector. Afterward, we form the unified representation funi of the grasp pose, the textual prompt, and the time step. In concrete, we concatenate the time embedding with the element-wise sum of the grasp embedding and the text embedding. Subsequently, we adopt the multi-head cross-attention mechanism to capture the intricate relationships among input components. Specifically, the query for the cross-attention is the unified feature funi while the ns scene tokens serve as keys and values. The output of the cross-attention module is then fed to an MLP to obtain the predicted noise εθ (gt, S, t, t). We supervise the noise prediction by optimizing the simplified objective function as described in [19]:
Lnoise = Eε,g0,S,t,t
h
∥εθ (gt, S, t, t) − ε∥2i
. (3)
Negative Prompt Learning. Along with estimating the noise, the denoising network also produces the negative prompt embedding t ̃. We subtract the text embedding t from the scene tokens, compute the average over ns resulting vectors, and then pass the output through an MLP to get t ̃. Our purpose for t ̃ is that it can encapsulate the notion of other objects in the same scene. Hence, our objective is to minimize the distance between t ̃ and the negative text embeddings which are text embeddings corresponding to other objects. Specifically, we define the loss function for the learning of negative prompt embedding as:
Lnegative = D t ̃,  ̄T = {t ̄i}m
i=1 = minm
i=1 t ̃ − t ̄i
2
2 , (4)


8 Nguyen et al.
where D (·) denotes the distance function,  ̄T = {t ̄i}m
i=1 is the set of m negative text embeddings. In training, we simultaneously optimize both the denoising loss Lnoise and the loss for negative prompt embedding learning Lnegative.
Reverse Process with Negative Prompt Guidance. Different from conventional diffusion models, our reverse diffusion process utilizes the negative prompt embedding learned during the training to guide the grasp pose toward the desired object while avoiding unwanted ones. Our generation process can be formulated as a conditional distribution p g|S, t, ¬t ̃ . The negation sign of t ̃ indicates that we aim to sample the grasp pose with the absence of the t ̃ prompt condition. We begin with the following proposition:
Proposition 1. The conditional distribution p g|S, t, ¬t ̃ can be factorized as
p g|S, t, ¬t ̃ ∝ p (g|S) p (g|t, S)
p g|t ̃, S . (5)
Proof. See Supplementary Material.
With Equation 5, alongside detecting grasps conditioning on the scene and the user-specified prompt via p (g|S) and p (g|t, S), we can now seamlessly incorporate the negative prompt guidance into our reverse process via p g|t ̃, S .
Remark 1. Liu et al. [30] demonstrated how diffusion models can be composed based on their connection to energy-based models [14]. We recall this relationship in detail in our Supplementary. Consequently, following the expression in [30], we can formulate our compositional denoising step in the reverse process as:
ε ̃θ gt, S, t, ¬t ̃, t = εθ (gt, S, ∅, t) + w εθ (gt, S, t, t) − εθ gt, S, t ̃, t . (6)
In Equation 6, p (g|S), p (g|t, S) and p g|t ̃, S are parameterized by εθ (gt, S, ∅, t), εθ (gt, S, t, t) and εθ gt, S, t ̃, t respectively. εθ gt, S, t ̃, t is the output of the denoising network when the learned negative prompt embedding t ̃ is plugged in as the text embedding. w is a hyperparameter that controls the strength of the negative guidance. εθ (gt, S, ∅, t) is the predicted noise when the text condition is discarded. In training, we learn εθ (gt, S, ∅, t) by randomly masking out the text embedding with a predefined probability pmask. Given the denoising step defined in Equation 6, we can now sample grasps from Gaussian noise by applying the reverse process from timestep T back to 0 using the following formulation:
gt−1 = √1αt
gt − 1 − αt
√1 − α ̄t
ε ̃θ gt, S, t, ¬t ̃, t + pβtz, (7)
where z ∼ N (0, I) if the time step t > 1, else z = 0.
4.3 Training and Sampling
We define the overall loss function for training as L = 0.9Lnoise +0.1Lnegative. We utilize the pretrained CLIP ViT-B/32 text encoder [51] for our text encoder and


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 9
freeze it during training. We set the number of timesteps to T = 200, and set the forward diffusion variances to increase linearly from β1 = 10−4 to βT = 0.02. The probability of masking out the text embedding is set to pmask = 0.1. The whole network is trained over 200 epochs on a cluster of 8 A100 GPUs with a batch size of 128. We use Adam optimizer [22] with the learning rate 10−3 and the weight decay 10−4. In sampling, we set the negative guidance scale to w = 0.2. To obtain a favorable inference speed, we pre-compute the scene tokens, the text embedding t, and the negative prompt embedding t ̃ since they are independent of the timestep. This precomputation substantially reduces the detection time, making our method feasible for practical implementation on real robots.
5 Experiments
In this section, we evaluate the effectiveness of our LGrasp6D trained on the Grasp-Anything-6D dataset via several vision-based and real robot experiments.
5.1 Language-Driven 6-DoF Grasp Detection Results
Baselines. We evaluate our method against generative approaches for 6-DoF grasp detection, which are 6-DoF GraspNet [35], SE(3)-DF [68], and 3DAPNet [39]. We adapt the frameworks of these baselines to integrate textual input into the detection process. To ensure a fair comparison, we utilize the CLIP ViT-B/32 [51] as the text encoder for all methods. We also include our method without utilizing negative prompt guidance (denotes as Ours w.o. NPG) as an additional baseline for comparison. Detailed implementation information for all baselines is available in our Supplementary Material. Setup. We train all baselines on 80% scenes of the Grasp-Anything-6D dataset and evaluate them on the remaining 20%. For each pair of point cloud scene-textual prompts, we detect 64 grasp poses for evaluation. To benchmark the methods’ detection capabilities, we use three metrics, which are the coverage rate [35], earth mover’s distance [68], and collision-free rate [83]. The coverage rate (CR) [35] measures how well the space of ground-truth grasps is covered by the detected grasps. The earth mover’s distance (EMD) [68] evaluates the dissimilarity between the distributions of ground-truth grasps and the detected ones. Finally, the collision-free rate (CFR) [83] assesses the occurrence of collisions between the gripper of the detected grasps and the scene. The final results for all metrics are averaged across all scene-text prompt pairs. Since latency is a critical factor for any robotics applications, we additionally benchmark the inference speeds of all methods using the inference time in seconds (IT). Specifically, for each baseline, we calculate its inference time for detecting 1000 grasp poses across 1000 different scene-text pairs and take the average result. Quantitative Results. Table 1 shows the results of language-driven 6-DoF grasp detection on our Grasp-Anything-6D dataset. The outcomes indicate the advantages of our methods, even without negative prompt guidance, over other baselines. Our complete method consistently achieves the highest scores across


10 Nguyen et al.
all three metrics for grasp detection capability. It significantly surpasses the second-best method, which is our framework without negative prompt guidance, with large margins of 0.1235 on CR, 0.2249 on EMD, and 0.0370 on CFR. This highlights the effectiveness of our proposed negative prompt guidance learning. Regarding latency, our methods achieve competitive IT scores compared to other diffusion model-based methods (SE(3)-DF and 3DAPNet). Although 6-DoF GraspNet achieves the best IT, it is important to note that this is a variational autoencoder-based method requiring only a single decoding step, and its results on the remaining metrics are poor.
Baseline CR↑ EMD↓ CFR↑ IT↓
6-DoF GraspNet [35] 0.3802 0.8035 0.6900 0.4216
SE(3)-DF [68] 0.4290 0.7565 0.7325 1.7233
3DAPNet [39] 0.4777 0.7381 0.7213 3.4274
LGrasp6D (ours) w.o. NPG 0.5459 0.6262 0.7336 1.4328
LGrasp6D (ours) 0.6694 0.4013 0.7706 1.4832
Table 1: Results on Grasp-Anything-6D dataset.
Qualitative Results. We present the qualitative results of all baselines in detecting language-driven grasps in Figure 4. Point cloud scenes are selected from our Grasp-Anything-6D dataset. The results indicate that LGrasp6D exhibits a significantly stronger capability in detecting language-driven grasp poses compared to the others. Specifically, our method excels at focusing on the desired objects, whereas other methods often get distracted by undesired ones. More qualitative results are provided in our Supplementary Material. Accelerating Detection. While latency is critical for robot applications, diffusion models are notorious for their low inference speed [61]. Despite our method achieving a competitive inference speed, as shown in Table 1, we continue to seek even faster models with comparable performance. Hence, we benchmark our LGrasp6D employing the fast reversion technique of denoising diffusion implicit models (DDIM) [60], with numbers of sampling steps of 200 (the original one), 100, 50, 20, and 10. The results are shown in Table 2. We can observe that decreases in the sampling step lead to decreases in performance. However, all the variants still outperform other baselines in Table 1. Regarding the inference time, these accelerated models obtain significantly better inference speed compared to the original one. The variant with 50 steps already surpasses the 6-DoF
Baseline CR↑ EMD↓ CFR↑ IT↓ LGrasp6D - 10 steps 0.5611 0.5273 0.7368 0.0726 LGrasp6D - 20 steps 0.6425 0.4300 0.7580 0.1464 LGrasp6D - 50 steps 0.6439 0.4254 0.7639 0.3991 LGrasp6D - 100 steps 0.6522 0.4110 0.7633 0.8427 LGrasp6D - 200 steps 0.6694 0.4013 0.7706 1.4832
Table 2: DDIM accelerating results.
Baseline CR↑ EMD↓ CFR↑ 6-DoF GraspNet [35] 0.3498 0.8501 0.6927 SE(3)-DF [68] 0.3892 0.7622 0.7205 3DAPNet [39] 0.4491 0.7434 0.7092 LGrasp6D (ours) w.o. NPG 0.5208 0.6422 0.7385 LGrasp6D (ours) 0.6420 0.4197 0.7683
Table 3: Cross-dataset results.


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 11
Pick up the steel knife.
Give me the black headphone.
Grasp the flower vase.
Ours Ours w.o. NPG 3DAPNet SE(3)-DF 6-DoF GraspNet
Hand me the orange.
Fig. 4: Language-driven 6-DoF grasp detection qualitative results.
GraspNet method (0.3991 seconds compared to 0.4216 seconds). Although the variant with 10 steps achieves the best detection speed, it is not recommended as its detection performance is severely compromised.
5.2 Generalization Analysis
Cross-Dataset Transferability. Given the extensive scale and diversity of our Grasp-Anything-6D dataset, we expect that our proposed method, trained on this dataset, will exhibit strong generalization capabilities when tested on a distinct dataset. Specifically, we evaluate the language-driven grasp detection performance of models trained on Grasp-Anything-6D using the Contact-GraspNet dataset [63]. This dataset comprises point cloud scenes of cluttered tabletops synthesized using objects and 6-DoF grasps from [15] and a random camera view. We utilize the object category names as textual prompts for language-driven grasping. The findings showcased in Table 3 exhibit a comparable trend to those observed in the Grasp-Anything-6D dataset. Our method continues to outperform its counterparts across all three metrics, with the version lacking negative prompt guidance following behind. Furthermore, the slight performance decrease


12 Nguyen et al.
on the new dataset is noteworthy. They underscore the efficacy of our dataset, as models trained on it demonstrate strong generalizability. Grasp Detection in the Wild. Figure 5 illustrates results of our method in point cloud scenes captured from diverse real-world environments, such as working desks, bathrooms, and kitchens. As we can observe, the detected grasp poses exhibit satisfactory quality. This indicates that despite being trained on synthetic data, our approach effectively generalizes to real-world environments.
Grasp the green ball. Get the toothpaste tube. Give me the controller. Pick up the thermometer. Bring me the kettle.
Fig. 5: In the wild language-driven 6-DoF grasp detection results.
5.3 Negative Prompt Guidance Analysis
We offer a more intuitive understanding of how negative prompt guidance influences the grasp detection results. Specifically, we ultimately sample 1000 grasp poses for each object in a given point cloud scene for both cases: our framework with negative prompt guidance and the one without it. We then employ t-SNE [33] to visualize all grasp poses on a 2D plane. The results are depicted in Figure 6, where grasp data points of the same color are detected for the same object. We can observe that negative prompt guidance significantly assists our method in discriminatively detecting grasp poses for different objects. Conversely, without negative prompt guidance, detecting grasp poses for one object is seriously confused by other ones. This further highlights the effectiveness of our proposed approach. More comparison results can be viewed in Figure 8.
Ours w.o. NPG Ours w. NPG
Fig. 6: Negative prompt guidance analysis.
Pick up the folk. Grasp the clock. Hold the phone.
Fig. 7: Failure cases.
Grasp the apple. Pick up the red pencil.
Ours w.o. NPG Ours w. NPG Ours w.o. NPG Ours w. NPG
Fig. 8: Comparisons between models with and without negative prompt guidance.


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 13
5.4 Robotics Experiment
RealSense
(a) (b)
Robotiq 2F-85
LGrasp6D
ROS Grapsing pose
Trajectory optimization
generation
NIC
TwinCAT
12
Query
3
4
Query: “Bring me the black cup.”
Fig. 9: (a) Experiment setup. (b) Example of the execution of a grasping task.
Baseline Input Modality Single Cluttered
GG-CNN [34] + CLIP [51] RGB-D 0.10 0.07
CLIPORT [58] RGB-D 0.27 0.30
Det-Seg-Refine [1] + CLIP [51] RGB-D 0.30 0.23
GR-ConvNet [24] + CLIP [51] RGB-D 0.33 0.30
CLIP-Fusion [80] RGB-D 0.40 0.40
LGD [71] RGB-D 0.43 0.42
6-DoF GraspNet [35] Point clouds 0.31 0.27
SE(3)-DF [68] Point clouds 0.35 0.34
3DAPNet [39] Point clouds 0.36 0.34
LGrasp6D (ours) w.o. NPG Point clouds 0.38 0.36
LGrasp6D (ours) Point clouds 0.43 0.42
Table 4: Robotic language-driven grasp detection results.
Setup. In Figure 9, we present the robotic experiment conducted on a KUKA robot. The success rate is used for evaluation. Using an Intel RealSense D435i depth camera, the detected 6-DoF grasp poses are mapped to robot’s 6-DoF end-effector poses using transformation matrices obtained via hand-eye calibration [38]. The trajectory planner and the computed torque controller [3, 70] are employed for the grasp execution. We use two computers for the experiment. The first computer executes the real-time control software Beckhoff TwinCAT of 8 kHz update frequency, while the second one utilizes the Robot Operating System (ROS) for the camera and the Robotiq 2F-85 gripper. Using EtherCAT protocol, PC1 communicates with the robot via a network interface card (NIC). The inference process is performed on PC2, utilizing an NVIDIA RTX 3080 graphic card. Our assessment encompasses both single-object and cluttered scenarios, involving a diverse set of real-world daily objects. To ensure the reliability, we repeat each experiment for all methods a total of 45 times. Baselines. Besides the baselines utilized in previous experiments, we additionally compare LGrasp6D with language-supported versions of state-of-the-art


14 Nguyen et al.
2D grasp detectors, including GR-CNN [24], Det-Seg-Refine [1], GG-CNN [34], CLIPORT [58], CLIP-Fusion [80], and LGD [71]. In all cases, we employ the pretrained CLIP ViT-B/32 [51] as the text encoder. The implementation details of all baselines can be found in our Supplementary Material. Results. Our method, incorporating negative prompt guidance, demonstrates better performance compared to other baselines in Table 4. Additionally, even though LGrasp6D is trained on Grasp-Anything-6D, a synthesis dataset exclusively created by foundation models, it still yields commendable results when applied to real-world objects.
6 Discussion
Despite promising results, it is important to acknowledge that our method still has limitations, as illustrated in Figure 7. The left case depicts an example of grasping the wrong object, while the middle one illustrates a detected grasp colliding with an object. The final case shows our method detecting a grasp that mis-targets the desired object. These underscore the challenges in languagedriven 6-DoF grasping, indicating its need for further investigation. For future research, we aim to enhance the performance by incorporating more advanced techniques to capture the intricate correlation among input modalities. In addition, our work can be extended to address language-driven 6-DoF grasping at both the part-level and task-level. For instance, instead of object-specific prompts like “Grasp the knife”, one can provide more detailed prompts such as “Grasp the knife by its handle” or "Grasp the knife for cutting”. Furthermore, it would be beneficial to extend our approach to accommodate different types of robot end-effectors to enhance the flexibility and adaptability of our framework. Lastly, integrating learning language-driven 6-DoF grasp detection with robotic control could create a more effective end-to-end pipeline, connecting human instructions directly to low-level robot actions.
7 Conclusion
We address the task of language-driven 6-DoF grasp detection in cluttered point clouds. In particular, we have presented the Grasp-Anything-6D dataset as a large-scale dataset for the task with 1M point cloud scenes. We have introduced a novel LGrasp6D diffusion model incorporating the new concept of negative prompt guidance learning. Our proposed negative prompt guidance assists in tackling the fine-grained challenge of the language-driven grasp detection task, directing the detection process toward the desired object by steering away from undesired ones. Empirical results demonstrate the superiority of our method over other baselines in various settings. Furthermore, extensive experiments validate the efficacy of our approach in real-world environments and robotic applications.


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 15
References
1. Ainetter, S., Fraundorfer, F.: End-to-end trainable deep neural network for robotic grasp detection and semantic segmentation from rgb. In: ICRA (2021) 2. Austin, J., Johnson, D.D., Ho, J., Tarlow, D., Van Den Berg, R.: Structured denoising diffusion models in discrete state-spaces. NeurIPS (2021) 3. Beck, F., Vu, M.N., Hartl-Nesic, C., Kugi, A.: Singularity avoidance with application to online trajectory optimization for serial manipulators. IFAC-PapersOnLine (2023) 4. Bhat, S.F., Birkl, R., Wofk, D., Wonka, P., Müller, M.: Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288 (2023) 5. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al.: Rt-1: Robotics transformer for real-world control at scale. In: RSS (2023) 6. Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., et al.: Do as i can, not as i say: Grounding language in robotic affordances. In: CoRL (2023) 7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. NeurIPS (2020) 8. Caldera, S., Rassau, A., Chai, D.: Review of deep learning methods in robotic grasp detection. Multimodal Technologies and Interaction (2018) 9. Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B., Song, S.: Diffusion policy: Visuomotor policy learning via action diffusion. In: RSS (2023) 10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. JMLR (2023) 11. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: NAACL (2019) 12. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS (2021) 13. Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimodal language model. In: ICML (2023) 14. Du, Y., Li, S., Mordatch, I.: Compositional visual generation with energy based models. NeurIPS (2020) 15. Eppner, C., Mousavian, A., Fox, D.: Acronym: A large-scale grasp dataset based on simulation. In: ICRA (2021) 16. Fang, H.S., Wang, C., Gou, M., Lu, C.: Graspnet-1billion: A large-scale benchmark for general object grasping. In: CVPR (2020) 17. Garg, D., Vaidyanath, S., Kim, K., Song, J., Ermon, S.: Lisa: Learning interpretable skill abstractions from language. NeurIPS (2022) 18. Gou, M., Fang, H.S., Zhu, Z., Xu, S., Wang, C., Lu, C.: Rgb matters: Learning 7-dof grasp poses on monocular rgbd images. In: ICRA (2021) 19. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS (2020) 20. Jiang, Y., Moseson, S., Saxena, A.: Efficient grasping from rgbd images: Learning using a new rectangle representation. In: ICRA (2011) 21. Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: Lerf: Language embedded radiance fields. In: ICCV (2023)


16 Nguyen et al.
22. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2015) 23. Kleeberger, K., Bormann, R., Kraus, W., Huber, M.F.: A survey on learning-based robotic grasping. Current Robotics Reports (2020) 24. Kumra, S., Joshi, S., Sahin, F.: Antipodal robotic grasping using generative residual convolutional neural network. In: IROS (2020) 25. Lee, S., Lee, J.: Posediff: Pose-conditioned multimodal diffusion model for unbounded scene synthesis from sparse inputs. In: WACV (2024) 26. Lenz, I., Lee, H., Saxena, A.: Deep learning for detecting robotic grasps. IJRR (2015) 27. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: ICML (2022) 28. Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR (2022) 29. Liang, H., Ma, X., Li, S., Görner, M., Tang, S., Fang, B., Sun, F., Zhang, J.: Pointnetgpd: Detecting grasp configurations from point sets. In: ICRA (2019) 30. Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual generation with composable diffusion models. In: ECCV (2022) 31. Liu, W., Hermans, T., Chernova, S., Paxton, C.: Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects. In: CoRL Workshop (2022) 32. Luo, S., Hu, W.: Diffusion probabilistic models for 3d point cloud generation. In: CVPR (2021) 33. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. JMLR (2008) 34. Morrison, D., Corke, P., Leitner, J.: Closing the loop for robotic grasping: A realtime, generative grasp synthesis approach. In: RSS (2018) 35. Mousavian, A., Eppner, C., Fox, D.: 6-dof graspnet: Variational grasp generation for object manipulation. In: ICCV (2019) 36. Murali, A., Mousavian, A., Eppner, C., Paxton, C., Fox, D.: 6-dof grasping for target-driven object manipulation in clutter. In: ICRA (2020) 37. Nakayama, G.K., Uy, M.A., Huang, J., Hu, S.M., Li, K., Guibas, L.: Difffacto: Controllable part-based 3d point cloud generation with cross diffusion. In: ICCV (2023) 38. Nguyen, H., Pham, Q.C.: On the Covariance of X in AX = XB. T-RO (2018) 39. Nguyen, T., Vu, M.N., Huang, B., Van Vo, T., Truong, V., Le, N., Vo, T., Le, B., Nguyen, A.: Language-conditioned affordance-pose detection in 3d point clouds. ICRA (2024) 40. Nguyen, T., Vu, M.N., Vuong, A., Nguyen, D., Vo, T., Le, N., Nguyen, A.: Openvocabulary affordance detection in 3d point clouds. In: IROS (2023) 41. Ni, P., Zhang, W., Zhu, X., Cao, Q.: Pointnet++ grasping: Learning an end-to-end spatial grasp generation algorithm from sparse point clouds. In: ICRA (2020) 42. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In: ICML (2022) 43. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In: ICML (2021) 44. Onishchik, A.L., Vinberg, E.B.: Lie groups and algebraic groups. Springer Science & Business Media (2012) 45. OpenAI: Introducing chatgpt. OpenAI Blog (2022), https://openai.com/blog/ chatgpt


Language-Driven 6-DoF Grasping w. Negative Prompt Guidance 17
46. OpenAI: GPT-4 technical report. CoRR (2023) 47. OpenAI: Video generation models as world simulators. OpenAI Technical Report (2024), https://openai.com/research/video-generation-models-as-worldsimulators
48. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classification and segmentation. In: CVPR (2017) 49. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS (2017) 50. Qin, Y., Chen, R., Zhu, H., Song, M., Xu, J., Su, H.: S4g: Amodal single-view single-shot se (3) grasp detection in cluttered scenes. In: CoRL (2020) 51. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021) 52. Rashid, A., Sharma, S., Kim, C.M., Kerr, J., Chen, L.Y., Kanazawa, A., Goldberg, K.: Language embedded radiance fields for zero-shot task-oriented grasping. In: CoRL (2023) 53. Redmon, J., Angelova, A.: Real-time grasp detection using convolutional neural networks. In: ICRA (2015) 54. Ren, A.Z., Govil, B., Yang, T.Y., Narasimhan, K.R., Majumdar, A.: Leveraging language for accelerated learning of tool manipulation. In: CoRL (2023) 55. Robotiq: Robotiq 2f-140. 2F-85 and 2F-140 Grippers (2018), https://robotiq. com/products/2f85-140-adaptive-robot-gripper
56. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 57. Samelson, H.: Notes on Lie algebras. Springer Science & Business Media (2012) 58. Shridhar, M., Manuelli, L., Fox, D.: Cliport: What and where pathways for robotic manipulation. In: CoRL (2022) 59. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: ICML (2015) 60. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021) 61. Song, Y., Dhariwal, P., Chen, M., Sutskever, I.: Consistency models. In: ICML (2023) 62. Song, Y., Durkan, C., Murray, I., Ermon, S.: Maximum likelihood training of scorebased diffusion models. NeurIPS (2021) 63. Sundermeyer, M., Mousavian, A., Triebel, R., Fox, D.: Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. In: ICRA (2021) 64. Tang, C., Huang, D., Ge, W., Liu, W., Zhang, H.: Graspgpt: Leveraging semantic knowledge from a large language model for task-oriented grasping. RA-L (2023) 65. Tang, C., Huang, D., Meng, L., Liu, W., Zhang, H.: Task-oriented grasp prediction with visual-language inputs. IROS (2023) 66. Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-Or, D., Bermano, A.H.: Human motion diffusion model. In: ICLR (2022) 67. Tziafas, G., Yucheng, X., Goel, A., Kasaei, M., Li, Z., Kasaei, H.: Language-guided robot grasping: Clip-based referring grasp synthesis in clutter. In: CoRL (2023) 68. Urain, J., Funk, N., Peters, J., Chalvatzaki, G.: Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion. In: ICRA (2023) 69. Van Vo, T., Vu, M.N., Huang, B., Nguyen, T., Le, N., Vo, T., Nguyen, A.: Openvocabulary affordance detection using knowledge distillation and text-point correlation. In: ICRA (2024)


18 Nguyen et al.
70. Vu, M.N., Beck, F., Schwegel, M., Hartl-Nesic, C., Nguyen, A., Kugi, A.: Machine learning-based framework for optimally solving the analytical inverse kinematics for redundant manipulators. Mechatronics (2023) 71. Vuong, A.D., Vu, M.N., Huang, B., Nguyen, N., Le, H., Vo, T., Nguyen, A.: Language-driven grasp detection. In: CVPR (2024) 72. Vuong, A.D., Vu, M.N., Le, H., Huang, B., Huynh, B., Vo, T., Kugi, A., Nguyen, A.: Grasp-anything: Large-scale grasp dataset from foundation models. In: ICRA (2024) 73. Vuong, A.D., Vu, M.N., Nguyen, T., Huang, B., Nguyen, D., Vo, T., Nguyen, A.: Language-driven scene synthesis using multi-conditional diffusion model. NeurIPS (2024) 74. Wang, C., Fang, H.S., Gou, M., Fang, H., Gao, J., Lu, C.: Graspness discovery in clutters for fast and accurate grasp detection. In: ICCV (2021) 75. Wang, T.H.J., Zheng, J., Ma, P., Du, Y., Kim, B., Spielberg, A., Tenenbaum, J., Gan, C., Rus, D.: Diffusebot: Breeding soft robots with physics-augmented generative diffusion models. NeurIPS (2024) 76. Wang, Y., Leng, Z., Li, F.W., Wu, S.C., Liang, X.: Fg-t2m: Fine-grained text-driven human motion generation via diffusion model. In: ICCV (2023) 77. Wang, Z., Jiang, Y., Zheng, H., Wang, P., He, P., Wang, Z., Chen, W., Zhou, M., et al.: Patch diffusion: Faster and more data-efficient training of diffusion models. NeurIPS (2024) 78. Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for textto-video generation. In: ICCV (2023) 79. Xie, S., Zhang, Z., Lin, Z., Hinz, T., Zhang, K.: Smartbrush: Text and shape guided object inpainting with diffusion model. In: CVPR (2023) 80. Xu, K., Zhao, S., Zhou, Z., Li, Z., Pi, H., Zhu, Y., Wang, Y., Xiong, R.: A joint modeling of vision-language-action for target-oriented grasping in clutter. In: ICRA (2023) 81. Ze, Y., Yan, G., Wu, Y.H., Macaluso, A., Ge, Y., Ye, J., Hansen, N., Li, L.E., Wang, X.: Gnfactor: Multi-task real robot learning with generalizable neural feature fields. In: CoRL (2023) 82. Zhang, H., Lan, X., Bai, S., Zhou, X., Tian, Z., Zheng, N.: Roi-based robotic grasp detection for object overlapping scenes. In: IROS (2019) 83. Zhao, B., Zhang, H., Lan, X., Wang, H., Tian, Z., Zheng, N.: Regnet: Region-based grasp network for end-to-end grasp detection in point clouds. In: ICRA (2021) 84. Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al.: Rt-2: Vision-language-action models transfer web knowledge to robotic control. In: CoRL (2023)