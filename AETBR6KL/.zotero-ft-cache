Less but Better: Parameter-Efficient Fine-Tuning of
Large Language Models for Personality Detection
Lingzhi Shen
University of Southampton Southampton, United Kingdom l.shen@soton.ac.uk
Yunfei Long
Queen Mary University of London London, United Kingdom yl20051@essex.ac.uk
Xiaohao Cai
University of Southampton Southampton, United Kingdom x.cai@soton.ac.uk
Guanming Chen
University of Southampton Southampton, United Kingdom gc3n21@soton.ac.uk
Imran Razzak
Mohamed bin Zayed University of Artificial Intelligence Abu Dhabi, United Arab Emirates imran.razzak@mbzuai.ac.ae
Shoaib Jameel
University of Southampton Southampton, United Kingdom M.S.Jameel@southampton.ac.uk
Abstract—Personality detection automatically identifies an individual’s personality from various data sources, such as social media texts. However, as the parameter scale of language models continues to grow, the computational cost becomes increasingly difficult to manage. Fine-tuning also grows more complex, making it harder to justify the effort and reliably predict outcomes. We introduce a novel parameter-efficient fine-tuning framework, PersLLM, to address these challenges. In PersLLM, a large language model (LLM) extracts high-dimensional representations from raw data and stores them in a dynamic memory layer. PersLLM then updates the downstream layers with a replaceable output network, enabling flexible adaptation to various personality detection scenarios. By storing the features in the memory layer, we eliminate the need for repeated complex computations by the LLM. Meanwhile, the lightweight output network serves as a proxy for evaluating the overall effectiveness of the framework, improving the predictability of results. Experimental results on key benchmark datasets like Kaggle and Pandora show that PersLLM significantly reduces computational cost while maintaining competitive performance and strong adaptability. Index Terms—Personality Detection, Large Language Models, Parameter-Efficient Fine-Tuning, Text Classification
I. INTRODUCTION
Personality refers to the stable traits in an individual’s emotions, thoughts, and behaviours that shape how they perceive, interpret, and interact with the world [1], [2]. It is a complex and multifaceted construct that combines various characteristics to form a person’s unique identity. The MyersBriggs Type Indicator (MBTI) [3] is a widely used personality assessment tool based on Carl Jung’s theory of psychological types. It categorizes individuals into 16 distinct personality types by evaluating them across four dimensions: Extraversion (E) vs. Introversion (I), Sensing (S) vs. Intuition (N), Thinking (T) vs. Feeling (F), and Judging (J) vs. Perceiving (P). In recent years, automated personality detection using machine learning has gained widespread attention in fields such as
This work was supported by the Alan Turing Institute and DSO National Laboratories under a grant on improving multimodal misinformation detection through affective analysis. Additional support was provided by the Interdisciplinary Research Pump-Priming Fund, University of Southampton.
personalized marketing [4], mental health assessment [5], and employee recruitment [6]. As consistently demonstrated [7], [8], the ability of language models to capture complex linguistic patterns and contextual dependencies is crucial for understanding personality traits from textual data [9]. These models are often extended through continued pre-training [10] and full fine-tuning [11] to adapt to specific domains or optimize for particular tasks. LLMs with billions or even trillions of parameters, represent a qualitative leap over traditional models. They have become increasingly popular in tasks involving complex reasoning [12] and deep semantic analysis [13]. However, as the number of parameters increases, LLMs face significant challenges in fine-tuning. These difficulties include enormous computational resource demands, longer training durations, and increased memory consumption, all of which severely impact the ability to comprehensively evaluate an LLM’s performance on specific tasks [14]. Additionally, the inability to quickly receive feedback and validate improvements during the development phase further complicates the process. Parameter-efficient fine-tuning (PEFT) [15] is a practical and effective knowledge transfer strategy that enables large pre-trained models to adapt to new tasks while significantly reducing computational and memory costs. Rather than updating all parameters, PEFT typically optimizes only a small subset. For example, by applying low-rank reparameterization techniques such as LoRA [16], introducing lightweight modules such as adapters [17], or incorporating soft prompts such as prefix tuning [18] and prompt tuning [19]. Recent work like AdapterHub [20] has unified these techniques under a generalized “adapter method” abstraction, further enhancing parameter efficiency and modular composability. This makes it particularly suitable for deployment in resource-constrained environments. PEFT has been widely adopted in natural language processing, where LLMs can be efficiently customized for specialized applications such as medical diagnosis, legal research, and customer service. In this work, we develop a novel PEFT framework for
arXiv:2504.05411v1 [cs.CL] 7 Apr 2025


personality detection tasks called PersLLM (Personality Large Language Model). Our intuition is that in traditional end-toend models, each time a new input is provided to the LLM, the model must repeat complex computations [21], such as feature extraction, forward propagation, and backpropagation. These steps involve extensive matrix operations and multi-layer computations, such as the self-attention mechanism in transformer architectures and gradient calculations, all requiring access to the vast parameters of the LLM, making it a highly computationally intensive process [22]. To address this issue, we introduce a novel dynamic memory layer after feature extraction. This layer leverages an indexing mechanism, a querying mechanism, and cache updates to store extracted features, eliminating the need for costly LLM computations during each inference. Besides the feature memory capability, another novelty lies in our choice of a lightweight neural network for the output layer. Compared to the LLM, these networks have fewer parameters and lower computational complexity, significantly reducing the computational cost of training. Unlike training from scratch, the downstream network only needs to learn task-specific relationships and patterns based on the features stored in the memory layer, further accelerating the training process. Additionally, since these output layers are replaceable, the framework gains adaptability. For different personality detection scenarios, we do not need to retrain the LLM. Instead, we can adjust the output network, selecting the appropriate one based on the performance of the individual output layers. Main Contributions: The PersLLM framework not only effectively mitigates the computational overhead and time costs associated with LLMs through PEFT, but also delivers more effective quantitative results. This introduces a novel strategy for fine-tuning LLMs. The key technical contribution is the use of a dynamic memory layer to store and update features, optimizing computational efficiency, while a flexible output network can quickly adapt to various personality detection scenarios. PersLLM has also fully demonstrated the immense potential of fine-tuning LLMs for this specific task. PersLLM outperforms the current state-of-the-art models on two benchmark datasets, setting a new standard for performance.
II. RELATED WORK
Classic Personality Detection: Some commonly used machine learning models such as SVM [23] and XGBoost [24] have shown excellent performance in classifying personality traits from social media data. Various deep neural networks also have also demonstrated their strengths in text-based personality detection tasks. A study using BiLSTM with an attention mechanism achieved notable results in detecting psychopathic traits in social media text [25]. Graph techniques capture the complex relationships between text and psychological traits by constructing graph structures. For example, both D-DGCN [26] and Semi-PerGCN [27] leverage graph convolutional networks to model the structure of user-generated content, with D-DGCN focusing on dynamic relationships between posts, and Semi-PerGCN integrating psycholinguistic features with
unsupervised learning. However, most of the above methods often struggle to capture deeper and contextual relationships within unstructured text.
Language Models for Personality Detection: BERT [28], known for capturing nuanced language patterns, has been fine-tuned for personality recognition [29], such as a study combined sentiment lexicons with the BERT model for multilabel personality detection [30]. Another study used a hybrid strategy to fine-tune RoBERTa for predicting continuous values of personality traits [31]. Recently, researchers have begun applying LLMs to personality classification tasks. PsyCoT [32] enhances the LLM by using psychological questionnaires as chain-of-thought steps, guiding the model to assess personality traits through a multi-turn dialogue process based on textual input; TAE [33] enhances smaller models by extracting knowledge from the LLM through text augmentation, improving post representation and labelling information by combining contrastive learning with semantic, emotional, and linguistic analysis. However, as the size of language models grows, finetuning becomes increasingly complex, leading researchers to favour prompt-based methods to reduce training costs. Finetuning LLMs for personality detection is even rarer [34], which may result in overlooking the most optimal approaches for this task. Our PersLLM differs from previous approaches as it introduces a PEFT method tailored for personality detection using LLMs. The LLM extracts rich semantic representations, which are stored in a dynamically updated memory layer, while training and prediction are focused on a lightweight and flexible output network. This approach significantly reduces training costs and enhances adaptability to tasks, making LLM fine-tuning feasible for personality detection.
III. METHODOLOGY
PersLLM is a novel PEFT framework (in Fig. 1) for personality detection tasks. Leveraging the powerful representation capabilities of the LLM, rich semantic information is extracted from text and stored in a memory layer for repeated access and dynamic updates. The subsequent training and inference processes are handled by a lightweight output network. This design eliminates the computational burden of repeatedly invoking the LLM’s large number of parameters, significantly reducing computational costs while maintaining performance.
A. Feature Extraction Layer
Under the PersLLM framework, we leverage Llama 3.1 [35] to extract high-dimensional representations from usergenerated text. Each user ui in the dataset has a collection of multiple posts Pi = {pi1, pi2, . . . , pim}. Since a user may have a large number of posts, they are processed in batches rather than as a single sequence. Each batch Pib ⊆ Pi consists of a subset of the user’s posts, where multiple batches are processed sequentially. Llama processes each batch Pib and generates a corresponding set of feature embeddings Eib = {eib1, eib2, . . . , eibk}, where each eibj represents the extracted embedding of the j-th


Fig. 1. The framework diagram of PersLLM. First, the LLM is utilized for feature extraction from posts, generating high-dimensional representations that are stored in a dynamic memory layer. Subsequent training is focused entirely on an output network for classification.
post in batch Pib. Consequently, instead of a single embedding set per user, we obtain multiple embeddings across batches, i.e., Ei = {Ei1, Ei2, . . . , EiB}, where B is the total number of batches for user ui. Llama incorporates the Grouped Query Attention (GQA) mechanism to improve efficiency when processing multiple user-generated inputs. GQA reduces the number of independent query heads by a factor of g, while keeping the full set of key and value heads. Specifically, the query representation is modified as Q′ ∈ Rs×(dk/g), where Q′ = W ′
Q · Eib, s is the total input size, dk is the original query dimensionality, and g is the number of query groups. The key and value matrices remain unchanged, with K ∈ Rs×dk and V ∈ Rs×dk . The attention computation becomes:
Attention(Q′, K, V ) = softmax Q′K⊤
pdk /g
!
V.
By reducing the dimensionality of queries while maintaining the expressiveness of key-value pairs, GQA lowers the computational overhead without compromising the ability to model long-range dependencies. This allows Llama to efficiently extract contextual representations from multiple posts, facilitating high-performance downstream processing at scale. After passing through multiple layers of GQA and feedforward neural networks, each batch Eib is transformed into its high-dimensional feature representation Hib = GQA(Eib). Consequently, instead of a single output representation, we obtain a sequence of feature representations across batches, denoted as Hi = {Hi1, Hi2, . . . , HiB}, where each Hib encapsulates the extracted features of the corresponding batch.
B. Dynamic Memory Layer
The dynamic memory layer consists of three core components: an indexing mechanism, a querying mechanism, and a cache update mechanism. Instead of recomputing Llamaextracted representations for every input, the Memory Layer
continuously updates its stored features based on recent inputs, leveraging locality-sensitive hashing (LSH) for efficient retrieval. Specifically, for each extracted feature batch Hib, the Memory Layer applies a LSH transformation to project it into a lower-dimensional space while preserving similarity relationships. LSH approximates nearest neighbor search by mapping similar feature embeddings into the same hash buckets, where the transformation is defined as hhash = sign(WLSHHib), with WLSH as a random projection matrix that preserves locality properties. The resulting binary vector hhash serves as the indexing key for fast retrieval. Using this hashed representation, the Memory Layer dynamically searches for the closest stored feature batch H∗ by minimizing a predefined distance metric:
H∗ = arg min
Hj∈M d(Hib, Hj ),
where M is the memory store and d(·, ·) is a similarity function such as cosine similarity:
S(Hib, Hj ) = Hib · Hj
||Hib|| · ||Hj|| .
To ensure adaptive retrieval, the Memory Layer dynamically determines whether H∗ can be reused based on a similarity threshold θ, ensuring that if S(Hib, H∗) ≥ θ, the stored representation is directly used. Otherwise, Llama recomputes the feature embeddings for the batch, and the updated representation is stored for future retrieval. Given limited storage capacity, the Memory Layer adopts an adaptive Least Recently Used (LRU) strategy, prioritizing frequently accessed embeddings while discarding the least recently used ones when memory is full:
M=
(
M ∪ {Hib}, if |M| < M ;
(M \ Holdest) ∪ {Hib}, if |M| ≥ M.


Here, M represents the memory capacity, and Holdest is the least recently accessed feature batch. By continuously adapting to new data, the Dynamic Memory Layer optimizes retrieval efficiency while maintaining relevance. Cached feature embeddings evolve dynamically based on recent queries, ensuring that frequently accessed personality representations remain available while outdated ones are discarded.
C. Replaceable Output Layer
Since the memory layer stores and retrieves embeddings at the batch-level, the output network efficiently transforms the retrieved feature embeddings into compact user-level representations without redundant recomputation. Given the batch-level feature embeddings Hi = {Hi1, Hi2, . . . , HiB} retrieved from the Memory Layer, the output network aggregates them into a final representation suitable for classification. To achieve this efficiently, we adopt a lightweight structure that processes the embeddings in sequence while maintaining adaptability to different architectures. A computationally efficient sequential model, such as a Gated Recurrent Unit (GRU), serves as an example to illustrate this process. The model iteratively refines an internal representation ht at each step t using the current batch embedding Hit and the prior state ht−1, ensuring that only necessary computations are performed. After processing all batches, the final output representation, denoted as hfinal, encapsulates the aggregated information across all stored embeddings. To maintain efficiency, the Replaceable Output Layer is designed to be lightweight, making it feasible to deploy alternative architectures such as Transformer-based encoders or convolutional networks without significant computational cost. The simplicity of this layer ensures seamless adaptability, enabling it to operate on top of cached feature embeddings without introducing additional complexity.
D. Personality Classification
The final feature representation, denoted as hfinal, obtained from the Replaceable Output Layer, serves as the input to the classification module. Since the output network is replaceable, the classification process remains independent of its specific architecture, ensuring adaptability to different model designs. The classification module performs both binary classification for the four MBTI dimensions and a 16-class classification for predicting the complete personality type. For the binary classification task, each MBTI dimension d ∈ {E/I, N/S, T /F, J/P } is treated as an independent classification problem. The model predicts a categorical outcome for each dimension, determining an individual’s preference along the four personality axes, where yˆ = (yˆE, yˆN , yˆT , yˆJ ) and each yˆd represents the predicted trait for the corresponding dimension. In addition to binary classification, the model also predicts the complete MBTI personality type as a single categorical variable. This is formulated as a 16-class classification problem, where each class corresponds to one
of the 16 MBTI personality types. The model assigns a probability distribution over all 16 personality types and selects the most probable one as the final prediction, where yˆ16 ∈ {INTJ, INTP, ENTJ, ENTP, INFJ, INFP, ENFJ, ENFP, ISTJ, ISFJ, ESTJ, ESFJ, ISTP, ISFP, ESTP, ESFP}.
IV. EXPERIMENTS
A. Experimental Setup
Datasets: To ensure a fair comparison with previous work, we selected the same two datasets – Kaggle and Pandora. The Kaggle dataset is sourced from the PersonalityCafe forum, an online community focused on discussions about personality types. This dataset contains posts from 8,675 users, with each user contributing approximately 45 to 50 posts. The posts cover a variety of topics, including psychology, personal experiences, and everyday discussions. Pandora is a larger corpus from the Reddit platform, which includes MBTI labels for 9,084 users. The number of posts per user varies from dozens to hundreds, and due to the diversity of the Reddit community, the content covers a broader range of topics compared to the Kaggle dataset. As shown in Table I, both datasets feature large-scale inputs, user-level aggregation, and significant class imbalance, particularly in the S/N and E/I dimensions. These factors increase the computational burden during training and inference and make it more challenging to capture long-range dependencies and coherent global structures across multiple user posts.
TABLE I STATISTICS OF THE KAGGLE AND PANDORA DATASETS IN TERMS OF THE SET DIVISION AND CLASS DISTRIBUTION.
Dataset Types Train Validation Test
Kaggle
E/I 1194 / 4011 409 / 1326 396 / 1339 S/N 610 / 4478 222 / 1513 248 / 1487 T/F 2410 / 2795 791 / 944 780 / 955 J/P 2109 / 3096 672 / 1063 653 / 1082 Posts 246794 82642 82152
Pandora
E/I 1162 / 4278 386 / 1427 377 / 1437 S/N 727 / 4830 208 / 1605 210 / 1604 T/F 3549 / 1891 1120 / 693 1182 / 632 J/P 2229 / 3211 770 / 1043 758 / 1056 Posts 523534 173005 174080
Implementation Details: We leveraged the “Llama-3.1-8BInstruct” model. For the output network, we primarily utilized GRU and flexibly replaced it with other neural networks based on task requirements. The feature vector length extracted by Llama and the input vector length of the output network were both standardized to 4096. To maintain lightweight computation, the GRU consists of only three hidden layers by default, and dropout regularization with a probability of 0.2 was applied to prevent overfitting. The hidden dimension of GRU output at each time step is 512. The model uses the Adam optimizer with a learning rate of 1 × 10−3. The loss function used is cross-entropy loss. To prevent information leakage, words that directly matched personality labels were removed during data preprocessing. The datasets were split


TABLE II COMPARISON OF PERSLLM WITH STATE-OF-THE-ART LANGUAGE MODEL-BASED BASELINES ON THE KAGGLE AND PANDORA DATASETS IN TERMS OF THE MACRO-F1 (%) SCORES ACROSS THE FOUR DIMENSIONS AND THEIR OVERALL AVERAGE (AVG).
Methods Kaggle Pandora
E/I S/N T/F J/P Avg E/I S/N T/F J/P Avg BERT 64.65 57.12 77.95 65.25 66.24 54.22 48.71 64.70 56.07 56.56 RoBERTa 61.89 57.59 78.69 70.07 67.06 54.80 55.12 63.78 55.94 57.41 Transformer-MD 66.08 69.10 79.19 67.50 70.47 55.26 58.77 69.26 60.90 61.05 PQ-Net 68.94 67.65 79.12 69.57 71.32 57.07 55.26 65.64 58.74 59.18 GPT-4 Turbo (Zero-shot) 68.86 54.69 80.10 66.93 67.65 57.38 51.47 71.75 62.29 60.72 GPT-4 Turbo (CoT) 66.63 61.85 77.23 60.80 66.62 60.97 57.14 66.39 56.92 60.36 DeepSeek-V3 (Zero-shot) 69.76 58.61 75.71 64.37 67.11 61.39 54.31 68.05 58.17 60.48 DeepSeek-V3 (CoT) 68.59 61.47 74.56 61.21 66.46 58.23 56.15 67.94 55.01 59.33 PsyCoT 66.56 61.70 74.80 57.83 65.22 60.91 57.12 66.45 53.34 59.45 TAE 70.90 66.21 81.17 70.20 72.07 62.57 61.01 69.28 59.34 63.05 PersLLM 76.71 75.55 85.11 75.96 78.33 68.89 66.72 73.70 68.55 69.47
into training, validation, and test sets using a 60%/20%/20% ratio. The results reported are averages of ten runs. Evaluation Metrics: Following previous work, we used Macro-F1 score as the primary evaluation metric, along with accuracy (ACC), precision (P), and recall (R) in the newly added scenarios for comprehensive evaluation. Baselines: We selected several widely-used pretrained language models for text classification, including fine-tuned BERT [36] and RoBERTa [37], as well as advanced LLMs like GPT-4 [38] and DeepSeek-V3 [39]. We also included more complex architectural designs based on different language models. The Transformer-MD [40] was chosen for its memory-augmented Transformer-XL architecture and dimension-specific attention, enabling effective integration of multi-post inputs for personality detection. The PQ-Net [41] model was selected for its dual-stream design that combines text and psychological questionnaires via cross-attention to extract key personality cues. The PsyCoT [32] was included for its chain-of-thought prompting based on questionnaire items, guiding the LLM through multi-turn reasoning to score each trait. The TAE [33] was chosen for leveraging LLM-generated text augmentations and label explanations with contrastive learning to capture psycholinguistic features.
B. Overall Results
Macro F1: The results comparing PersLLM with baselines in Macro-F1 scores are shown in Table II. PersLLM achieves the top performance across all four dimensions and the overall average. On the Kaggle dataset, the average score is 78.33%, surpassing the best existing scheme, TAE, by 6.26%. On the Pandora dataset, PersLLM achieves an average score of 69.47%, outperforming TAE by 6.42%. PersLLM also delivers outstanding results across all four dimensions. Compared to the baselines, PersLLM shows significant improvements in dimensions with severe class imbalance, particularly narrowing the performance gap between T/F and the other three dimensions. Compared to previously reported Transformer-based language models such as BERT and RoBERTa, the improvements observed in PersLLM indicate that increasing the number of parameters significantly enhances the model’s ability to
capture richer semantic representations and handle deeper linguistic nuances. We also compared PersLLM with several contemporary LLM-based approaches and found that PersLLM consistently outperforms methods that rely on prompting, such as zero-shot. Surprisingly, chain-of-thought (CoT) reasoning with step-by-step inference produced results that were even slightly worse than zero-shot performance. This finding suggests that the model’s capacity constraints limit its ability to further push performance boundaries, and the complex decomposition process in CoT can even introduce hallucinations and noise. When the model’s discriminative power is sufficiently strong, adjusting its parameters should be prioritized to break through its performance ceiling.
TABLE III THE PERFORMANCE COMPARISON OF THE MODELS WITHIN THE 16-CLASS EVALUATION FRAMEWORK.
Dataset Method Acc P R F1
Kaggle
BERT 34.64 16.29 14.43 15.87 RoBERTa 38.56 25.26 21.25 22.72 Llama 3.1 43.16 37.53 32.37 34.79 Llama 3.1 (LoRA) 47.24 44.15 36.62 39.43 PersLLM 46.54 45.54 39.60 41.61 PersLLM (GCN) 51.14 48.60 46.57 44.21 PersLLM (LSTM) 58.43 50.86 45.09 47.15
Pandora
BERT 27.40 5.03 8.49 5.65 RoBERTa 30.49 10.65 9.63 9.98 Llama 3.1 37.88 45.30 24.99 30.40 Llama 3.1 (LoRA) 39.51 46.38 28.71 32.88 PersLLM 42.67 60.62 31.18 37.25 PersLLM (GCN) 43.72 55.52 31.73 36.86 PersLLM (LSTM) 45.59 69.67 30.49 38.75
16-class Evaluation: We also developed a new 16-class evaluation framework designed to classify the 16 personality types of the MBTI. Unlike trait-based theories, such as the Big Five personality model [42] that describes personality through continuous trait dimensions, MBTI is grounded in a discrete typology theory. Due to MBTI’s emphasis on type distinctions in personality assessment, a specialized classification approach is required to address the challenges in practical applications and optimize costs. As shown in Table III, we selected BERT and RoBERTa


Fig. 2. The paired bar charts in a logarithmic scale for the computational resource usage comparison between our PersLLM, Llama 3.1 and its LoRA fine-tuning on the Kaggle dataset.
as our baselines, as they are widely used language models known for their strong performance on various classification tasks. However, both models performed poorly in the 16class classification scenario, even achieving near single-digit scores on some metrics for the Pandora dataset. This is primarily due to their lack of task-specific tuning for the 16class scenario. However, such adjustments often come at a high cost, especially for language models that require fullparameter updates or rely heavily on large-scale labeled data. We compared PersLLM with Llama 3.1 fine-tuned using LowRank Adaptation (LoRA) [43]. Although LoRA is also an efficient fine-tuning method, its performance is significantly worse across all metrics compared to the various configurations of PersLLM. In contrast, PersLLM can achieve better results by swapping the output network according to the scenario’s requirements. For instance, using LSTM on large and complex datasets helps learn more detailed features; or in more sensitive applications, such as medical diagnostics [44], using GCN can improve the detection of positive samples. Either replacing LSTM or GCN yields better results than the initial PersLLM, making it more suited for the 16-class classification task. This highlights the flexibility of the PersLLM framework.
Computational Costs Comparison: As shown in Fig. 2, we use paired bar charts to compare the performance of Llama 3.1, LoRA fine-tuning, and our PersLLM, incorporating different output networks such as CNN and GCN, across multiple computational dimensions on the Kaggle dataset, including FLOPs, training time, inference time, model parameters, GPU memory usage, and throughput. Due to the large scale of model parameters, we used a logarithmic scale for resource values. PersLLM demonstrates significant advantages in computational efficiency compared to Llama 3.1 and LoRA fine-tuning, with a notable reduction in FLOPs
Fig. 3. The line chart reflects the correlation between the performance of the output neural network and the overall performance of PersLLM. The test was conducted on the Pandora dataset.
and GPU memory usage. Additionally, PersLLM requires less training time while maintaining competitive inference times. Interestingly, despite its more compact architecture, PersLLM achieves higher throughput (samples per second), highlighting its excellent ability to balance computational efficiency and performance, effectively addressing the challenges of finetuning large models.
As previously mentioned, a key advantage of the PersLLM framework is its ability to assess overall model performance through the output network, thereby improving efficiency and reducing computational costs. As shown in Fig. 3, the independent performance of output layers such as GRU, RNN, and CNN on the Pandora dataset is almost identical to their performance when integrated into the PersLLM framework as output networks. This consistency indicates that output networks can reliably serve as proxies to predict PersLLM’s final results, allowing us to make stable performance estimates


Fig. 4. The heatmap for ablation study. It illustrates the performance of various key components of PersLLM on the Kaggle (left) and Pandora (right) datasets. Performance improves as the colour deepens.
without running the full model. Therefore, in optimization scenarios, we can first experiment with the output network to predict PersLLM’s outcomes, ensuring that we can accurately identify necessary adjustments before performing expensive and time-consuming full-scale model tuning.
C. Ablation Study
In the ablation study, we observed changes in the overall performance of PersLLM by replacing its key components. The results of the ablation analysis are visualized using a heatmap for a more intuitive representation, as shown in Fig. 4. The heatmap shows that both PersLLM and its variants with replaced components exhibit a significant performance drop on the Pandora dataset compared to the Kaggle dataset (indicated by lighter colours). Similarly, the performance of output modules with different layer configurations declines across all dimensions. This drop may be attributed to inherent differences between the two datasets, such as greater variability or more complex textual patterns in the Pandora dataset. The heatmap highlights the challenge of extending models optimized for one dataset to another without sacrificing performance. Due to the flexibility of PersLLM’s PEFT, as previously mentioned, it can quickly adapt to the Pandora dataset by replacing the output layer with other neural networks, such as GRU, without needing to retrain the entire model. Next, we removed the output layer and used fine-tuned Llama 3.1 with a simple linear layer for direct classification. The average F1 score drops by 11.41% on Kaggle and 9.59% on Pandora. The standalone GRU exhibits a more significant decline. Among the four specific dimensions, the largest decline is observed in the S/N dimension. This demonstrates that PEFT significantly enhances the model’s adaptability and its ability to capture task-specific nuances. To further investigate the impact of different backbone models, we replaced Llama with BERT and GPT-4. BERT performed significantly worse than Llama on both datasets. In contrast, GPT-4 outperformed Llama in most dimensions, demonstrating its strong generalization capability on MBTI personality data.
We also experimented with the output network of varying depths. Our observations show that stacking up to three hidden layers in the GRU maximizes the overall performance of PersLLM. However, adding a fourth layer leads to overfitting, causing a performance drop, highlighting the trade-off between model depth and generalization. Interestingly, the four-layer and three-layer models perform better on different dimensions, suggesting that neural networks of varying depths enable the framework to learn more subtle or specific features on certain dimensions. This points to a clear direction for optimizing model performance: rather than solely increasing depth, selectively adjusting the architecture for specific dimensions can better meet particular needs. This ability to rapidly tune parameters is a key advantage of our PersLLM architecture. This flexibility, combined with targeted fine-tuning, provides a more efficient approach to improving model performance without incurring the computational cost of retraining the entire model.
V. CONCLUSIONS
We proposed PersLLM, a novel PEFT framework that leverages LLMs for personality detection tasks. The LLM extracts rich representations, which are stored in a dynamic memory layer, avoiding the need for repeated feature computation. The subsequent training and prediction processes focus on a low-parameter output network, which can be replaced based on task requirements. This approach allows for flexible and lightweight fine-tuning of the LLM, significantly reducing computational costs. Experimental results demonstrate that PersLLM outperforms existing state-of-the-art methods by a large margin, proving its effectiveness and efficiency. Future work will incorporate multimodal data, such as images and audio, to enhance the framework’s scalability. Additionally, we will develop adaptive fine-tuning techniques that dynamically adjust the training effort based on task requirements, further improving the framework’s efficiency.


REFERENCES
[1] D. Cervone and L. A. Pervin, Personality: Theory and research. John Wiley & Sons, 2022. [2] T. Teng, H. Li, Y. Fang, and L. Shen, “Understanding the differential effectiveness of marketer versus user-generated advertisements in closed social networking sites: An empirical study of wechat,” Internet Research, vol. 32, no. 6, pp. 1910–1929, 2022. [3] E. J. Choong and K. D. Varathan, “Predicting judging-perceiving of myers-briggs type indicator (mbti) in online social forum,” PeerJ, vol. 9, p. e11382, 2021. [4] G. Jenifa, P. Sevanthi, K. Karthika, V. S. Pandi, D. Arumugam et al., “An effective personality recognition model design using generative artificial intelligence based learning principles,” in 2024 International Conference on Computing and Data Science (ICCDS). IEEE, 2024, pp. 1–6.
[5] N. K. Iyortsuun, S.-H. Kim, M. Jhon, H.-J. Yang, and S. Pant, “A review of machine learning and deep learning approaches on mental health diagnosis,” in Healthcare, vol. 11, no. 3. MDPI, 2023, p. 285. [6] M. Nirmala, B. Rajalakshmi, S. M. Sarvepally, S. Mandava, and M. Megana, “Personality detection for recruitment using machine learning,” in International Conference on Communications and Cyber Physical Engineering 2018. Springer, 2024, pp. 399–406. [7] Q. Zhao, Y. Xia, Y. Long, G. Xu, and J. Wang, “Leveraging sensory knowledge into text-to-text transfer transformer for enhanced emotion analysis,” Information Processing & Management, vol. 62, no. 1, p. 103876, 2025. [8] L. Shen, Y. Long, X. Cai, I. Razzak, G. Chen, K. Liu, and S. Jameel, “Gamed: Knowledge adaptive multi-experts decoupling for multimodal fake news detection,” in Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, 2025, pp. 586–595. [9] J. Killian Jr and R. Sun, “Detecting big-5 personality dimensions from text based on large language models,” in International Conference on Deep Learning Theory and Applications. Springer, 2024, pp. 264–278. [10] Q. Lu, X. Sun, Y. Long, Z. Gao, J. Feng, and T. Sun, “Sentiment analysis: Comprehensive reviews, recent advances, and open challenges,” IEEE Transactions on Neural Networks and Learning Systems, 2023. [11] Q. Lu, X. Sun, Y. Long, X. Zhao, W. Zou, J. Feng, and X. Wang, “Multimodal dual perception fusion framework for multimodal affective analysis,” Information Fusion, vol. 115, p. 102747, 2025. [12] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, “Large language models: A survey,” arXiv preprint arXiv:2402.06196, 2024.
[13] Y. Zhu, X. Shen, and R. Xia, “Personality-aware human-centric multimodal reasoning: A new task, dataset and baselines,” arXiv preprint arXiv:2304.02313, 2023.
[14] H. Li, C. Zhu, Y. Zhang, Y. Sun, Z. Shui, W. Kuang, S. Zheng, and L. Yang, “Task-specific fine-tuning via variational information bottleneck for weakly-supervised pathology whole slide image classification,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 7454–7463.
[15] T. Hui, Z. Zhang, S. Wang, W. Xu, Y. Sun, and H. Wu, “Hft: Half finetuning for large language models,” arXiv preprint arXiv:2404.18466, 2024. [16] Y. Wu, Y. Xiang, S. Huo, Y. Gong, and P. Liang, “Lora-sp: streamlined partial parameter adaptation for resource efficient fine-tuning of large language models,” in Third International Conference on Algorithms, Microchips, and Network Applications (AMNA 2024), vol. 13171. SPIE, 2024, pp. 488–496. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, “Towards a unified view of parameter-efficient transfer learning,” arXiv preprint arXiv:2110.04366, 2021.
[18] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for generation,” arXiv preprint arXiv:2101.00190, 2021.
[19] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, “Visual prompt tuning,” in European Conference on Computer Vision. Springer, 2022, pp. 709–727. [20] C. Poth, H. Sterz, I. Paul, S. Purkayastha, L. Engl ̈ander, T. Imhof, I. Vuli ́c, S. Ruder, I. Gurevych, and J. Pfeiffer, “Adapters: A unified library for parameter-efficient and modular transfer learning,” arXiv preprint arXiv:2311.11077, 2023.
[21] T. N. Sainath, Y. He, A. Narayanan, R. Botros, R. Pang, D. Rybach, C. Allauzen, E. Variani, J. Qin, Q.-N. Le-The et al., “An efficient streaming non-recurrent on-device end-to-end model with improvements to rare-word modeling.” in Interspeech, vol. 8, 2021, pp. 1777–1781.
[22] S. Dave, R. Baghdadi, T. Nowatzki, S. Avancha, A. Shrivastava, and B. Li, “Hardware acceleration of sparse and irregular tensor computations of ml models: A survey and insights,” Proceedings of the IEEE, vol. 109, no. 10, pp. 1706–1752, 2021. [23] R. Krishna, R. Teja, N. Neelima, and N. Peddi, “Advanced machine learning models for depression level categorization using dsm 5 and personality traits,” Procedia Computer Science, 2024. [24] S. A. Salasa and W. Maharani, “Personality detection of twitter social media users using the support vector machine method,” Jurnal Sistem Komputer dan Informatika (JSON), 2022.
[25] J. Asghar, S. Akbar, M. Z. Asghar, B. Ahmad, M. S. Al-Rakhami, and A. H. Gumaei, “Detection and classification of psychopathic personality trait from social media text using deep learning model,” Comput. Math. Methods Medicine, vol. 2021, pp. 5 512 241:1–5 512 241:10, 2021. [26] T. Yang, J. Deng, X. Quan, and Q. Wang, “Orders are unwanted: dynamic deep graph convolutional network for personality detection,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 13 896–13 904. [27] Y. Zhu, Y. Xia, M. Li, T. Zhang, and B. Wu, “Data augmented graph neural networks for personality detection,” in AAAI Conference on Artificial Intelligence, 2024.
[28] A. Alsuhaibani, H. Zogan, I. Razzak, S. Jameel, and G. Xu, “Idofew: intermediate training using dual-clustering in language models for few labels text classification,” in Proceedings of the 17th ACM International Conference on Web Search and Data Mining, 2024, pp. 18–27.
[29] D. Jain, A. Kumar, and R. Beniwal, “Personality bert: A transformerbased model for personality detection from textual data,” in Proceedings of international conference on computing and communication networks: ICCCN 2021. Springer, 2022, pp. 515–522. [30] Z. Ren, Q. Shen, X. Diao, and H. Xu, “A sentiment-aware deep learning approach for personality detection from text,” Inf. Process. Manag., vol. 58, p. 102532, 2021. [31] R. Wang and K. Sun, “Continuous output personality detection models via mixed strategy training,” ArXiv, vol. abs/2406.16223, 2024. [32] T. Yang, T. Shi, F. Wan, X. Quan, Q. Wang, B. Wu, and J. Wu, “Psycot: Psychological questionnaire as powerful chain-of-thought for personality detection,” arXiv preprint arXiv:2310.20256, 2023.
[33] L. Hu, H. He, D. Wang, Z. Zhao, Y. Shao, and L. Nie, “Llm vs small model? large language model based text augmentation enhanced personality detection model,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18 234–18 242. [34] L. Shen, Y. Long, X. Cai, G. Chen, Y. Wang, I. Razzak, and S. Jameel, “Ll4g: Self-supervised dynamic optimization for graph-based personality detection,” arXiv preprint arXiv:2504.02146, 2025.
[35] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., “The llama 3 herd of models,” arXiv preprint arXiv:2407.21783, 2024.
[36] S. S. Keh, I. Cheng et al., “Myers-briggs personality classification and personality-specific language generation using pre-trained language models,” arXiv preprint arXiv:1907.06333, 2019.
[37] Y. Liu, “Roberta: A robustly optimized bert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.
[38] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023.
[39] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., “Deepseek-v3 technical report,” arXiv preprint arXiv:2412.19437, 2024.
[40] F. Yang, X. Quan, Y. Yang, and J. Yu, “Multi-document transformer for personality detection,” in Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 16, 2021, pp. 14 221–14 229. [41] F. Yang, T. Yang, X. Quan, and Q. Su, “Learning to answer psychological questionnaire for personality detection,” in Findings of the Association for Computational Linguistics: EMNLP 2021, 2021, pp. 1131–1142.
[42] B. De Raad, The big five personality factors: the psycholexical approach to personality. Hogrefe & Huber Publishers, 2000. [43] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.
[44] J. G. D. Ochoa and F. E. Mustafa, “Graph neural network modelling as a potentially effective method for predicting and analyzing procedures based on patients’ diagnoses,” Artificial Intelligence in Medicine, vol. 131, p. 102359, 2022.