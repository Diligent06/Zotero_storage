Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped
Online Reinforcement Learning
Dongchi Huang, Tianle Zhang*, Yihang Li, Ling Zhao, Jiayi Li, Zhirui Fang, Chunhe Xia, Lusong Li, and Xiaodong He
Abstract— Dexterous hand manipulation in real-world scenarios presents considerable challenges due to its demands for both dexterity and precision. While imitation learning approaches have thoroughly examined these challenges, they still require a significant number of expert demonstrations and are limited by a constrained performance upper bound. In this paper, we propose a novel and efficient ImitationBootstrapped Online Reinforcement Learning (IBORL) method tailored for robotic dexterous hand manipulation in real-world environments. Specifically, we pretrain the policy using a limited set of expert demonstrations and subsequently finetune this policy through direct reinforcement learning in the real world. To address the catastrophic forgetting issues that arise from the distribution shift between expert demonstrations and real-world environments, we design a regularization term that balances the exploration of novel behaviors with the preservation of the pretrained policy. Our experiments with real-world tasks demonstrate that our method significantly outperforms existing approaches, achieving an almost 100% success rate and a 23% improvement in cycle time. Furthermore, by finetuning with online reinforcement learning, our method surpasses expert demonstrations and uncovers superior policies. Our code and empirical results are available in https://hggforget.github. io/iborl.github.io/.
I. INTRODUCTION
Robotic manipulation in real-world scenarios [27]–[30], which necessitates the utilization of raw sensory observations as inputs to generate corresponding robotic actions, presents a longstanding challenge due to the requirement for precise hardware and efficient algorithms. Given that our environment is predominantly designed for human hands, dexterous hand manipulation [13] emerges as a critical component of robotic systems intended for human-centric environments. Compared with other end effectors, the dexterous hand has a larger action and state space due to its higher degree of freedom, which greatly increases the complexity of the dexterous hand manipulation tasks. Consequently, achieving human-level dexterity in robotic hand manipulation tasks remains an unresolved challenge. Recent advancements [18]–[20] in robotic dexterity, particularly in the control of highly articulated hands, have
Dongchi Huang and Chunhe Xia are affiliated with Beihang University, Beijing, China. Email: {zy2306316,xch}@buaa.edu.cn.
Tianle Zhang*, Yihang Li, Ling Zhao, Lusong Li, and Xiaodong He are associated with JD Explore Academy, Beijing, China. Email: tianle-zhang@outlook.com; {liyihang18, zhaolin120, lilvsong, hexiaodong}@jd.com.
Jiayi Li is affiliated with Beijing Jiaotong University, Beijing, China. Email: lijiayi140@jd.com.
Zhirui Fang is associated with Tsinghua University, Beijing, China. Email: fangzhirui.2@jd.com.
yielded significant progress. These approaches predominantly rely on imitation learning, which frames the policy learning problem within a supervised learning framework. Specifically, imitation learning first collects data by teleoperating robotic hands to execute the tasks. Subsequently, the gathered data is utilized to train policy networks that minimize the discrepancy between the robot’s actions and the demonstrated actions. Nevertheless, the imitation learning methods [3], [22] encounter the substantial costs associated with acquiring expert demonstrations. Furthermore, they are based on the assumption that human demonstrations are optimal, which constrains their performance to the quality of these demonstrations. Real-world robotic reinforcement learning [21] presents a promising solution to the aforementioned challenges. In contrast to imitation learning, reinforcement learning learns a policy through extensive trial and error, enabling the generation of policies that can surpass expert performance without relying on expert demonstrations. Although the reinforcement learning methods offer significant advantages, they face challenges in real-world training [2]; for instance, they require algorithms that are sample-efficient in handling highdimensional inputs, such as onboard perception, and supporting easy specification of rewards and resets. Moreover, implementations of online training systems severely affect the efficiency and performance of reinforcement learning. Despite this, there are still some real-world reinforcement learning works [2], [8]–[12], but most of them use end effectors in the form of grippers instead of dexterous hands. Because dexterous hands bring problems with high action space and state space, reinforcement learning methods will be very inefficient or even impossible to work. Based on the above discussion, we propose IBORL, a method specifically designed to acquire hand manipulation skills directly in real-world environments. Our method entails the creation of a system that enhances automation and efficiency in real-world robotic reinforcement learning for dexterous hand manipulation while ensuring compatibility across diverse tasks. This system comprises data collection through teleoperation, reward annotation, reward detectors, high-performance robotic controller and distributional training architecture. Moreover, we propose an algorithm that leverages imitation learning to enhance sample efficiency and minimize unnecessary exploration during the online reinforcement learning process. In this algorithm, prior to engaging in online reinforcement learning, we pretrain the policy using behavior cloning to reduce extraneous explo
arXiv:2503.04014v1 [cs.RO] 6 Mar 2025


ration. Subsequently, we implement symmetric sampling to further augment efficiency within the reinforcement learning framework. To mitigate the issue of catastrophic forgetting associated with an untrained critic, we have devised a regularization mechanism within the reinforcement learning framework. This mechanism adaptively balances reward maximization with the preservation of the pretrained policy, proving essential for optimal performance as evidenced by our empirical results. Our experiments demonstrate that our method outperforms existing imitation learning methods, achieving a 1.6x increase in success rate and improved cycle time efficiency. Moreover, by leveraging the pretrained policy, our approach successfully completes several tasks that traditional real-world robotic reinforcement learning methods find challenging. In summary, the primary contributions of this paper are as follows:
• We establish a system specifically designed for dexterous hand manipulation, which enhances the automation, efficiency, and compatibility of online reinforcement training. • We introduce a novel method, IBORL, that utilizes imitation learning to tackle the exploration and efficiency challenges posed by the expansive action space in robotic hands. • We develop a regularization mechanism to mitigate the issues of catastrophic forgetting associated with an untrained critic in the online reinforcement learning process. • Our empirical results in real-world tasks demonstrate that our approach outperforms existing methods, achieving a near 100% success rate and a 23% improvement in cycle time.
II. PROBLEM FORMULATION
As shown in Fig. 1, this paper investigates a dexterous hand manipulation (DHM) problem in the real world, where a robot with arms and dexterous hands needs to complete grasping or manipulating tasks, such as grabbing a cup, picking up a barcode scanner, etc. Specifically, the robot uses Realsense D435 and ZED2 as chest and head cameras respectively. The chest camera perceives a small core manipulation area, and the head camera captures a more comprehensive view. Meanwhile, Realman RM65-B and Inspire RH56BFX are used as the robot’s arm and dexterous hand, respectively. The robot mainly uses RGB images o = {oc, oh} from the chest and head cameras for visual perception, and adds robot proprioception to control robot arm and hand movement to complete the specified task. The robot proprioception p = [pa, ph] includes the end pose pa = [x, y, z, φ, θ, ψ] of the arm and the bending angle ph = [ f1, ... f6] of the fingers. The purpose is to find a policy to generate an action a = [aa, ah] to complete the task according to o and p in each time step. The action consists of six-dimensional arm end pose aa and six-dimensional finger bending angle ah. This DHM problem can naturally be formulated as a Markov decision process [1] with sparse reward in the
reinforcement learning framework. At timestep t, a robot can obtain a state st = {ot, pt} from the cameras and proprioception. Then, the robot based on the state generates an appropriate action at for finishing a specified task. A round ends by continuously making decisions and outputting actions to interact with the real-world environment until the task is completed or the maximum time step is reached. Before the end of each round, the robot accrues no reward, registering a value of 0. Once a round is completed, the robot is then assigned a binary sparse reward R ∈ {0, 1}. Here, R = 0 designates the failure of the task, while R = 1 indicates its successful completion. This paper aims to design an optimal policy π : s → a for maximizing the expected accumulated discounted return E[PT
t=0 γtRt], where γ ∈ [0, 1] is a discount factor and T is the time horizon. From the problem definition, we can see that the state space and action space of this problem are huge, and the rewards are extremely sparse. Direct real-machine reinforcement learning will be inefficient and difficult to explore effective solutions. Therefore, this paper focuses on pretraining first, strengthening on the basis of imitation, and efficiently finding the optimal policy.
Realman RM65-B
ZED2
Realsense D435
Inspire RH56BFX
Fig. 1: Illustration of Robotic Dexterous Hand Manipulation Task. The robot is assigned the task of grasping a cup, utilizing image observations from the ZED2 and Realsense D435 cameras as inputs to control its arm and hand for for task completion. The robot receives a reward exclusively upon successfully grasping the cup.
III. METHOD
In this section, we present IBORL, a novel method that tackles the challenges of exploration and efficiency in online reinforcement learning by establishing a high-performance online training system and an efficient online reinforcement learning algorithm.
A. Overall Design of IBORL
System Design. Our training system constructs a comprehensive pipeline that includes expert demonstration collection, reward annotation, task status detection, robotic


Actor Node
Environment
Model Parameters
Success Episode
Online Transitions
Actor
trivial data
high-quality data
demos
high-quality data
Learner Node
Pretrained Policy
Critic
Critic
Ensemble
Regularization
Reward
Equal Sample
10Hz
MLP
Encoder
Pretrained ResNet
Robot Server Sensors Reward Classifier
Proprioception
Transitions
Action Observation
......
Cam_body Cam_head
Fig. 2: Overview of IBORL. This figure illustrates the architecture of our system, which comprises two primary components: the actor process, the learner process. These components communicate asynchronously to facilitate efficient data flow. The actor process receives updated policy parameters from the learner process, interacts with the environment, and sends collected interaction data to the replay buffers. The learner process samples data evenly from two replay buffers and updates the policy using IBORL. During the policy update, the ensemble of critics provides the reinforcement learning loss, while the pretrained teacher policy regularizes the actor to prevent forgetting.
control, and an online training architecture. A failure in any component can result in an underperforming policy. Specifically, we teleoperate the robotic hand to successfully execute tasks and record each episode. Subsequently, we automatically replay these episodes and annotate the frames that signify successful task completion. Utilizing the annotated data, we train a binary classifier to ascertain whether a task has been performed successfully. This binary classifier serves as the reward detector during the online reinforcement learning process. Building on prior efforts, we implement our online training architecture. To facilitate efficient online reinforcement learning, we separate the actor and learner into two distinct threads. The actor infers actions to interact with the robot controller and transmits the collected transitions to the learner. Meanwhile, the learner updates the policy based on the provided data and periodically refreshes the actor’s policy. This distributed architecture not only decouples policy learning from environmental interactions but also maintains a consistent control frequency. Learning Algorithm. To address the exploration and efficiency challenges inherent in online reinforcement learning within real-world scenarios, our algorithm comprises three key components: imitation pretraining, symmetric sampling, and imitation regularization. Specifically, we first pretrain
the policy by imitating expert demonstrations. Subsequently, we finetune the pretrained policy using the RLPD algorithm [3], which utilizes symmetric sampling between expert demonstrations and online data. This approach has demonstrated sample efficiency and achieved reliable performance in real-world reinforcement learning applications [2]. However, naive reinforcement learning from a pretrained policy exhibits a forgetting behavior in the policy, resulting in performance degradation [4]. To address this challenge, we incorporate a regularization term into the policy objective to adaptively balance reinforcement learning and pretrained policy.
B. Real-world Robotic Reinforcement Learning System
Data Collection via Teleoperation Unlike other robotic systems, teleoperation via keyboard is impractical. We control the robotic hand using electromagnetic field (EMF) gloves [13] that track finger movements and monitor the six degrees of freedom (6-DoF) of the wrist pose based on the SLAM algorithm. This teleoperation device enables us to manipulate the robotic hand as effortlessly as we do our own, thereby facilitating task execution. During task execution, we record camera images captured from the robot’s chest and head, along with the end-effector pose and the angles of the


robot’s fingers. These data serve as expert demonstrations and constitute the training data for the reward classifier after being annotated with our automatic scripts.
Reward Specification with Binary Classifiers Reward specification through hand design is often infeasible, as certain forms of success status are challenging to define using explicit rules. Consequently, we employ binary classifiers to ascertain whether a task has been performed successfully. Specifically, we train binary classifiers with the annotated data to predict rewards based on image observations. Although binary rewards may encounter issues related to sparsity, they can be easily adapted to various tasks, thereby circumventing the substantial effort required to design reward functions.
High-performance Robot Controller In particular, we implement a high-performance robot controller that employs gRPC instead of HTTP, diverging from conventional practices. Compared to controllers implemented with HTTP, our controller receives actions from the actor and returns observations post-execution with reduced latency and enhanced stability.
Distributed Architecture We separate the actor and learner into two distinct threads. The learner maintains both the online replay buffer and the demonstration replay buffer, sampling batches from these buffers to update the policy. The actor oversees the robot controller, inferring actions from the current observations and executing them accordingly. At each step, the current transition is transmitted to the online replay buffer within the learner. Notably, when an episode is deemed successful, it is considered a high-quality episode and subsequently transferred to the demo replay buffer in the learner. During this process, the learner updates the policy based on the provided data and periodically refreshes the actor’s policy.
C. Expert Behavior Learning From Prior Data
Online Reinforcement learning from scratch in real-world environments faces challenges related to exploration and sample efficiency [23]. To address these issues, we pretrained the policy through behavior cloning [24] of expert demonstrations. Specifically, we collected 30 demonstrations via teleoperation and employed behavior cloning with the following objective:
LBC = E(se,ae)∼τe ∥ae − πBC (se)∥2 (1)
where, τe represents expert demonstrations, se and ae denote the corresponding state and action in expert demonstrations, and πBC signifies the learned behavior cloning policy.
D. Regularized Online Reinforcement Learning
Although reinforcement learning from a pretrained policy can significantly alleviate exploration issues, utilizing reinforcement learning with an untrained critic may lead to catastrophic forgetting [25], [26]. As seen in Eq.(2), we address this issue by incorporating a regularization term to balance
Algorithm 1 IBORL
1: Initialization: Randomly initialize Critic parameters θi (set target parameters θ′
i = θi) for i = 1, 2, . . . , E.
2: Load pretrained Actor parameters φ′, and initialize Actor parameters φ = φ′. 3: Set the discount factor γ, temperature α, ensemble size E, UTD ratio G, and critic exponential moving average (EMA) weight ρ. 4: Determine the number of Critic targets to subset, Z ∈ {1, 2}.
5: Initialize an empty replay buffer R. 6: Initialize the buffer D with offline data. 7: while True do
8: for g = 1, . . . , G do
9: Sample a minibatch bR of size N
2 from R.
10: Sample a minibatch bD of size N
2 from D. 11: Combine bR and bD to form a batch b of size N. 12: Sample a set Z of Z indices from {1, 2, . . . , E}. 13: Compute target:
y = r + γ im∈iZn Qθ′
i (s′, a ̃′)
!
,  ̃a′ ∼ πφ(·|s′).
14: for i = 1, . . . , E do
15: Update θi by minimizing the loss:
L= 1
N
N X
j=1
(y − Qθi (s, a))2.
16: end for
17: Update target networks: θ′
i ← ρθ′
i + (1 − ρ)θi. 18: end for
19: Sample a minibatch bR of size N
2 from R.
20: Sample a minibatch bD of size N
2 from D. 21: Combine bR and bD to form a batch b of size N. 22: For batch bR, compute:
λ(φ) = 1
N
N X
j=1
1 E
E X
i=1
IQθi (s,πφ′ (s))>Qθi (s,πφ(s)).
23: For batch bD, compute:
LBC = 2
N
N X2
i=1
βλ(φ)∥ae − π(se)∥2.
24: Update φ by maximizing the objective:
1 E
E X
i=1
Qθi (s, a ̃) − α log πφ(a ̃|s)+LBC, a ̃ ∼ πφ(·|s).
25: end while
the exploration of novel behaviors and the exploitation of expert behaviors.
LRL = (1−λ(π))E(s,a)∼Dβ [Q(s, a)]−αλ(π)E(se,ae)∼T e ∥ae−π(se)∥2. (2) Here, Q(s, a) denotes the Q-value derived from the critic employed in actor-critic policy optimization. The parameter α represents a fixed weight, while λ(π) is a policy-dependent


adaptive weight that regulates the contributions of the two loss terms. Dβ refers to the replay buffer utilized for online rollouts. Since the balancing coefficient λ(π) between the task reward and regularization significantly affects the algorithm’s performance, it is crucial to establish a principled method for its determination. While previous studies [5]–[7] employ hand-tuned schedules or hyperparameter searches for λ(π), we propose an adaptive scheme to ascertain λ(π):
λ(π) = E(s,·)∼Dβ
hIQ(s,πBC (s))>Q(s,π(s))
i . (3)
Here, λ(π) represents the extent to which the pretrained policy outperforms the current policy. A higher value of λ(π) indicates that greater effort should be devoted to regularization.
E. Online Training Algorithm
Here, in Algorithm 1, we present the pseudo-code for our method. The elements highlighted in red indicate the loading of the pretrained actor parameters at the beginning of the reinforcement learning process, while those in blue illustrate the calculation of the regularization term. The key factors of our algorithm can be summarized as follows:
• Line 2: Pretrain the reinforcement learning policy using imitation learning. • Lines 9-10 and 19-21: Employ a symmetric sampling approach to integrate online and demonstration data. • Lines 8-18: Implement high UTD updates to ensure optimal efficiency. • Lines 22-24: Apply regularization with the pretrained policy.
IV. EXPERIMENTS
In this section, we discuss our experiments aimed at evaluating the IBORL method. We first outline our experimental settings. We then present our experimental results and conduct an ablation analysis to investigate the effects of various components.
A. Experimental Settings
In our experiments, we deploy our policies on the Inspire robot, which includes a dexterous hand with six degrees of freedom, to perform various object manipulation tasks using its right arm and hand. For all tasks, the observation space consists of camera images captured from the robot’s chest and head, along with proprioceptive information, such as the end-effector pose and the angles of the robot’s fingers. The action space corresponds to the proprioceptive information found in the observation space. Specifically, the end-effector pose is represented as the relative position with respect to its initial position at the beginning of the task, comprising 3D Cartesian displacement and 3D rotational deviation using axis-angle representation. During the policy deployment phase, we trained a binary classifier to serve as a reward detector. This classifier analyzes images to predict whether the current state successfully
accomplishes the task. When the reward detector identifies the current state as successful, the episode concludes, and we manually reset the environment. Finally, we present the evaluation tasks in Figure 3, accompanied by detailed descriptions provided below: Cup Taking. In this task, the robot is required to grasp and lift a cup from a designated position on a table. The objective is to ensure a secure grip while smoothly elevating the cup. This task assesses the robot’s precision in handling lightweight objects and its ability to adjust grip strength. Scanner Taking. In this task, the robot must pick up a scanner using a specified method. The challenge arises from the scanner’s irregular shape and designated grasping mode, necessitating careful planning of the grip and lifting strategy. This task evaluates the robot’s capability to handle objects with complex geometries and its adaptability in mastering the specified grasping mode. Cube Taking. In this task, the robot is tasked with grasping and lifting a cube by its hand. Unlike the cup, the cube is smaller and heavier, thus requiring the robot to grasp the cube with its fingers. This task assesses the robot’s dexterity in manipulating its fingers. Loopy Taking. The robot is tasked with grasping and lifting a loopy doll. This endeavor presents unique challenges due to the doll’s unconventional shape and the soft, pliable characteristics of the material.
B. Experimental Results
Now, we discuss our empirical results for the real-world evaluation. For each task, we report success rate, cycle time and detailed task parameters. The training time includes all scripted motion, policy rollouts, intended stops, as well as onboard computation which is carried on a single NVIDIA RTX 4090 GPU. Unless otherwise noted, all results are based on 40 evaluation trials. To illustrate IBORL’s superiority over existing methods, we compare IBORL with the following baselines: • BC: Behavior Cloning (BC) is an imitation learning method in which an agent learns to replicate an expert’s behavior by training on expert demonstrations. This approach employs supervised learning to approximate the expert’s policy. • SERL: Sample-Efficient Robotic Reinforcement Learning (SERL) is an open-source software framework that provides a high-quality reinforcement learning implementation specifically designed for real-world robotic learning, supporting both image observations and demonstrations. • IBORL†: IBORL† is a ablated version of IBORL, where IBORL† directly applies reinforcement learning to the pretrained policy without any regularization. Results. As illustrated in Table.II, Our approach achieve competitive performance across all tasks mentioned above. Compared to BC, which serves as the initial policy for IBORL, IBORL outperforms BC by a significant margin, particularly in the tasks of Cup Taking and Cube Taking. In the Cup Taking task, IBORL not only increases the success


Fig. 3: Illustration of the robot performing each task with our method. For each task, the robot starts from a predefined initial pose and subsequently performs the task by utilizing the proprioceptive state and image observations. The process continues until the robot receives a reward for successfully completing the task, as indicated within the green box.
Tasks Demos Image Inputs Reset Training Time Classifier
Cup Taking 40 Chest Camera, Head Camera Fixed 59 mins 98% Cube Taking 36 Chest Camera, Head Camera Fixed 72 mins 96% Scanner Taking 31 Chest Camera, Head Camera Random 41 mins 95% Loopy Taking 36 Chest Camera, Head Camera Random 42 mins 99%
TABLE I: Task Parameters. During demo collection for both BC and RL, as well as online training, each episode’s initial end-effector pose resets uniformly at random within a fixed region
Method Cup Taking Cube Taking Scanner Taking Loopy Taking
SR (%) Cycle Time SR (%) Cycle Time SR (%) Cycle Time SR (%) Cycle Time
BC 70.0 122.21 7.5 129.67 80.0 204.25 95.0 116.11 SERL 0 N/A 0 N/A 0 N/A 0 N/A IBORL† 0 N/A 0 N/A 0 N/A 100.0 101.8 IBORL (Ours) 87.5 68.51 90.0 110.55 90.0 147.19 100.0 115.55
TABLE II: Experimental results comparing our method with baselines. † Ablated version without regularization terms. SR: Success Rate, Cycle Time: Total steps for complete the task.
rate from 70% to 87.5% but also reduces the cycle time required to complete the task by half. This indicates that the policy can be further optimized beyond expert performance through efficient exploration in reinforcement learning. Meanwhile, in the Cube Taking task, which requires pre
cise finger control, BC achieves only a 7.5% success rate and an average cycle time of 129.67. This highlights that naive imitation learning struggles to master precise manipulation tasks due to compounding error issues. In contrast, IBORL effectively masters the Cube Taking task, achieving a 90%


success rate and an average cycle time of 110.55 through extensive interactions with the real-world environment. This outcome demonstrates the superiority of real-world robotic reinforcement learning, as it can progressively refine its policy based on environmental feedback. Conversely, the SERL method, which focuses on real-world robotic reinforcement learning, struggles to achieve task completion due to the vast exploration space resulting from the large action space of dexterous hands and the extensive observation space derived from high-dimensional images. While IBORL benefits from imitation pretraining, successfully mastering dexterous hand manipulation skills, SERL faces significant challenges in this regard. Ablation Analysis. As illustrated in Table II, without regularization, IBORL† fails to complete three tasks during real-world training. We find that reinforcement learning encounters difficulties in mastering precise and dexterous hand behaviors, such as grasping the cup, pinching the cube, and manipulating the scanner. Specifically, during the reinforcement learning process, the robotic hand tends to shake, causing the cup to be swept away and the scanner to be knocked down. Consequently, the robot is unable to complete the tasks, resulting in the absence of reward signals and leading to continued underperformance. While the Loopy Taking task, being the only one that does not require precise and complicated hand behaviors, is successfully accomplished by IBORL†.
C. Discussions
Comparison with ACT. We compare our method with Action Chunking with Transformers (ACT) to further demonstrate its superiority. ACT is a notable imitation learning algorithm that addresses compounding error issues in imitation learning by employing the transformer architecture, action chunking, and temporal ensemble techniques, resulting in significant performance improvements. As depicted in Fig.4, IBORL demonstrates competitive performance compared to the state-of-the-art imitation learning algorithm, ACT. Notably, in the Cup Taking task, IBORL accomplishes half the total steps of ACT while maintaining a comparable success rate. This indicates that reinforcement learning can surpass the limitations of expert demonstrations to derive a more optimal policy through adequate trial and error. Conversely, in the Cube Taking task, although IBORL achieves the same success rate, it is slower than ACT by an average of 20 steps. We attribute this issue to the underperforming backbone utilized in our method. In both IBORL and BC, we employ an ImageNet pre-trained ResNet-10 as the vision backbone for the policy network. In contrast, ACT utilizes the most advanced transformer architecture. When comparing BC to ACT, it is evident that although both ACT and BC share the same objective, they yield completely different performance outcomes due to the differing backbones.
Regularization Analysis. To demonstrate how the regularization mechanism functions within the reinforcement learning process, we present Fig. 5, illustrating the changing
Fig. 4: Comparison with ACT. In our evaluation, we conducted 40 trials per task, recording both the success rate and the average number of steps taken by the robot to achieve success in each task.
trends of the balancing coefficient and the success rate. Based on this figure, we analyze the relationship between these two metrics.
Fig. 5: Regularization Analysis. This figure illustrates the changing trend of the balancing coefficient λ(π) and the success rate throughout the training process. A higher value of BC weight indicates a greater extent to which the pretrained policy outperforms the current policy.
As illustrated in Fig. 5, the BC weight shows no significant changes during the first 1000 steps, while the success rate remains consistently low. This is because the critic estimates the current policy based on online data, which must be sufficiently collected to fully demonstrate the performance of the current policy. After the first 1000 steps, the collected online data becomes sufficient to reflect the performance of the current policy. Consequently, the BC weight exhibits a rapid increase, and the regularization begins to take effect. As the regularization progresses, the success rate increases, resulting in a decrease of the BC weight. Finally, the success rate converges to nearly 100%, while the BC weight declines to approximately zero.


V. CONCLUSIONS
This paper introduces IBORL, a method that leverages expert demonstrations to efficiently acquire dexterous hand manipulation skills in real-world environments. In terms of system design, IBORL establishes a framework that enhances the automation, efficiency, and compatibility of online reinforcement training. Regarding the algorithm, IBORL utilizes behavior cloning to pretrain the policy, thereby minimizing unnecessary exploration during real-world reinforcement learning. Furthermore, to tackle the challenges associated with catastrophic forgetting in reinforcement learning, we propose an adaptive regularization mechanism. Our empirical results across several real-world dexterous hand manipulation tasks demonstrate that IBORL surpasses existing methods, achieving a 1.6-fold improvement in success rate and enhanced cycle time efficiency. Additionally, through pretraining with imitation learning, IBORL successfully masters several tasks that current real-world robotic reinforcement learning methods struggle to accomplish. Despite IBORL’s strong performance and training efficiency, we recognize that its effectiveness is significantly contingent upon the quality of the pretrained policy. In scenarios where the pretrained policy is inadequately trained, the exploration space becomes excessively vast, complicating the effective identification of sparse reward signals for reinforcement learning and thereby hindering the development of a practical policy. Consequently, our future research will concentrate on online reinforcement learning utilizing Vision-Language-Action Models in real-world applications.
ACKNOWLEDGMENT
References
[1] M. L. Puterman, “Markov decision processes,” Handbooks in operations research and management science, vol. 2, pp. 331–434, 1990. [2] J. Luo, Z. Hu, C. Xu, Y. L. Tan, J. Berg, A. Sharma, S. Schaal, C. Finn, A. Gupta, and S. Levine, “Serl: A software suite for sample-efficient robotic reinforcement learning,” arXiv preprint arXiv:2401.16013, 2024. [3] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine, “Efficient online reinforcement learning with offline data,” 2023. [Online]. Available: https://arxiv.org/abs/2302.02948 [4] S. Haldar, V. Mathur, D. Yarats, and L. Pinto, “Watch and match: Supercharging imitation with regularized optimal transport,” 2023. [Online]. Available: https://arxiv.org/abs/2206.15469 [5] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine, “Learning complex dexterous manipulation with deep reinforcement learning and demonstrations,” 2018. [Online]. Available: https://arxiv.org/abs/1709.10087 [6] R. Jena, C. Liu, and K. Sycara, “Augmenting gail with bc for sample efficient imitation learning,” 2020. [Online]. Available: https://arxiv.org/abs/2001.07798 [7] H. Nguyen, A. Baisero, D. Wang, C. Amato, and R. Platt, “Leveraging fully observable policies for learning under partial observability,” 2022. [Online]. Available: https://arxiv.org/abs/2211.01991 [8] J. Luo, C. Xu, J. Wu, and S. Levine, “Precise and dexterous robotic manipulation via human-in-the-loop reinforcement learning,” 2024. [9] J. Luo, P. Dong, Y. Zhai, Y. Ma, and S. Levine, “Rlif: Interactive imitation learning as reinforcement learning,” 2024. [Online]. Available: https://arxiv.org/abs/2311.12996 [10] T. Z. Zhao, J. Luo, O. Sushkov, R. Pevceviciute, N. Heess, J. Scholz, S. Schaal, and S. Levine, “Offline meta-reinforcement learning for industrial insertion,” 2022. [Online]. Available: https: //arxiv.org/abs/2110.04276
[11] P. Wu, A. Escontrela, D. Hafner, K. Goldberg, and P. Abbeel, “Daydreamer: World models for physical robot learning,” 2022. [Online]. Available: https://arxiv.org/abs/2206.14176 [12] L. Smith, I. Kostrikov, and S. Levine, “A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning,” 2022. [Online]. Available: https://arxiv.org/abs/2208.07860 [13] C. Wang, H. Shi, W. Wang, R. Zhang, L. Fei-Fei, and C. K. Liu, “Dexcap: Scalable and portable mocap data collection system for dexterous manipulation,” 2024. [Online]. Available: https://arxiv.org/abs/2403.07788 [14] Z. Q. Chen, K. V. Wyk, Y.-W. Chao, W. Yang, A. Mousavian, A. Gupta, and D. Fox, “Dextransfer: Real world multi-fingered dexterous grasping with minimal human demonstrations,” 2022. [Online]. Available: https://arxiv.org/abs/2209.14284 [15] S. P. Arunachalam, I. G ̈uzey, S. Chintala, and L. Pinto, “Holo-dex: Teaching dexterity with immersive mixed reality,” 2022. [Online]. Available: https://arxiv.org/abs/2210.06463 [16] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang, “Open-television: Teleoperation with immersive active visual feedback,” 2024. [Online]. Available: https://arxiv.org/abs/2407.01512 [17] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine-grained bimanual manipulation with low-cost hardware,” 2023. [Online]. Available: https://arxiv.org/abs/2304.13705 [18] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray et al., “Learning dexterous in-hand manipulation,” The International Journal of Robotics Research, vol. 39, no. 1, pp. 3–20, 2020. [19] M. Saeed, M. Nagdi, B. Rosman, and H. H. Ali, “Deep reinforcement learning for robotic hand manipulation,” in 2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE). IEEE, 2021, pp. 1–5. [20] H. Zhu, A. Gupta, A. Rajeswaran, S. Levine, and V. Kumar, “Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost,” in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 3651–3657. [21] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey,” Journal of artificial intelligence research, vol. 4, pp. 237–285, 1996. [22] Y. Lu, K. Hausman, Y. Chebotar, M. Yan, E. Jang, A. Herzog, T. Xiao, A. Irpan, M. Khansari, D. Kalashnikov, and S. Levine, “Aw-opt: Learning robotic skills with imitation and reinforcement at scale,” 2021. [Online]. Available: https://arxiv.org/abs/2111.05424 [23] T. Johannink, S. Bahl, A. Nair, J. Luo, A. Kumar, M. Loskyll, J. A. Ojea, E. Solowjow, and S. Levine, “Residual reinforcement learning for robot control,” 2018. [Online]. Available: https: //arxiv.org/abs/1812.03201 [24] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from observation,” 2018. [Online]. Available: https://arxiv.org/abs/1805. 01954 [25] A. Nair, A. Gupta, M. Dalal, and S. Levine, “Awac: Accelerating online reinforcement learning with offline datasets,” 2021. [Online]. Available: https://arxiv.org/abs/2006.09359 [26] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, C. Fu, C. Ma, J. Jiao, S. Levine, and K. Hausman, “Jump-start reinforcement learning,” 2023. [Online]. Available: https://arxiv.org/abs/2204.02372 [27] A. Billard and D. Kragic, “Trends and challenges in robot manipulation,” Science, vol. 364, no. 6446, p. eaat8414, 2019. [28] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao, “Evaluating real-world robot manipulation policies in simulation,” 2024. [Online]. Available: https://arxiv.org/abs/2405.05941 [29] W. Burgard, O. Brock, and C. Stachniss, “Robot manipulation: Sensing and adapting to the real world,” 2008. [Online]. Available: https://api.semanticscholar.org/CorpusID:1617068 [30] Z. Liu, C. Chi, E. Cousineau, N. Kuppuswamy, B. Burchfiel, and S. Song, “Maniwav: Learning robot manipulation from in-the-wild audio-visual data,” 2024. [Online]. Available: https: //arxiv.org/abs/2406.19464